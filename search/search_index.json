{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Docs2KG","text":"<p>Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models</p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>We have published the package to PyPi: Docs2KG,</p> <p>You can install it via:</p> <pre><code>pip install Docs2KG\n</code></pre>"},{"location":"#tutorial","title":"Tutorial","text":"<p>We have a demonstration to walk through the components of Docs2KG.</p> <p>The downstream usage examples is also included.</p> <p>Video is available at Demo Docs2KG</p> <p>The tutorial details is available at Tutorial Docs2KG</p> <p>Which includes:</p> <ul> <li>How to get started with Docs2KG?</li> <li>How to process the documents with Docs2KG?</li> <li>Modules in Docs2KG?</li> <li>How to construct the unified multimodal knowledge graph?</li> <li>How to load the unified multimodal knowledge graph into the GraphDB?</li> <li>How to further enhance the unified multimodal knowledge graph?</li> <li>How to use the unified multimodal knowledge graph in downstream applications?</li> </ul> <p>We also provide the Example Codes in Example Codes Docs2KG</p> <p>The source codes documentation is available at Docs2KG Documentation</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Three pillars of the LLM applications in our opinion:</p> <ul> <li>Data</li> <li>RAG</li> <li>LLM</li> </ul> <p>Most of the tools in the market nowadays are focusing on the Retrieval Augmented Generation (RAG) pipelines or How to get Large Language Models (LLMs) to run locally.</p> <p>Typical tools include: Ollama, LangChain, LLamaIndex, etc.</p> <p>However, to make sure the wider community can benefit from the latest research, we need to first solve the data problem.</p> <p>The Wider community includes personal users, small business, and even large enterprises. Some of them might have developed databases, while most of them do have a lot of data, but they are all in unstructured form, and distributed in different places.</p> <p>So the first challenges will be:</p> <ul> <li>How can we easily process the unstructured data into a centralized place?</li> <li>What is the best way to organize the data within the centralized place?</li> </ul>"},{"location":"#proposed-solution","title":"Proposed Solution","text":"<p>This package is a proposed solution to the above challenges.</p> <ul> <li>We developed the tool for the wider community to easily process the unstructured data into a centralized place.</li> <li>We proposed a way to organize the data within the centralized place, via a Unified Multimodal Knowledge Graph   altogether with semi-structured data.</li> </ul> <p>Given the nature of unstructured and heterogeneous data, information extraction and knowledge representation pose significant challenges. In this package, we introduce Docs2KG, a novel framework designed to extract multi-modal information from diverse and heterogeneous unstructured data sources, including emails, web pages, PDF files, and Excel files. Docs2KG dynamically generates a unified knowledge graph that represents the extracted information, enabling efficient querying and exploration of the data. Unlike existing approaches that focus on specific data sources or pre-designed schemas, Docs2KG offers a flexible and extensible solution that can adapt to various document structures and content types. The proposed framework not only simplifies data processing but also improves the interpretability of models across diverse domains.</p>"},{"location":"#overall-architecture","title":"Overall Architecture","text":"<p>The overall architecture design will be shown in:</p> <p></p> <p>The data from multiple sources will be processed by the Dual-Path Data Processing. Some of the data, for example, the exported PDF files, Excel files, etc., they can be processed and handle by programming parser. So it will be converted generally into the markdown, and then transformed into the unified knowledge graph. For data like scanned PDF, images, etc., we will need the help from Doc Layout Analysis and OCR to extract the information, then we will convert the extracted information into the markdown, and then transformed into the unified knowledge graph.</p> <p>Then the unified multimodal knowledge graph will be generated based on the outputs:</p> <ul> <li>Text<ul> <li>Markdown</li> <li>Text2KG Output</li> </ul> </li> <li>Table CSV</li> <li>Table Image</li> <li>Image</li> </ul> <p>The unified multimodal knowledge graph will have mainly two aspects:</p> <ul> <li>Layout Knowledge Graph<ul> <li>The layout of the documents are helping us to understand the structure of the documents.</li> <li>So it will be also necessary and important represented within the unified multimodal knowledge graph.</li> </ul> </li> <li>Semantic Knowledge Graph<ul> <li>The semantic connections are the part our brain will be interested in when we read the documents.</li> <li>So with the help of the LLM, we can try to extract the semantic connections from the documents.</li> <li>Which can help human to understand the documents better from the semantic perspective.</li> </ul> </li> </ul>"},{"location":"#implemented-system-architecture","title":"Implemented System Architecture","text":"<p>The overall steps include:</p> <ul> <li>Data Processing<ul> <li>Dual-Path Data Processing</li> <li>Get the documents from diverse sources with diverse formats into Markdown, CSV, JSON, etc.</li> </ul> </li> <li>Unified Multimodal Knowledge Graph Construction</li> <li>GraphDB Loader<ul> <li>Load the unified multimodal knowledge graph into the GraphDB</li> <li>We use Neo4j as the GraphDB in this project</li> </ul> </li> <li>Further Enhancement<ul> <li>The KG schema is generated and dynamic, and will not be perfect at the beginning.</li> <li>So we need to further enhance the KG schema<ul> <li>Via automatic schema merge: node label frequency based merge, label semantic similarity based merge</li> <li>Via human in the loop: human review and further enhance the KG schema</li> </ul> </li> </ul> </li> <li>Downstream Applications<ul> <li>Traditional Cypher Query: NLP Query to Cypher Query (Can with help from LLM)</li> <li>Vector Based RAG:<ul> <li>Get the embedding of each node first.</li> <li>Then use the embedding of the query to do the similarity search to extract the anchor nodes within the graph.</li> <li>Use these nodes as the anchor nodes, doing multi hop information extraction to augment the query.</li> <li>Use LLM to do the final generation based on the augmented query.</li> </ul> </li> </ul> </li> </ul>"},{"location":"#setup-and-development","title":"Setup and Development","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npip install -r requirements.dev.txt\n\npip install -e .\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you find this package useful, please consider citing our work:</p> <pre><code>@misc{sun2024docs2kg,\n      title={Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models}, \n      author={Qiang Sun and Yuanyi Luo and Wenxiao Zhang and Sirui Li and Jichunyang Li and Kai Niu and Xiangrui Kong and Wei Liu},\n      year={2024},\n      eprint={2406.02962},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"Video/","title":"Video Demo","text":""},{"location":"Tutorial/1.GettingStarted/","title":"How to get started with Docs2KG?","text":""},{"location":"Tutorial/1.GettingStarted/#installation","title":"Installation","text":"<p>We have published the package to PyPi: Docs2KG,</p> <p>You can install it via:</p> <pre><code>pip install Docs2KG\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#confirm-the-installation","title":"Confirm the Installation","text":"<p>Run python in your terminal, and then import the package:</p> <pre><code>import Docs2KG\n\nprint(Docs2KG.__author__)  # ai4wa\n\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#set-up-the-openai-api-key","title":"Set up the OpenAI API Key","text":"<p>We are using the OpenAI to help us during the unified multimodal knowledge graph construction.</p> <p>So we will need to set up the OpenAI API Key first.</p> <p>You can get the API Key from the OpenAI website: OpenAI</p> <p>Then you can set it up via:</p> <pre><code>export OPENAI_API_KEY=sk-proj-xxx\n</code></pre> <p>And after this, you can start to use the Docs2KG.</p> <p>Another way you can set this up is to create a <code>.env</code> file in the root directory of your project, and then add the following line:</p> <pre><code>OPENAI_API_KEY=sk-proj-xxx\n</code></pre> <p>Then you can use the <code>python-dotenv</code> package to load the environment variables.</p> <pre><code>from dotenv import load_dotenv\n\nload_dotenv()\n</code></pre>"},{"location":"Tutorial/1.GettingStarted/#setup-data-directory","title":"Setup data directory","text":"<p>You need to specify the data input and output directory for your documents.</p> <p>Otherwise, we will default the output into the folder under your project.</p> <pre><code>data/output/\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/","title":"How to process the documents with Docs2KG?","text":"<p>We currently can process</p> <ul> <li>PDF files<ul> <li>Scanned PDFs</li> <li>Generated/Exported PDFs</li> </ul> </li> <li>Web pages</li> <li>Excels</li> <li>Emails</li> </ul> <p>This can be checked within our code sources, all the related modules are under the <code>parser</code> folder.</p>"},{"location":"Tutorial/2.DualPathProcessing/#pdf-files","title":"PDF Files","text":"<p>With the help of PyMuPDF, we can easily process the PDF files, especially the exported ones.</p> <p>For the scanned PDF files, we can extract the text, and then associate the page image to each page node.</p>"},{"location":"Tutorial/2.DualPathProcessing/#pdf-files-metadata-summary","title":"PDF Files Metadata Summary","text":"<p>We provide the function <code>get_metadata_for_files</code> to get the metadata for the PDF files.</p> <p>In this way, you can understand your bulk of PDF files better.</p> <p>For example, you will know:</p> <ul> <li>format: PDF 1.3/1.4, ect</li> <li>author</li> <li>title</li> <li>producer: with this you can estimate whether it is scanned or generated</li> </ul> <p>We also calculate the tokens within each document, you can have an estimated price to use different OpenAI models</p> <pre><code>import argparse\nfrom pathlib import Path\n\nfrom Docs2KG.parser.pdf.pdf2metadata import get_metadata_for_files\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Loop a folder of pdf files and process them\n    \"\"\"\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\n        \"--input_dir\",\n        type=str,\n        help=\"Input directory of pdf files\",\n        default=DATA_INPUT_DIR,\n    )\n    args = args.parse_args()\n    data_input_dir = Path(args.input_dir)\n\n    all_files = list(data_input_dir.rglob(\"*.pdf\"))\n    if len(all_files) == 0:\n        logger.info(\"No pdf files found in the input directory\")\n        raise Exception(\"No pdf files found in the input directory\")\n\n    all_metadata_df = get_metadata_for_files(all_files, log_summary=True)\n    \"\"\"\n    Then you can save it to a file\n\n    Example:\n        all_metadata_df.to_csv(DATA_OUTPUT_DIR / \"metadata.csv\", index=False)\n\n    Or use can use the metadata as the orchestrator\n    So files can be directed to different processing pipelines\n    And modules based on the metadata\n    \"\"\"\n    all_metadata_df.to_csv(DATA_OUTPUT_DIR / \"metadata.csv\", index=False)\n\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/#exported-pdf-process","title":"Exported PDF Process","text":"<p>Each individual PDF file will be processed, intermediate results will be saved into the output folder.</p> <pre><code>from Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2tables import PDF2Tables\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the exported PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Overall steps include:\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"historic information.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"historic information.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n    else:\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # This will extract the tables from the pdf file\n        # Output also will be the csv of the summary and each individual table csv\n\n        pdf2tables = PDF2Tables(pdf_file)\n        pdf2tables.extract2tables(output_csv=True)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u251c\u2500\u2500 tables\n        # \u2502         \u251c\u2500\u2500 page_16-table_1.csv\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u2502         \u2514\u2500\u2500 tables.csv\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/#scanned-pdf-process","title":"Scanned PDF Process","text":"<pre><code>from Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2image import PDF2Image\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the scanned PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Different from the exported PDF, current state of art still face some challenges in the scanned PDF\n    Especially when trying to extract the figures and tables from the scanned PDF\n\n    So we skip that step, only extract the text, and construct a layout knowledge graph based on the text\n    Next is add the\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"3.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"3.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n        # under the image folder, are not valid image, we need better models for that.\n        input_md_file = output_folder / \"texts\" / \"md.csv\"\n\n        markdown2json = LLMMarkdown2Json(\n            input_md_file,\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n\n        markdown2json.clean_markdown()\n        markdown2json.markdown_file = output_folder / \"texts\" / \"md.cleaned.csv\"\n\n        markdown2json.extract2json()\n\n        pdf_2_image = PDF2Image(pdf_file)\n        pdf_2_image.extract_page_2_image_df()\n        # after this we will have a added `md.json.csv` in the `texts` folder\n    else:\n        logger.info(\"This is an exported pdf, we will handle it in another demo\")\n\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/#web-pages","title":"Web Pages","text":"<p>It is quite easy to process the web pages, as it is structured with the tree structure.</p> <p>All we need to do is to download the html, and the corresponding images, this will directly happen during the KG Construction Stage</p>"},{"location":"Tutorial/2.DualPathProcessing/#excel","title":"Excel","text":"<p>For Excel files, simple one can be directly transformed into the csv.</p> <p>However, for very complex ones, we have multiple sheets within an Excel (similar to the Page concept in PDF).</p> <p>So the key task here is to separate the description part and the table part within a sheet.</p> <pre><code>from Docs2KG.modules.llm.sheet2metadata import Sheet2Metadata\nfrom Docs2KG.parser.excel.excel2image import Excel2Image\nfrom Docs2KG.parser.excel.excel2markdown import Excel2Markdown\nfrom Docs2KG.parser.excel.excel2table import Excel2Table\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Plan of the attack:\n\n    1. For each sheet, extract the description stuff, and tables will be kept still in csv\n    2. Then create the kg mainly based on the description\n    \"\"\"\n    excel_file = DATA_INPUT_DIR / \"excel\" / \"GCP_10002.xlsx\"\n    excel2table = Excel2Table(excel_file=excel_file)\n    excel2table.extract_tables_from_excel()\n\n    excel2image = Excel2Image(excel_file=excel_file)\n    excel2image.excel2image_and_pdf()\n\n    excel2markdown = Excel2Markdown(excel_file=excel_file)\n    excel2markdown.extract2markdown()\n\n    sheet_2_metadata = Sheet2Metadata(\n        excel2markdown.md_csv,\n        llm_model_name=\"gpt-3.5-turbo\",\n    )\n    sheet_2_metadata.extract_metadata()\n\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/#emails","title":"Emails","text":"<p>We provide the modules for you to connect to a specific email account, and then download the emails based on the search or numbers.</p> <p>Email will be further transformed into web pages, and attachments will be directed to the corresponding modules.</p>"},{"location":"Tutorial/2.DualPathProcessing/#download-module","title":"Download module","text":"<p>This will download your email into a <code>.eml</code> file, this will actually include all data you need, include the attachments.</p> <pre><code>from Docs2KG.parser.email.utils.email_connector import EmailConnector\n\nif __name__ == \"__main__\":\n    email_address = \"\"\n    password = \"\"\n    imap_server = \"imap.gmail.com\"\n    port = 993\n    email_connector = EmailConnector(\n        email_address=email_address,\n        password=password,\n        search_keyword=\"test search\",\n        num_emails=50,\n        imap_server=imap_server,\n        imap_port=port,\n    )\n    email_connector.pull()\n</code></pre>"},{"location":"Tutorial/2.DualPathProcessing/#process-module","title":"Process module","text":"<p>Email will be decomposed into different formats, and waiting for further processing.</p> <pre><code>from Docs2KG.parser.email.email_compose import EmailDecompose\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    email_filename = DATA_INPUT_DIR / \"email.eml\"\n    email_decomposer = EmailDecompose(email_file=email_filename)\n    email_decomposer.decompose_email()\n\n</code></pre>"},{"location":"Tutorial/3.Modules/","title":"Modules in Docs2KG?","text":"<p>We have several modules in Docs2KG to help you to construct the unified multimodal knowledge graph.</p> <p>We separate them into two main categories:</p> <ul> <li>Native</li> <li>LLM</li> </ul> <p>Native will be traditional programming methods, while LLM will be using the help from the Language Model.</p> <p>Native ones will be more deterministic, while LLM will be more probabilistic.</p> <p>LLM will be with more power, however, hard to test, less stable, and more expensive.</p> <p>The modules we have include:</p> <ul> <li>Native:<ul> <li>Markdown2JSON</li> </ul> </li> <li>LLM:<ul> <li>Markdown2JSON</li> <li>Image2Description</li> <li>Sheet2Metadata</li> <li>OpenAICall (Wrapper)</li> <li>OpenAIEmbedding (Wrapper)</li> </ul> </li> </ul> <p>For the LLM ones, generally it is using a prompt to generate the output.</p> <p>For example the <code>LLM.Markdown2JSON</code> will be used inside the <code>PDF2Text</code> to convert the markdown to JSON.</p> <pre><code>from Docs2KG.parser.pdf.pdf2text import PDF2Text\n\n# define your pdf file\npdf_file = \"path/to/your/pdf/file\"\n\npdf_to_text = PDF2Text(pdf_file)\ntext = pdf_to_text.extract2text(output_csv=True)\nmd_text = pdf_to_text.extract2markdown(output_csv=True)\n\n</code></pre>"},{"location":"Tutorial/4.Construction/","title":"How to construct the unified multimodal knowledge graph?","text":"<p>Generally when we start to construct the multimodal knowledge graph, we will need to go through the following steps:</p> <ul> <li>Construct the Layout KG</li> <li>Construct the Semantic KG</li> </ul>"},{"location":"Tutorial/4.Construction/#pdf","title":"PDF","text":""},{"location":"Tutorial/4.Construction/#generated-pdf","title":"Generated PDF","text":"<pre><code>from Docs2KG.kg.pdf_layout_kg import PDFLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2tables import PDF2Tables\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the exported PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Overall steps include:\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    2. Markdown to JSON to get the foundation of the layout knowledge graph\n        - There are two ways we can do this\n        - However, the LLM based looks like much better than the rule based, due to the noise in the PDF\n    3. Graph Construction\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"historic information.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"historic information.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n    else:\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # This will extract the tables from the pdf file\n        # Output also will be the csv of the summary and each individual table csv\n\n        pdf2tables = PDF2Tables(pdf_file)\n        pdf2tables.extract2tables(output_csv=True)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u251c\u2500\u2500 tables\n        # \u2502         \u251c\u2500\u2500 page_16-table_1.csv\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u2502         \u2514\u2500\u2500 tables.csv\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n\n        input_md_file = output_folder / \"texts\" / \"md.csv\"\n\n        markdown2json = LLMMarkdown2Json(\n            input_md_file,\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n        markdown2json.extract2json()\n\n        # after this we will have a added `md.json.csv` in the `texts` folder\n\n        # next we will start to extract the layout knowledge graph first\n\n        layout_kg = PDFLayoutKG(output_folder)\n        layout_kg.create_kg()\n        # After this, you will have the layout.json in the `kg` folder\n\n        # then we add the semantic knowledge graph\n        semantic_kg = SemanticKG(output_folder, llm_enabled=True)\n        semantic_kg.add_semantic_kg()\n\n        # After this, the layout_kg.json will be augmented with the semantic connections\n        # in the `kg` folder\n\n        # then we do the triplets extraction\n        json_2_triplets = JSON2Triplets(output_folder)\n        json_2_triplets.transform()\n\n        # After this, you will have the triplets_kg.json in the `kg` folder\n        # You can take it from here, load it into your graph db, or handle it in any way you want\n\n        # If you want to load it into Neo4j, you can refer to the `examples/kg/utils/neo4j_connector.py`\n        # to get it quickly loaded into Neo4j\n        # You can do is run the `docker compose -f examples/compose/docker-compose.yml up`\n        # So we will have a Neo4j instance running, then you can run the `neo4j_connector.py` to load the data\n        uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n        username = \"neo4j\"\n        password = \"testpassword\"\n        json_file_path = output_folder / \"kg\" / \"triplets_kg.json\"\n\n        neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n        neo4j_loader.load_data()\n        neo4j_loader.close()\n\n</code></pre>"},{"location":"Tutorial/4.Construction/#scanned-pdf","title":"Scanned PDF","text":"<pre><code>from Docs2KG.kg.pdf_layout_kg import PDFLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2image import PDF2Image\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the scanned PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Different from the exported PDF, current state of art still face some challenges in the scanned PDF\n    Especially when trying to extract the figures and tables from the scanned PDF\n\n    So we skip that step, only extract the text, and construct a layout knowledge graph based on the text\n    Next is add the\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    2. Markdown to JSON to get the foundation of the layout knowledge graph\n        - There are two ways we can do this\n        - However, the LLM based looks like much better than the rule based, due to the noise in the PDF\n    3. Graph Construction, hook the page image to the layout knowledge graph\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"3.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"3.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n        # under the image folder, are not valid image, we need better models for that.\n        input_md_file = output_folder / \"texts\" / \"md.csv\"\n\n        markdown2json = LLMMarkdown2Json(\n            input_md_file,\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n\n        markdown2json.clean_markdown()\n        markdown2json.markdown_file = output_folder / \"texts\" / \"md.cleaned.csv\"\n\n        markdown2json.extract2json()\n\n        pdf_2_image = PDF2Image(pdf_file)\n        pdf_2_image.extract_page_2_image_df()\n        # after this we will have a added `md.json.csv` in the `texts` folder\n\n        # next we will start to extract the layout knowledge graph first\n\n        layout_kg = PDFLayoutKG(output_folder, scanned_pdf=True)\n        layout_kg.create_kg()\n\n        # After this, you will have the layout.json in the `kg` folder\n\n        # then we add the semantic knowledge graph\n        semantic_kg = SemanticKG(\n            output_folder, llm_enabled=True, input_format=\"pdf_scanned\"\n        )\n        semantic_kg.add_semantic_kg()\n\n        # After this, the layout_kg.json will be augmented with the semantic connections\n        # in the `kg` folder\n\n        # then we do the triplets extraction\n        json_2_triplets = JSON2Triplets(output_folder)\n        json_2_triplets.transform()\n\n        # After this, you will have the triplets_kg.json in the `kg` folder\n        # You can take it from here, load it into your graph db, or handle it in any way you want\n\n        # If you want to load it into Neo4j, you can refer to the `examples/kg/utils/neo4j_connector.py`\n        # to get it quickly loaded into Neo4j\n        # You can do is run the `docker compose -f examples/compose/docker-compose.yml up`\n        # So we will have a Neo4j instance running, then you can run the `neo4j_connector.py` to load the data\n        uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n        username = \"neo4j\"\n        password = \"testpassword\"\n        json_file_path = output_folder / \"kg\" / \"triplets_kg.json\"\n\n        neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n        neo4j_loader.load_data()\n        neo4j_loader.close()\n    else:\n        logger.info(\"This is an exported pdf, we will handle it in another demo\")\n</code></pre>"},{"location":"Tutorial/4.Construction/#web-pages","title":"Web pages","text":"<pre><code>from Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.kg.web_layout_kg import WebLayoutKG\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Extract the HTML file to images, markdown, tables, and urls and save it to the output directory\n\n    1. Get html, images, markdown, tables, and urls from the given URL\n    \"\"\"\n    url = \"https://abs.gov.au/census/find-census-data/quickstats/2021/LGA57080\"\n\n    web_layout_kg = WebLayoutKG(url=url)\n    web_layout_kg.create_kg()\n\n    semantic_kg = SemanticKG(\n        folder_path=web_layout_kg.output_dir, input_format=\"html\", llm_enabled=True\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(web_layout_kg.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = web_layout_kg.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"Tutorial/4.Construction/#excel","title":"Excel","text":"<pre><code>from Docs2KG.kg.email_layout_kg import EmailLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.parser.email.email_compose import EmailDecompose\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    email_filename = DATA_INPUT_DIR / \"email.eml\"\n    email_decomposer = EmailDecompose(email_file=email_filename)\n    email_decomposer.decompose_email()\n\n    email_layout_kg = EmailLayoutKG(output_dir=email_decomposer.output_dir)\n    email_layout_kg.create_kg()\n\n    semantic_kg = SemanticKG(\n        email_decomposer.output_dir, llm_enabled=True, input_format=\"email\"\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(email_decomposer.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = email_decomposer.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"Tutorial/4.Construction/#emails","title":"Emails","text":"<pre><code>from Docs2KG.kg.email_layout_kg import EmailLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.parser.email.email_compose import EmailDecompose\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    email_filename = DATA_INPUT_DIR / \"email.eml\"\n    email_decomposer = EmailDecompose(email_file=email_filename)\n    email_decomposer.decompose_email()\n\n    email_layout_kg = EmailLayoutKG(output_dir=email_decomposer.output_dir)\n    email_layout_kg.create_kg()\n\n    semantic_kg = SemanticKG(\n        email_decomposer.output_dir, llm_enabled=True, input_format=\"email\"\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(email_decomposer.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = email_decomposer.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n</code></pre>"},{"location":"Tutorial/4.Construction/#documentation-linkage","title":"Documentation Linkage","text":"<p>To have a Docs KG, we need to link multiple documents together, so we can have a unified multimodal knowledge graph.</p> <p>Currently, we use LLM to help us to do this from summary level of each document.</p> <p>Find the linkage from</p> <ul> <li>temporal</li> <li>semantic</li> </ul> <p>via the summary of each docs and the metadata of each doc.</p> <pre><code>import json\n\nfrom Docs2KG.kg.docs_linkage import DocsLinkage\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    input_folder = DATA_INPUT_DIR / \"docslinkage\" / \"docs.json\"\n\n    docs_linkage = DocsLinkage(input_folder)\n    rels = docs_linkage.openai_link_docs()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    # get the rels into a temp file json\n    json_file_path = DATA_INPUT_DIR / \"docslinkage\" / \"docs_linkage.json\"\n    json_file_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(json_file_path, \"w\") as f:\n        json.dump(rels, f)\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n    # remove the temp file\n    json_file_path.unlink()\n\n</code></pre>"},{"location":"Tutorial/5.Loader/","title":"How to load the unified multimodal knowledge graph into the GraphDB?","text":"<p>The process we designed is that after the KG Construction process, you will have two files within your output dir</p> <ul> <li><code>kg/layout_kg.json</code></li> <li><code>kg/triplets_kg.json</code></li> </ul>"},{"location":"Tutorial/5.Loader/#triplets-generation","title":"Triplets Generation","text":"<p>Example format of the <code>layout_kg.json</code></p> <pre><code>{\n  \"node_type\": \"document\",\n  \"uuid\": \"77d231ae-93db-4073-a592-10caccca86d2\",\n  \"node_properties\": {\n    \"format\": \"PDF 1.3\",\n    \"title\": \"Microsoft Word - WR_C19_2000_2006A.doc\",\n    \"author\": \"michelle\",\n    \"subject\": \"\",\n    \"keywords\": \"\",\n    \"creator\": \"PScript5.dll Version 5.2\",\n    \"producer\": \"GNU Ghostscript 7.06\",\n    \"creationDate\": \"2/7/2006 16:39:28\",\n    \"modDate\": \"\",\n    \"trapped\": \"\",\n    \"encryption\": null,\n    \"text_token\": 25959,\n    \"estimated_price_gpt35\": 0.103836,\n    \"estimated_price_gpt4o\": 1.03836,\n    \"estimated_price_4_turbo\": 2.07672,\n    \"file_path\": \"/Users/pascal/PhD/Docs2KG/data/input/tests_pdf/4.pdf\",\n    \"scanned_or_exported\": \"exported\"\n  },\n  \"children\": [\n    {\n      \"node_type\": \"page\",\n      \"uuid\": \"854072eb-58cf-4b79-b02f-0830d586a71f\",\n      \"node_properties\": {\n        \"page_number\": 0,\n        \"page_text\": \"\\n**Midwest Corporation Limited**\\n****\\n**Weld Range Tenement Group**\\n****\\n**Annual Report**\\n****\\n**Combined Report Group C19/2000**\\n****\\n**TR70/3902, M20/402, M20/403 and E20/176**\\n****\\n**5 January 2005 \\u2013 6 January 2006**\\n****\\n\\n\\n\\n\\n\\n**Author:**  **Michael Brown** \\n         B.Sc., Geol, Grad Dip (GIS)\\n  **Graeme Johnston** \\nB.Sc., (Geol), M.Sc., D.I.C., F.G.S\\n**Date:**   6 th January 2006\\n\\n\\n\\n\\n32 Kings Park Rd West Perth 6005\\n\\n\\n\\n-----\\n\\n\"\n      },\n      \"children\": [\n        {\n          \"node_type\": \"h1\",\n          \"uuid\": \"7dbb9ae2-27e0-4e8e-8737-23f0cd345f20\",\n          \"node_properties\": {\n            \"content\": \"Midwest Corporation Limited\",\n            \"text\": \"\",\n            \"records\": []\n          },\n          \"children\": [\n            {\n              \"node_type\": \"h2\",\n              \"uuid\": \"309f0aa5-992a-4583-9d49-2a879992c231\",\n              \"node_properties\": {\n                \"content\": \"Weld Range Tenement Group\",\n                \"text\": \"\",\n                \"records\": []\n              },\n              \"children\": []\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n</code></pre> <p>The first one is the nested JSON version of the KG.</p> <p>If one node is connected to another node with relationships further than the tree, then we will have a linkage</p> <p>for example <code>mentioned_in</code> field within the node properties.</p> <p>The target node uuid will be stored in the <code>mentioned_in</code> field.</p> <p>To get this format be able to load into the GraphDB, we need to convert it into the proper format.</p> <p>We use Neo4j as an example here.</p> <p>We will convert the <code>kg_layout.json</code> into a json with two list</p> <pre><code>{\n  \"nodes\": [\n    {\n      \"uuid\": \"4d2ab6c5-bcd0-4373-9e5c-15abdad53e92\",\n      \"labels\": [\n        \"TEXT_BLOCK\"\n      ],\n      \"properties\": {\n        \"text_block_bbox\": \"(171.1199493408203, 417.1202087402344, 446.0379943847656, 432.7322082519531)\",\n        \"content\": \" Plate 4: Looking up to W16 Hematite\\u2013Goethite Lens \",\n        \"position\": \"below\",\n        \"text_block_number\": 10\n      }\n    }\n  ],\n  \"relationships\": [\n    {\n      \"start_node\": \"4d2ab6c5-bcd0-4373-9e5c-15abdad53e92\",\n      \"end_node\": \"4d2ab6c5-bcd0-4373-9e5c-15abdad53e92\",\n      \"type\": \"mentioned_in\",\n      \"properties\": {\n        \"page_number\": 0\n      }\n    }\n  ]\n}\n</code></pre> <p>This will be done in the class <code>json2triplets.py</code></p>"},{"location":"Tutorial/5.Loader/#load-the-kg-into-graphdb","title":"Load the KG into GraphDB","text":"<p>Then the next thing is easy, we just need to have a class to load it into the Neo4j with cypher.</p> <p>You can use cypher directly to load the json into the Neo4j. Or use the class we provided <code>neo4j_connector.py</code></p> <pre><code>uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\nusername = \"neo4j\"\npassword = \"testpassword\"\njson_file_path = excel2markdown.output_dir / \"kg\" / \"triplets_kg.json\"\n\nneo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\nneo4j_loader.load_data()\nneo4j_loader.close()\n</code></pre>"},{"location":"Tutorial/5.Loader/#start-the-neo4j","title":"Start the neo4j","text":"<p>Before you start it, if you do not have a neo4j instance, you can start one with our docker-compose file</p> <pre><code>docker compose -f examples/compose/docker-compose.yml up\n</code></pre>"},{"location":"Tutorial/6.Enhancement/","title":"How to further enhance the unified multimodal knowledge graph?","text":"<p>After the Unified Multimodal KG is generated, we can find that the KG schema (node label) is dynamic generated.</p> <p>Which will not be perfect as you expected at the beginning.</p> <p>To improve the schema, one way to do it is using the ontology based way to do the prompting and pre checking.</p> <p>We will provide that solution later.</p> <p>Currently, what we think is more intuitive is</p> <ul> <li>automatic schema merge<ul> <li>node label frequency based merge</li> <li>label semantic similarity based merge</li> </ul> </li> <li>human in the loop<ul> <li>human review and further enhance the KG schema</li> </ul> </li> </ul> <pre><code>from Docs2KG.kg.dynamic_schema import DynamicSchema\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    \"\"\"\n    1. From the generated KG, how can we do the schema merge?\n        - You can hook this into the neo4j after the KG is loaded\n    2. Human in the loop for the schema merge\n    \"\"\"\n    kg_json_file = (\n            DATA_OUTPUT_DIR / \"Excellent_Example_Report.pdf\" / \"kg\" / \"triplets_kg.json\"\n    )\n    dynamic_schema = DynamicSchema(kg_json_file=kg_json_file)\n    dynamic_schema.schema_extraction()\n    dynamic_schema.schema_freq_merge()\n    dynamic_schema.schema_similarity_merge()\n    dynamic_schema.human_in_the_loop_input()\n\n</code></pre> <p>As the function name indicates, automatically ones will remove the low frequency labels and merge them into the high frequency labels.</p> <p>The semantic similarity based merge will use the word embedding to calculate the similarity between the labels.</p> <p>The human in the loop will provide the frequency of the labels and the label itself, ask the human to decide whether to merge them or not.</p>"},{"location":"Tutorial/7.Applications/","title":"How to use the unified multimodal knowledge graph in downstream applications?","text":"<p>To use a unified multimodal knowledge graph in downstream applications, typically there are two ways:</p> <ul> <li>Querying the KG with Cypher or SPARQL</li> <li>Vector Based RAG</li> </ul>"},{"location":"Tutorial/7.Applications/#querying-the-kg","title":"Querying the KG","text":"<p>We have the schema and data rest in the KG, traditionally, we can use Cypher or SPARQL to query the KG.</p> <p>With the advance of the LLM, we can also use the NLP query to Cypher Query.</p> <p>First, we need to convert the NLP query into the Cypher Query.</p> <p>Then we can use the Cypher Query to query the KG.</p> <p>This is quite a standard way, we provide example in our paper.</p> <p>All you need is the interface to run the query in Neo4j.</p> <p>For example if we asked the query for the document we processed earlier:</p> <pre><code>Can you show me all documents and their components related to events that occurred in the years 2011 and 2021?\n</code></pre> <p>We can create a cypher query like this:</p> <p></p> <p>And we will have the result like this:</p> <p></p>"},{"location":"Tutorial/7.Applications/#vector-based-rag","title":"Vector Based RAG","text":"<p>Another way is to embed all the content and metadata of each node. When a new query comes in, we can embed the query and do the similarity search to extract the anchor nodes within the graph.</p> <p>Then we can use these nodes as the anchor nodes, doing multi hop information extraction to augment the query.</p> <p>Finally, we can use the LLM to do the final generation based on the augmented query.</p> <p>The strategy to extract relevant information from the KG can not limit to the multi hop information extraction, you can also design your own information extraction strategy based on the specific task, for example, embedding based BFS or DFS.</p> <p>So the same NLP query like above, we will have a cypher query like this after similarity match:</p> <pre><code>MATCH (startNode)-[*1..3]-&gt;(endNode)\nWHERE startNode.uuid IN ['17da8cc5-9986-4126-8752-545162ca6aa7', 'dae828f5-e968-49a7-bf03-0a9a81022434', '2cc13873-9bce-45ed-a0a3-df9765e90579', '32eb0955-b983-446f-b883-db575f304f88', 'd6735d2d-afa0-41c0-842b-c1ecb9a4b4d5', '503e64f8-1a22-44b2-be99-240578925c1f', '1b2fe6fb-2949-40ce-9ba1-61496f164a67', '1a0e5f3a-b250-442d-afc2-1f139a367e21', 'c632fceb-3af9-4b11-9082-3ce615d1fd5b', '29544460-0135-46e9-94f5-60c2b86caa53']\nRETURN endNode\n</code></pre> <p>Output will be like this:</p> <p></p>"},{"location":"examples/demo/dynamic_schema/","title":"Dynamic schema","text":""},{"location":"examples/demo/dynamic_schema/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.dynamic_schema import DynamicSchema\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    \"\"\"\n    1. From the generated KG, how can we do the schema merge?\n        - You can hook this into the neo4j after the KG is loaded\n    2. Human in the loop for the schema merge\n    \"\"\"\n    kg_json_file = (\n        DATA_OUTPUT_DIR / \"Excellent_Example_Report.pdf\" / \"kg\" / \"triplets_kg.json\"\n    )\n    dynamic_schema = DynamicSchema(kg_json_file=kg_json_file)\n    dynamic_schema.schema_extraction()\n    dynamic_schema.schema_freq_merge()\n    dynamic_schema.schema_similarity_merge()\n    dynamic_schema.human_in_the_loop_input()\n\n</code></pre>"},{"location":"examples/demo/emails/","title":"Emails","text":""},{"location":"examples/demo/emails/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.email_layout_kg import EmailLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.parser.email.email_compose import EmailDecompose\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    email_filename = DATA_INPUT_DIR / \"email.eml\"\n    email_decomposer = EmailDecompose(email_file=email_filename)\n    email_decomposer.decompose_email()\n\n    email_layout_kg = EmailLayoutKG(output_dir=email_decomposer.output_dir)\n    email_layout_kg.create_kg()\n\n    semantic_kg = SemanticKG(\n        email_decomposer.output_dir, llm_enabled=True, input_format=\"email\"\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(email_decomposer.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = email_decomposer.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"examples/demo/excel/","title":"Excel","text":""},{"location":"examples/demo/excel/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.excel_layout_kg import ExcelLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.modules.llm.sheet2metadata import Sheet2Metadata\nfrom Docs2KG.parser.excel.excel2image import Excel2Image\nfrom Docs2KG.parser.excel.excel2markdown import Excel2Markdown\nfrom Docs2KG.parser.excel.excel2table import Excel2Table\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Plan of the attack:\n\n    1. For each sheet, extract the description stuff, and tables will be kept still in csv\n    2. Then create the kg mainly based on the description\n    \"\"\"\n    excel_file = DATA_INPUT_DIR / \"excel\" / \"GCP_10002.xlsx\"\n    excel2table = Excel2Table(excel_file=excel_file)\n    excel2table.extract_tables_from_excel()\n\n    excel2image = Excel2Image(excel_file=excel_file)\n    excel2image.excel2image_and_pdf()\n\n    excel2markdown = Excel2Markdown(excel_file=excel_file)\n    excel2markdown.extract2markdown()\n\n    sheet_2_metadata = Sheet2Metadata(\n        excel2markdown.md_csv,\n        llm_model_name=\"gpt-3.5-turbo\",\n    )\n    sheet_2_metadata.extract_metadata()\n\n    excel_layout_kg = ExcelLayoutKG(excel2markdown.output_dir, input_format=\"excel\")\n    excel_layout_kg.create_kg()\n    # After this, you will have the layout.json in the `kg` folder\n\n    # then we add the semantic knowledge graph\n    semantic_kg = SemanticKG(\n        excel2markdown.output_dir, llm_enabled=True, input_format=\"excel\"\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(excel2markdown.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = excel2markdown.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"examples/demo/pdf_exported/","title":"Pdf exported","text":""},{"location":"examples/demo/pdf_exported/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.pdf_layout_kg import PDFLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2tables import PDF2Tables\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the exported PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Overall steps include:\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    2. Markdown to JSON to get the foundation of the layout knowledge graph\n        - There are two ways we can do this\n        - However, the LLM based looks like much better than the rule based, due to the noise in the PDF\n    3. Graph Construction\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"historic information.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"historic information.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n    else:\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # This will extract the tables from the pdf file\n        # Output also will be the csv of the summary and each individual table csv\n\n        pdf2tables = PDF2Tables(pdf_file)\n        pdf2tables.extract2tables(output_csv=True)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u251c\u2500\u2500 tables\n        # \u2502         \u251c\u2500\u2500 page_16-table_1.csv\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u2502         \u2514\u2500\u2500 tables.csv\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n\n        input_md_file = output_folder / \"texts\" / \"md.csv\"\n\n        markdown2json = LLMMarkdown2Json(\n            input_md_file,\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n        markdown2json.extract2json()\n\n        # after this we will have a added `md.json.csv` in the `texts` folder\n\n        # next we will start to extract the layout knowledge graph first\n\n        layout_kg = PDFLayoutKG(output_folder)\n        layout_kg.create_kg()\n        # After this, you will have the layout.json in the `kg` folder\n\n        # then we add the semantic knowledge graph\n        semantic_kg = SemanticKG(output_folder, llm_enabled=True)\n        semantic_kg.add_semantic_kg()\n\n        # After this, the layout_kg.json will be augmented with the semantic connections\n        # in the `kg` folder\n\n        # then we do the triplets extraction\n        json_2_triplets = JSON2Triplets(output_folder)\n        json_2_triplets.transform()\n\n        # After this, you will have the triplets_kg.json in the `kg` folder\n        # You can take it from here, load it into your graph db, or handle it in any way you want\n\n        # If you want to load it into Neo4j, you can refer to the `examples/kg/utils/neo4j_connector.py`\n        # to get it quickly loaded into Neo4j\n        # You can do is run the `docker compose -f examples/compose/docker-compose.yml up`\n        # So we will have a Neo4j instance running, then you can run the `neo4j_connector.py` to load the data\n        uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n        username = \"neo4j\"\n        password = \"testpassword\"\n        json_file_path = output_folder / \"kg\" / \"triplets_kg.json\"\n\n        neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n        neo4j_loader.load_data()\n        neo4j_loader.close()\n\n</code></pre>"},{"location":"examples/demo/pdf_scanned/","title":"Pdf scanned","text":""},{"location":"examples/demo/pdf_scanned/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.pdf_layout_kg import PDFLayoutKG\nfrom Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2image import PDF2Image\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Here what we want to achieve is how to get the scanned PDF into the triplets for the Neo4j.\n\n    We will require some of the OpenAI functions, we are currently default the model to `gpt-3.5-turbo`\n\n    The cost itself is quite low, but you can determine whether you want to use it or not.\n\n    Different from the exported PDF, current state of art still face some challenges in the scanned PDF\n    Especially when trying to extract the figures and tables from the scanned PDF\n\n    So we skip that step, only extract the text, and construct a layout knowledge graph based on the text\n    Next is add the\n\n    1. Extract Text, Images, Tables from the PDF\n        - Extract text **block** and image **blocks** from the PDF\n            - This will provide bounding boxes for the text and images\n        - Extract tables from the PDF\n        - Extract the text and markdown from the PDF, each page will be one markdown file\n            - Output will be in a csv\n    2. Markdown to JSON to get the foundation of the layout knowledge graph\n        - There are two ways we can do this\n        - However, the LLM based looks like much better than the rule based, due to the noise in the PDF\n    3. Graph Construction, hook the page image to the layout knowledge graph\n    \"\"\"\n\n    # you can name your file here\n    pdf_file = DATA_INPUT_DIR / \"3.pdf\"\n\n    output_folder = DATA_OUTPUT_DIR / \"3.pdf\"\n    # the output will be default to `DATA_OUTPUT_DIR / \"4.pdf\" /` folder\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, we will handle it in another demo\")\n\n        # This will extract the text, images.\n        #\n        # Output images/text with bounding boxes into a df\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        # Processing the text from the pdf file\n        # For each page, we will have a markdown and text content,\n        # Output will be in a csv\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n        # Until now, your output folder should be some like this\n        # .\n        # \u251c\u2500\u2500 4.pdf\n        # \u251c\u2500\u2500 images\n        # \u2502         \u251c\u2500\u2500 blocks_images.csv\n        # \u2502         \u251c\u2500\u2500 page_0_block_1.jpeg\n        # \u2502         \u251c\u2500\u2500 page_0_block_4.jpeg\n        # \u2502         \u251c\u2500\u2500 ....\n        # \u251c\u2500\u2500 metadata.json\n        # \u2514\u2500\u2500 texts\n        #     \u251c\u2500\u2500 blocks_texts.csv\n        #     \u251c\u2500\u2500 md.csv\n        #     \u2514\u2500\u2500 text.csv\n        # under the image folder, are not valid image, we need better models for that.\n        input_md_file = output_folder / \"texts\" / \"md.csv\"\n\n        markdown2json = LLMMarkdown2Json(\n            input_md_file,\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n\n        markdown2json.clean_markdown()\n        markdown2json.markdown_file = output_folder / \"texts\" / \"md.cleaned.csv\"\n\n        markdown2json.extract2json()\n\n        pdf_2_image = PDF2Image(pdf_file)\n        pdf_2_image.extract_page_2_image_df()\n        # after this we will have a added `md.json.csv` in the `texts` folder\n\n        # next we will start to extract the layout knowledge graph first\n\n        layout_kg = PDFLayoutKG(output_folder, input_format=\"pdf_scanned\")\n        layout_kg.create_kg()\n\n        # After this, you will have the layout.json in the `kg` folder\n\n        # then we add the semantic knowledge graph\n        semantic_kg = SemanticKG(\n            output_folder, llm_enabled=True, input_format=\"pdf_scanned\"\n        )\n        semantic_kg.add_semantic_kg()\n\n        # After this, the layout_kg.json will be augmented with the semantic connections\n        # in the `kg` folder\n\n        # then we do the triplets extraction\n        json_2_triplets = JSON2Triplets(output_folder)\n        json_2_triplets.transform()\n\n        # After this, you will have the triplets_kg.json in the `kg` folder\n        # You can take it from here, load it into your graph db, or handle it in any way you want\n\n        # If you want to load it into Neo4j, you can refer to the `examples/kg/utils/neo4j_connector.py`\n        # to get it quickly loaded into Neo4j\n        # You can do is run the `docker compose -f examples/compose/docker-compose.yml up`\n        # So we will have a Neo4j instance running, then you can run the `neo4j_connector.py` to load the data\n        uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n        username = \"neo4j\"\n        password = \"testpassword\"\n        json_file_path = output_folder / \"kg\" / \"triplets_kg.json\"\n\n        neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n        neo4j_loader.load_data()\n        neo4j_loader.close()\n    else:\n        logger.info(\"This is an exported pdf, we will handle it in another demo\")\n\n</code></pre>"},{"location":"examples/demo/web/","title":"Web","text":""},{"location":"examples/demo/web/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.kg.web_layout_kg import WebLayoutKG\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Extract the HTML file to images, markdown, tables, and urls and save it to the output directory\n\n    1. Get html, images, markdown, tables, and urls from the given URL\n    \"\"\"\n    url = \"https://abs.gov.au/census/find-census-data/quickstats/2021/LGA57080\"\n\n    web_layout_kg = WebLayoutKG(url=url)\n    web_layout_kg.create_kg()\n\n    semantic_kg = SemanticKG(\n        folder_path=web_layout_kg.output_dir, input_format=\"html\", llm_enabled=True\n    )\n    semantic_kg.add_semantic_kg()\n\n    json_2_triplets = JSON2Triplets(web_layout_kg.output_dir)\n    json_2_triplets.transform()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = web_layout_kg.output_dir / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"examples/kg/docs_linkage/","title":"Docs linkage","text":""},{"location":"examples/kg/docs_linkage/#code-example","title":"Code Example","text":"<pre><code>import json\n\nfrom Docs2KG.kg.docs_linkage import DocsLinkage\nfrom Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    input_folder = DATA_INPUT_DIR / \"docslinkage\" / \"docs.json\"\n\n    docs_linkage = DocsLinkage(input_folder)\n    rels = docs_linkage.openai_link_docs()\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    # get the rels into a temp file json\n    json_file_path = DATA_INPUT_DIR / \"docslinkage\" / \"docs_linkage.json\"\n    json_file_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(json_file_path, \"w\") as f:\n        json.dump(rels, f)\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n    # remove the temp file\n    json_file_path.unlink()\n\n</code></pre>"},{"location":"examples/kg/layout_kg/","title":"Layout kg","text":""},{"location":"examples/kg/layout_kg/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.pdf_layout_kg import PDFLayoutKG\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    input_folder = DATA_OUTPUT_DIR / \"4.pdf\"\n\n    layout_kg = PDFLayoutKG(input_folder)\n    layout_kg.create_kg()\n\n</code></pre>"},{"location":"examples/kg/semantic_kg/","title":"Semantic kg","text":""},{"location":"examples/kg/semantic_kg/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.semantic_kg import SemanticKG\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    input_folder = DATA_OUTPUT_DIR / \"4.pdf\"\n\n    semantic_kg = SemanticKG(input_folder, llm_enabled=True)\n    semantic_kg.add_semantic_kg()\n\n</code></pre>"},{"location":"examples/kg/utils/json2triplets/","title":"Json2triplets","text":""},{"location":"examples/kg/utils/json2triplets/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.utils.json2triplets import JSON2Triplets\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    input_folder = DATA_OUTPUT_DIR / \"4.pdf\"\n\n    layout_kg = JSON2Triplets(input_folder)\n    layout_kg.transform()\n\n</code></pre>"},{"location":"examples/kg/utils/neo4j_connector/","title":"Neo4j connector","text":""},{"location":"examples/kg/utils/neo4j_connector/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.kg.utils.neo4j_connector import Neo4jLoader\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    \"\"\"\n    This is to demonstrate how to load the triplets into Neo4j.\n\n    After we have the output from the json2triplets.py,\n    we can load the triplets into Neo4j.\n\n    If you have a hosted Neo4j instance,\n    you can change the uri to the hosted uri, also the username and password.\n\n    If you do not, and just want to test out.\n\n    All you need to do is get the docker installed\n    Then run\n    `docker compose -f examples/compose/docker-compose.yml up`\n    \"\"\"\n    uri = \"bolt://localhost:7687\"  # if it is a remote graph db, you can change it to the remote uri\n    username = \"neo4j\"\n    password = \"testpassword\"\n    json_file_path = DATA_OUTPUT_DIR / \"4.pdf\" / \"kg\" / \"triplets_kg.json\"\n\n    neo4j_loader = Neo4jLoader(uri, username, password, json_file_path, clean=True)\n    neo4j_loader.load_data()\n    neo4j_loader.close()\n\n</code></pre>"},{"location":"examples/modules/markdown_clean_md/","title":"Markdown clean md","text":""},{"location":"examples/modules/markdown_clean_md/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    input_md_file = DATA_OUTPUT_DIR / \"3.pdf\" / \"texts\" / \"md.csv\"\n\n    markdown2json = LLMMarkdown2Json(\n        input_md_file,\n        llm_model_name=\"gpt-3.5-turbo\",\n    )\n    markdown2json.clean_markdown()\n\n</code></pre>"},{"location":"examples/modules/markdown_json/","title":"Markdown json","text":""},{"location":"examples/modules/markdown_json/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.utils.constants import DATA_OUTPUT_DIR\n\nif __name__ == \"__main__\":\n    input_md_file = DATA_OUTPUT_DIR / \"4.pdf\" / \"texts\" / \"md.csv\"\n\n    markdown2json = LLMMarkdown2Json(\n        input_md_file,\n        llm_model_name=\"gpt-3.5-turbo\",\n    )\n    markdown2json.extract2json()\n\n</code></pre>"},{"location":"examples/parser/email/email_process/","title":"Email process","text":""},{"location":"examples/parser/email/email_process/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.parser.email.email_compose import EmailDecompose\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    email_filename = DATA_INPUT_DIR / \"email.eml\"\n    email2md = EmailDecompose(email_file=email_filename)\n    email2md.decompose_email()\n\n</code></pre>"},{"location":"examples/parser/email/email_pull/","title":"Email pull","text":""},{"location":"examples/parser/email/email_pull/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.parser.email.utils.email_connector import EmailConnector\n\nif __name__ == \"__main__\":\n    email_address = \"\"\n    password = \"\"\n    imap_server = \"imap.gmail.com\"\n    port = 993\n    email_connector = EmailConnector(\n        email_address=email_address,\n        password=password,\n        search_keyword=\"test search\",\n        num_emails=50,\n        imap_server=imap_server,\n        imap_port=port,\n    )\n    email_connector.pull()\n\n</code></pre>"},{"location":"examples/parser/excel/excel_process/","title":"Excel process","text":""},{"location":"examples/parser/excel/excel_process/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.parser.excel.excel2image import Excel2Image\nfrom Docs2KG.parser.excel.excel2markdown import Excel2Markdown\nfrom Docs2KG.parser.excel.excel2table import Excel2Table\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\n\nif __name__ == \"__main__\":\n    excel_file = DATA_INPUT_DIR / \"excel\" / \"GCP_10002.xlsx\"\n    excel2table = Excel2Table(excel_file=excel_file)\n    excel2table.extract_tables_from_excel()\n    excel2image = Excel2Image(excel_file=excel_file)\n    excel2image.excel2image_and_pdf()\n    excel2markdown = Excel2Markdown(excel_file=excel_file)\n    excel2markdown.extract2markdown()\n\n</code></pre>"},{"location":"examples/parser/pdf/pdf_metadata_summary/","title":"Pdf metadata summary","text":""},{"location":"examples/parser/pdf/pdf_metadata_summary/#code-example","title":"Code Example","text":"<pre><code>import argparse\nfrom pathlib import Path\n\nfrom Docs2KG.parser.pdf.pdf2metadata import get_metadata_for_files\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Loop a folder of pdf files and process them\n    \"\"\"\n\n    args = argparse.ArgumentParser()\n    args.add_argument(\n        \"--input_dir\",\n        type=str,\n        help=\"Input directory of pdf files\",\n        default=DATA_INPUT_DIR,\n    )\n    args = args.parse_args()\n    data_input_dir = Path(args.input_dir)\n\n    all_files = list(data_input_dir.rglob(\"*.pdf\"))\n    if len(all_files) == 0:\n        logger.info(\"No pdf files found in the input directory\")\n        raise Exception(\"No pdf files found in the input directory\")\n\n    all_metadata_df = get_metadata_for_files(all_files, log_summary=True)\n    \"\"\"\n    Then you can save it to a file\n\n    Example:\n        all_metadata_df.to_csv(DATA_OUTPUT_DIR / \"metadata.csv\", index=False)\n\n    Or use can use the metadata as the orchestrator\n    So files can be directed to different processing pipelines\n    And modules based on the metadata\n    \"\"\"\n    all_metadata_df.to_csv(DATA_OUTPUT_DIR / \"metadata.csv\", index=False)\n\n</code></pre>"},{"location":"examples/parser/pdf/pdf_metadata_summary/#examples.parser.pdf.pdf_metadata_summary.all_metadata_df","title":"<code>all_metadata_df = get_metadata_for_files(all_files, log_summary=True)</code>  <code>module-attribute</code>","text":"<p>Then you can save it to a file</p> Example <p>all_metadata_df.to_csv(DATA_OUTPUT_DIR / \"metadata.csv\", index=False)</p> <p>Or use can use the metadata as the orchestrator So files can be directed to different processing pipelines And modules based on the metadata</p>"},{"location":"examples/parser/pdf/pdf_process/","title":"Pdf process","text":""},{"location":"examples/parser/pdf/pdf_process/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2tables import PDF2Tables\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    pdf_file = DATA_INPUT_DIR / \"OMR241-Chartbook.pdf\"\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\"This is a scanned pdf, can not process it now\")\n    else:\n        \"\"\"\n        This will extract the text, images.\n\n        Output images/text with bounding boxes into a df\n\n        \"\"\"\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n\n        \"\"\"\n        This will extract the tables from the pdf file\n        \"\"\"\n        pdf2tables = PDF2Tables(pdf_file)\n        pdf2tables.extract2tables(output_csv=True)\n\n        \"\"\"\n        Processing the text from the pdf file\n        \"\"\"\n\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n\n</code></pre>"},{"location":"examples/parser/pdf/pdf_scanned_process/","title":"Pdf scanned process","text":""},{"location":"examples/parser/pdf/pdf_scanned_process/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.modules.llm.markdown2json import LLMMarkdown2Json\nfrom Docs2KG.parser.pdf.pdf2blocks import PDF2Blocks\nfrom Docs2KG.parser.pdf.pdf2metadata import PDF_TYPE_SCANNED, get_scanned_or_exported\nfrom Docs2KG.parser.pdf.pdf2text import PDF2Text\nfrom Docs2KG.utils.constants import DATA_INPUT_DIR, DATA_OUTPUT_DIR\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    pdf_file = DATA_INPUT_DIR / \"3.pdf\"\n    output_folder = DATA_OUTPUT_DIR / \"3.pdf\"\n    scanned_or_exported = get_scanned_or_exported(pdf_file)\n    if scanned_or_exported == PDF_TYPE_SCANNED:\n        logger.info(\n            \"This is a scanned pdf, we can only process it to the markdown and a image for whole page\"\n        )\n        pdf_2_blocks = PDF2Blocks(pdf_file)\n        blocks_dict = pdf_2_blocks.extract_df(output_csv=True)\n        logger.info(blocks_dict)\n        pdf_to_text = PDF2Text(pdf_file)\n        text = pdf_to_text.extract2text(output_csv=True)\n        md_text = pdf_to_text.extract2markdown(output_csv=True)\n        ll_markdown2json = LLMMarkdown2Json(\n            md_text[\"output_file\"],\n            llm_model_name=\"gpt-3.5-turbo\",\n        )\n        logger.info(\"Cleaning the markdown file\")\n        ll_markdown2json.clean_markdown()\n\n</code></pre>"},{"location":"examples/parser/web/web_process/","title":"Web process","text":""},{"location":"examples/parser/web/web_process/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.parser.web.web2images import Web2Images\nfrom Docs2KG.parser.web.web2markdown import Web2Markdown\nfrom Docs2KG.parser.web.web2tables import Web2Tables\nfrom Docs2KG.parser.web.web2urls import Web2URLs\n\nif __name__ == \"__main__\":\n    url = \"https://abs.gov.au/census/find-census-data/quickstats/2021/LGA57080\"\n    web_2_md = Web2Markdown(url=url)\n    web_2_images = Web2Images(url=url)\n    web_2_tables = Web2Tables(url=url)\n    web_2_urls = Web2URLs(url=url)\n\n    web_2_md.convert2markdown()\n    web_2_images.extract2images()\n    web_2_tables.extract2tables()\n    web_2_urls.extract2tables()\n\n</code></pre>"},{"location":"examples/rag/neo4j_retrieval/","title":"Neo4j retrieval","text":""},{"location":"examples/rag/neo4j_retrieval/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.rag.neo4j_rag import Neo4jRAG\nfrom Docs2KG.utils.get_logger import get_logger\n\nlogger = get_logger(__name__)\n\nif __name__ == \"__main__\":\n    uri = \"bolt://localhost:7687\"\n    username = \"neo4j\"\n    password = \"testpassword\"\n\n    rag = Neo4jRAG(uri, username, password)\n    query = (\n        \"Can you show me all documents and their components\"\n        \"related to events that occurred in the years 2011 and 2021?\"\n    )\n    result = rag.retrieval(query)\n    logger.info(result)\n\n    rag.retrieval_strategy_hops_away(\n        uuids=result[\"top_k_content\"],\n        hops=3,\n    )\n\n</code></pre>"},{"location":"examples/rag/neo4j_vector/","title":"Neo4j vector","text":""},{"location":"examples/rag/neo4j_vector/#code-example","title":"Code Example","text":"<pre><code>from Docs2KG.rag.neo4j_vector import Neo4jVector\n\nif __name__ == \"__main__\":\n    uri = \"bolt://localhost:7687\"\n    username = \"neo4j\"\n    password = \"testpassword\"\n\n    neo4j_vector = Neo4jVector(uri, username, password)\n    neo4j_vector.add_embedding()\n    neo4j_vector.close()\n\n</code></pre>"},{"location":"sources/kg/constants/","title":"Constants","text":""},{"location":"sources/kg/docs_linkage/","title":"Docs linkage","text":""},{"location":"sources/kg/docs_linkage/#Docs2KG.kg.docs_linkage.logger","title":"<code>logger = get_logger(__name__)</code>  <code>module-attribute</code>","text":"<p>Link documents within the same knowledge graph based on - Temporal information - Semantic information</p>"},{"location":"sources/kg/docs_linkage/#Docs2KG.kg.docs_linkage.DocsLinkage","title":"<code>DocsLinkage</code>","text":"Source code in <code>Docs2KG/kg/docs_linkage.py</code> <pre><code>class DocsLinkage:\n    def __init__(self, nodes_json: Path):\n        \"\"\"\n        We will need to borrow LLM to do this.\n\n        So the input should be a json file with a list of nodes.\n\n        ```JSON\n            [\n                {\n                \"uuid\": \"0361cd85-c990-4060-a739-70bfdac317b8\",\n                \"labels\": [\n                    \"DOCUMENT\"\n                ],\n                \"properties\": {\n                    \"format\": \"PDF 1.3\",\n                    \"title\": \"Microsoft Word - WR_C19_2000_2006A.doc\",\n                    \"author\": \"michelle\",\n                    \"subject\": \"\",\n                    \"keywords\": \"\",\n                    \"creator\": \"PScript5.dll Version 5.2\",\n                    \"producer\": \"GNU Ghostscript 7.06\",\n                    \"creationDate\": \"2/7/2006 16:39:28\",\n                    \"modDate\": \"\",\n                    \"trapped\": \"\",\n                    \"encryption\": null,\n                    \"text_token\": 25959,\n                    \"estimated_price_gpt35\": 0.103836,\n                    \"estimated_price_gpt4o\": 1.03836,\n                    \"estimated_price_4_turbo\": 2.07672,\n                    \"file_path\": \"/Users/pascal/PhD/Docs2KG/data/input/tests_pdf/4.pdf\",\n                    \"scanned_or_exported\": \"exported\"\n                    }\n                },\n                ...\n            ]\n        ```\n\n        Then we will rely on the LLM to determine the links between the documents.\n        \"\"\"\n        self.nodes_json = nodes_json\n        self.nodes_df = pd.read_json(nodes_json)\n        self.nodes_json = json.loads(open(nodes_json).read())\n        self.nodes_json_str = json.dumps(self.nodes_json)\n        # if len of nodes_df is less than 2, we cannot link documents\n        if len(self.nodes_df) &lt; 2:\n            raise ValueError(\"There should be at least 2 documents to link\")\n        self.cost = 0\n\n    def openai_link_docs(self) -&gt; List:\n        \"\"\"\n        Link documents based on the LLM model.\n        Returns:\n\n        \"\"\"\n        logger.info(self.nodes_json)\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"Link documents if they have obvious relationships,\n                                  for example temporal or semantic relationships.\n                                \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                                Link the below array of nodes within a knowledge graph.\n\n                                If they have obvious relationships, for example temporal or semantic relationships.\n\n                                Create a relationship between the nodes.\n\n                                Return a list in JSON under the format:\n\n                                [{{\n                                    \"source_node_uuid\": |uuid of the source node|,\n                                    \"target_node_uuid\": |uuid of the target node|,\n                                    \"relationship\": |description of the relationship|\n                                }}]\n\n                                Then given nodes are:\n                                {self.nodes_json_str}\n                                \"\"\",\n                },\n            ]\n            openai_response, cost = openai_call(messages)\n            self.cost += cost\n            logger.info(openai_response)\n            return self.extract_links(openai_response)\n        except Exception as e:\n            logger.exception(e)\n\n    @staticmethod\n    def extract_links(openai_response: str):\n        \"\"\"\n\n        Args:\n            openai_response (str): OpenAI response\n\n        Returns:\n\n        \"\"\"\n        try:\n            rels = []\n            openai_response = json.loads(openai_response)\n            # if not list of links, transform to list\n            if not isinstance(openai_response, list):\n                openai_response = [openai_response]\n            for link in openai_response:\n                logger.info(link)\n                source_node_uuid = link.get(\"source_node_uuid\", None)\n                target_node_uuid = link.get(\"target_node_uuid\", None)\n                relationship = link.get(\"relationship\", None)\n                logger.info(f\"Source node: {source_node_uuid}\")\n                logger.info(f\"Target node: {target_node_uuid}\")\n                logger.info(f\"Relationship: {relationship}\")\n                if any(\n                    [\n                        source_node_uuid is None,\n                        target_node_uuid is None,\n                        relationship is None,\n                    ]\n                ):\n                    logger.error(\n                        \"Missing source_node_uuid, target_node_uuid or relationship\"\n                    )\n                    continue\n                rels = {\n                    \"start_node\": source_node_uuid,\n                    \"end_note\": target_node_uuid,\n                    \"relationship\": \"DOCUMENT_LINK\",\n                }\n            return rels\n\n        except Exception as e:\n            logger.exception(e)\n</code></pre>"},{"location":"sources/kg/docs_linkage/#Docs2KG.kg.docs_linkage.DocsLinkage.__init__","title":"<code>__init__(nodes_json)</code>","text":"<p>We will need to borrow LLM to do this.</p> <p>So the input should be a json file with a list of nodes.</p> <pre><code>    [\n        {\n        \"uuid\": \"0361cd85-c990-4060-a739-70bfdac317b8\",\n        \"labels\": [\n            \"DOCUMENT\"\n        ],\n        \"properties\": {\n            \"format\": \"PDF 1.3\",\n            \"title\": \"Microsoft Word - WR_C19_2000_2006A.doc\",\n            \"author\": \"michelle\",\n            \"subject\": \"\",\n            \"keywords\": \"\",\n            \"creator\": \"PScript5.dll Version 5.2\",\n            \"producer\": \"GNU Ghostscript 7.06\",\n            \"creationDate\": \"2/7/2006 16:39:28\",\n            \"modDate\": \"\",\n            \"trapped\": \"\",\n            \"encryption\": null,\n            \"text_token\": 25959,\n            \"estimated_price_gpt35\": 0.103836,\n            \"estimated_price_gpt4o\": 1.03836,\n            \"estimated_price_4_turbo\": 2.07672,\n            \"file_path\": \"/Users/pascal/PhD/Docs2KG/data/input/tests_pdf/4.pdf\",\n            \"scanned_or_exported\": \"exported\"\n            }\n        },\n        ...\n    ]\n</code></pre> <p>Then we will rely on the LLM to determine the links between the documents.</p> Source code in <code>Docs2KG/kg/docs_linkage.py</code> <pre><code>def __init__(self, nodes_json: Path):\n    \"\"\"\n    We will need to borrow LLM to do this.\n\n    So the input should be a json file with a list of nodes.\n\n    ```JSON\n        [\n            {\n            \"uuid\": \"0361cd85-c990-4060-a739-70bfdac317b8\",\n            \"labels\": [\n                \"DOCUMENT\"\n            ],\n            \"properties\": {\n                \"format\": \"PDF 1.3\",\n                \"title\": \"Microsoft Word - WR_C19_2000_2006A.doc\",\n                \"author\": \"michelle\",\n                \"subject\": \"\",\n                \"keywords\": \"\",\n                \"creator\": \"PScript5.dll Version 5.2\",\n                \"producer\": \"GNU Ghostscript 7.06\",\n                \"creationDate\": \"2/7/2006 16:39:28\",\n                \"modDate\": \"\",\n                \"trapped\": \"\",\n                \"encryption\": null,\n                \"text_token\": 25959,\n                \"estimated_price_gpt35\": 0.103836,\n                \"estimated_price_gpt4o\": 1.03836,\n                \"estimated_price_4_turbo\": 2.07672,\n                \"file_path\": \"/Users/pascal/PhD/Docs2KG/data/input/tests_pdf/4.pdf\",\n                \"scanned_or_exported\": \"exported\"\n                }\n            },\n            ...\n        ]\n    ```\n\n    Then we will rely on the LLM to determine the links between the documents.\n    \"\"\"\n    self.nodes_json = nodes_json\n    self.nodes_df = pd.read_json(nodes_json)\n    self.nodes_json = json.loads(open(nodes_json).read())\n    self.nodes_json_str = json.dumps(self.nodes_json)\n    # if len of nodes_df is less than 2, we cannot link documents\n    if len(self.nodes_df) &lt; 2:\n        raise ValueError(\"There should be at least 2 documents to link\")\n    self.cost = 0\n</code></pre>"},{"location":"sources/kg/docs_linkage/#Docs2KG.kg.docs_linkage.DocsLinkage.extract_links","title":"<code>extract_links(openai_response)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>openai_response</code> <code>str</code> <p>OpenAI response</p> required <p>Returns:</p> Source code in <code>Docs2KG/kg/docs_linkage.py</code> <pre><code>@staticmethod\ndef extract_links(openai_response: str):\n    \"\"\"\n\n    Args:\n        openai_response (str): OpenAI response\n\n    Returns:\n\n    \"\"\"\n    try:\n        rels = []\n        openai_response = json.loads(openai_response)\n        # if not list of links, transform to list\n        if not isinstance(openai_response, list):\n            openai_response = [openai_response]\n        for link in openai_response:\n            logger.info(link)\n            source_node_uuid = link.get(\"source_node_uuid\", None)\n            target_node_uuid = link.get(\"target_node_uuid\", None)\n            relationship = link.get(\"relationship\", None)\n            logger.info(f\"Source node: {source_node_uuid}\")\n            logger.info(f\"Target node: {target_node_uuid}\")\n            logger.info(f\"Relationship: {relationship}\")\n            if any(\n                [\n                    source_node_uuid is None,\n                    target_node_uuid is None,\n                    relationship is None,\n                ]\n            ):\n                logger.error(\n                    \"Missing source_node_uuid, target_node_uuid or relationship\"\n                )\n                continue\n            rels = {\n                \"start_node\": source_node_uuid,\n                \"end_note\": target_node_uuid,\n                \"relationship\": \"DOCUMENT_LINK\",\n            }\n        return rels\n\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"sources/kg/docs_linkage/#Docs2KG.kg.docs_linkage.DocsLinkage.openai_link_docs","title":"<code>openai_link_docs()</code>","text":"<p>Link documents based on the LLM model. Returns:</p> Source code in <code>Docs2KG/kg/docs_linkage.py</code> <pre><code>def openai_link_docs(self) -&gt; List:\n    \"\"\"\n    Link documents based on the LLM model.\n    Returns:\n\n    \"\"\"\n    logger.info(self.nodes_json)\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Link documents if they have obvious relationships,\n                              for example temporal or semantic relationships.\n                            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                            Link the below array of nodes within a knowledge graph.\n\n                            If they have obvious relationships, for example temporal or semantic relationships.\n\n                            Create a relationship between the nodes.\n\n                            Return a list in JSON under the format:\n\n                            [{{\n                                \"source_node_uuid\": |uuid of the source node|,\n                                \"target_node_uuid\": |uuid of the target node|,\n                                \"relationship\": |description of the relationship|\n                            }}]\n\n                            Then given nodes are:\n                            {self.nodes_json_str}\n                            \"\"\",\n            },\n        ]\n        openai_response, cost = openai_call(messages)\n        self.cost += cost\n        logger.info(openai_response)\n        return self.extract_links(openai_response)\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"sources/kg/dynamic_schema/","title":"Dynamic schema","text":""},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema","title":"<code>DynamicSchema</code>","text":"<p>For the unified knowledge graph, especially the semantic part, the schema is dynamic.</p> <p>So which will require two things:</p> <ul> <li>From top-down methodological perspective, we can use ontology based way to implement the schema.<ul> <li>However, it will require quite a lot of pre-work before we can embrace the usage of LLM</li> </ul> </li> <li>So we use it from another perspective, which is bottom-up.<ul> <li>We will have the defined schema first, and then merge the schema</li> <li>The merge process will include two parts:<ul> <li>Machine based, automatic merge<ul> <li>Frequency based merge</li> <li>Similarity based merge</li> <li>Other strategies</li> </ul> </li> <li>Human based, manual merge</li> </ul> </li> </ul> </li> </ul> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>class DynamicSchema:\n    \"\"\"\n    For the unified knowledge graph, especially the semantic part, the schema is dynamic.\n\n    So which will require two things:\n\n    - From top-down methodological perspective, we can use ontology based way to implement the schema.\n        - However, it will require quite a lot of pre-work before we can embrace the usage of LLM\n    - So we use it from another perspective, which is bottom-up.\n        - We will have the defined schema first, and then merge the schema\n        - The merge process will include two parts:\n            - Machine based, automatic merge\n                - Frequency based merge\n                - Similarity based merge\n                - Other strategies\n            - Human based, manual merge\n\n    \"\"\"\n\n    def __init__(\n        self, kg_json_file: Path, merge_freq: int = 10, merge_similarity: float = 0.98\n    ):\n        \"\"\"\n        Initialize the dynamic schema class\n        Args:\n            kg_json_file (Path): The path of the knowledge graph json file\n            merge_freq (int): The frequency of the label, if it is lower than this, we will ignore it\n            merge_similarity (float): The similarity threshold for the merge\n\n        Returns:\n\n        \"\"\"\n        self.kg_json_file = kg_json_file\n        self.kg_json = json.load(kg_json_file.open())\n        self.merge_freq = merge_freq\n        self.merge_similarity = merge_similarity\n        self.nodes_freq = {}\n        # use bert to calculate the similarity\n        self.similarity_model = None\n        self.tokenizer = None\n\n    def schema_extraction(self):\n        \"\"\"\n        Extract the schema from the knowledge graph\n        \"\"\"\n        nodes = self.kg_json[\"nodes\"]\n\n        node_df = pd.DataFrame(nodes)\n        # extract the unique labels and its occurrence (labels is a list field)\n        unique_labels = node_df[\"labels\"].explode().value_counts()\n        logger.info(f\"Unique labels: {unique_labels}\")\n        self.nodes_freq = unique_labels.to_dict()\n\n    def schema_freq_merge(self) -&gt; dict:\n        \"\"\"\n        Replace the label under the threshold into text_block label\n\n        Returns:\n            merge_mapping (dict): The mapping of the merge, key is the original label, value is the new label\n        \"\"\"\n\n        # for the one with lower occurrence, we can ignore them\n        merge_mapping = {}\n        for key, value in self.nodes_freq.items():\n            if key.lower() in HTML_TAGS:\n                continue\n            if value &lt; self.merge_freq:\n                merge_mapping[key] = \"text_block\"\n        logger.debug(f\"Merge mapping: {merge_mapping} based on frequency\")\n        return merge_mapping\n\n    def schema_similarity_merge(self) -&gt; dict:\n        \"\"\"\n        Merge the schema based on the similarity\n\n        Returns:\n            merge_mapping (dict): The mapping of the merge, key is the original label, value is the new label\n        \"\"\"\n        merge_mapping = {}\n        # calculate the pairwise similarity for all the labels using the sentence transformer\n        # for the one with high similarity, we can merge them\n        # so, we should first construct a 2D matrix\n        if self.similarity_model is None:\n            # use bert to calculate the key similarity\n            self.similarity_model = BertModel.from_pretrained(\"bert-base-uncased\")\n            self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n        def encode_label(label):\n            inputs = self.tokenizer(label, return_tensors=\"pt\")\n            with torch.no_grad():\n                outputs = self.similarity_model(**inputs)\n            return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n\n        labels = list(self.nodes_freq.keys())\n        logger.debug(labels)\n        label_matrix = [encode_label([label]) for label in labels]\n        label_matrix = np.array(label_matrix)\n\n        for i in range(len(labels)):\n            for j in range(len(labels)):\n                if i == j:\n                    continue\n                if (\n                    cosine_similarity(\n                        label_matrix[i].reshape(1, -1), label_matrix[j].reshape(1, -1)\n                    )\n                    &gt; self.merge_similarity\n                ):\n                    if labels[i].lower() in HTML_TAGS or labels[j].lower() in HTML_TAGS:\n                        continue\n                    merge_mapping[labels[i]] = labels[j]\n\n        # then we can calculate the similarity\n        logger.info(f\"Merge mapping: {merge_mapping} based on similarity\")\n        return merge_mapping\n\n    def human_in_the_loop_input(self) -&gt; dict:\n        \"\"\"\n        Convert the schema into the dict\n        {\n            key: number of occurrence\n            ...\n        }\n\n        Then human will do the decision based on the value, to do the mapping\n        \"\"\"\n        logger.info(f\"Schema: {self.nodes_freq}\")\n        return self.nodes_freq\n</code></pre>"},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema.__init__","title":"<code>__init__(kg_json_file, merge_freq=10, merge_similarity=0.98)</code>","text":"<p>Initialize the dynamic schema class Args:     kg_json_file (Path): The path of the knowledge graph json file     merge_freq (int): The frequency of the label, if it is lower than this, we will ignore it     merge_similarity (float): The similarity threshold for the merge</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>def __init__(\n    self, kg_json_file: Path, merge_freq: int = 10, merge_similarity: float = 0.98\n):\n    \"\"\"\n    Initialize the dynamic schema class\n    Args:\n        kg_json_file (Path): The path of the knowledge graph json file\n        merge_freq (int): The frequency of the label, if it is lower than this, we will ignore it\n        merge_similarity (float): The similarity threshold for the merge\n\n    Returns:\n\n    \"\"\"\n    self.kg_json_file = kg_json_file\n    self.kg_json = json.load(kg_json_file.open())\n    self.merge_freq = merge_freq\n    self.merge_similarity = merge_similarity\n    self.nodes_freq = {}\n    # use bert to calculate the similarity\n    self.similarity_model = None\n    self.tokenizer = None\n</code></pre>"},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema.human_in_the_loop_input","title":"<code>human_in_the_loop_input()</code>","text":"<p>Convert the schema into the dict {     key: number of occurrence     ... }</p> <p>Then human will do the decision based on the value, to do the mapping</p> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>def human_in_the_loop_input(self) -&gt; dict:\n    \"\"\"\n    Convert the schema into the dict\n    {\n        key: number of occurrence\n        ...\n    }\n\n    Then human will do the decision based on the value, to do the mapping\n    \"\"\"\n    logger.info(f\"Schema: {self.nodes_freq}\")\n    return self.nodes_freq\n</code></pre>"},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema.schema_extraction","title":"<code>schema_extraction()</code>","text":"<p>Extract the schema from the knowledge graph</p> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>def schema_extraction(self):\n    \"\"\"\n    Extract the schema from the knowledge graph\n    \"\"\"\n    nodes = self.kg_json[\"nodes\"]\n\n    node_df = pd.DataFrame(nodes)\n    # extract the unique labels and its occurrence (labels is a list field)\n    unique_labels = node_df[\"labels\"].explode().value_counts()\n    logger.info(f\"Unique labels: {unique_labels}\")\n    self.nodes_freq = unique_labels.to_dict()\n</code></pre>"},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema.schema_freq_merge","title":"<code>schema_freq_merge()</code>","text":"<p>Replace the label under the threshold into text_block label</p> <p>Returns:</p> Name Type Description <code>merge_mapping</code> <code>dict</code> <p>The mapping of the merge, key is the original label, value is the new label</p> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>def schema_freq_merge(self) -&gt; dict:\n    \"\"\"\n    Replace the label under the threshold into text_block label\n\n    Returns:\n        merge_mapping (dict): The mapping of the merge, key is the original label, value is the new label\n    \"\"\"\n\n    # for the one with lower occurrence, we can ignore them\n    merge_mapping = {}\n    for key, value in self.nodes_freq.items():\n        if key.lower() in HTML_TAGS:\n            continue\n        if value &lt; self.merge_freq:\n            merge_mapping[key] = \"text_block\"\n    logger.debug(f\"Merge mapping: {merge_mapping} based on frequency\")\n    return merge_mapping\n</code></pre>"},{"location":"sources/kg/dynamic_schema/#Docs2KG.kg.dynamic_schema.DynamicSchema.schema_similarity_merge","title":"<code>schema_similarity_merge()</code>","text":"<p>Merge the schema based on the similarity</p> <p>Returns:</p> Name Type Description <code>merge_mapping</code> <code>dict</code> <p>The mapping of the merge, key is the original label, value is the new label</p> Source code in <code>Docs2KG/kg/dynamic_schema.py</code> <pre><code>def schema_similarity_merge(self) -&gt; dict:\n    \"\"\"\n    Merge the schema based on the similarity\n\n    Returns:\n        merge_mapping (dict): The mapping of the merge, key is the original label, value is the new label\n    \"\"\"\n    merge_mapping = {}\n    # calculate the pairwise similarity for all the labels using the sentence transformer\n    # for the one with high similarity, we can merge them\n    # so, we should first construct a 2D matrix\n    if self.similarity_model is None:\n        # use bert to calculate the key similarity\n        self.similarity_model = BertModel.from_pretrained(\"bert-base-uncased\")\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    def encode_label(label):\n        inputs = self.tokenizer(label, return_tensors=\"pt\")\n        with torch.no_grad():\n            outputs = self.similarity_model(**inputs)\n        return outputs.last_hidden_state[:, 0, :].numpy().flatten()\n\n    labels = list(self.nodes_freq.keys())\n    logger.debug(labels)\n    label_matrix = [encode_label([label]) for label in labels]\n    label_matrix = np.array(label_matrix)\n\n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            if i == j:\n                continue\n            if (\n                cosine_similarity(\n                    label_matrix[i].reshape(1, -1), label_matrix[j].reshape(1, -1)\n                )\n                &gt; self.merge_similarity\n            ):\n                if labels[i].lower() in HTML_TAGS or labels[j].lower() in HTML_TAGS:\n                    continue\n                merge_mapping[labels[i]] = labels[j]\n\n    # then we can calculate the similarity\n    logger.info(f\"Merge mapping: {merge_mapping} based on similarity\")\n    return merge_mapping\n</code></pre>"},{"location":"sources/kg/email_layout_kg/","title":"Email layout kg","text":""},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.logger","title":"<code>logger = get_logger(__name__)</code>  <code>module-attribute</code>","text":"<p>TODO:</p> <ul> <li>Try to extract the image and file captions</li> </ul>"},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.EmailLayoutKG","title":"<code>EmailLayoutKG</code>","text":"Source code in <code>Docs2KG/kg/email_layout_kg.py</code> <pre><code>class EmailLayoutKG:\n    def __init__(self, output_dir: Path = None) -&gt; None:\n        \"\"\"\n        Initialize the WebParserBase class\n\n        Args:\n            output_dir (Path): Path to the output directory where the converted files will be saved\n\n        \"\"\"\n\n        self.output_dir = output_dir\n\n        self.kg_json = {}\n        self.kg_folder = self.output_dir / \"kg\"\n        self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n        # image and table output directories\n        self.image_output_dir = self.output_dir / \"images\"\n        self.image_output_dir.mkdir(parents=True, exist_ok=True)\n        # attachment output directories\n        self.attachment_output_dir = self.output_dir / \"attachments\"\n        self.attachment_output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.images_df = pd.read_csv(f\"{self.image_output_dir}/images.csv\")\n        self.attachments_df = pd.read_csv(\n            f\"{self.attachment_output_dir}/attachments.csv\"\n        )\n\n    def create_kg(self):\n        \"\"\"\n        Create the knowledge graph from the HTML file\n\n        \"\"\"\n        with open(f\"{self.output_dir}/email.html\", \"r\") as f:\n            html_content = f.read()\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        \"\"\"\n        Loop and extract the whole soup into a tree\n        Each node will have\n\n        ```\n        {\n            \"uuid\": str,\n            \"node_type\": str,\n            \"node_properties\": {\n                \"content\": str,\n                // all other stuff\n            },\n            \"children\": List[Node]\n        }\n        ```\n        \"\"\"\n        self.kg_json = self.extract_kg(soup)\n        self.export_kg()\n\n    def extract_kg(self, soup):\n        \"\"\"\n        Extract the knowledge graph from the HTML file\n\n        Args:\n            soup (BeautifulSoup): Parsed HTML content\n\n        Returns:\n            dict: Knowledge graph in JSON format\n\n        \"\"\"\n        node = {\n            \"uuid\": str(uuid4()),\n            \"children\": [],\n        }\n\n        for child in soup.children:\n            if child.name is not None and soup.name != \"table\":\n                child_node = self.extract_kg(child)\n                node[\"children\"].append(child_node)\n        # content should be text if exists, if not, leave \"\"\n        content = str(soup.text) if soup.text is not None else \"\"\n        content = content.strip()\n        logger.info(content)\n        logger.info(soup.name)\n        # if there is no parent, then it is the root node, which we call it document\n        node_type = str(soup.name) if soup.name is not None else \"text\"\n        if \"document\" in node_type:\n            node_type = \"email\"\n        node[\"node_type\"] = node_type\n        soup_attr = soup.attrs\n        copied_soup = deepcopy(soup_attr)\n        for key in copied_soup.keys():\n            if \"-\" in key:\n                soup_attr[key.replace(\"-\", \"_\")] = copied_soup[key]\n                del soup_attr[key]\n            if \"$\" in key or \":\" in key:\n                del soup_attr[key]\n        node[\"node_properties\"] = {\"content\": content, **soup_attr}\n        # if it is an image tag, then extract the image and save it to the output directory\n        if soup.name == \"img\":\n            img_url = soup.get(\"src\")\n\n            if img_url.startswith(\"cid:\"):\n                image_cid = img_url.split(\":\")[1]\n                logger.info(image_cid)\n                image_file_path = self.images_df[\n                    self.images_df[\"cid\"] == f\"&lt;{image_cid}&gt;\"\n                ][\"path\"].values[0]\n                logger.info(image_file_path)\n                node[\"node_properties\"][\"img_path\"] = image_file_path\n            else:\n                img_data = requests.get(img_url).content\n                img_name = img_url.split(\"/\")[-1]\n                logger.info(\"image_url\")\n                logger.info(img_url)\n                if \"?\" in img_name:\n                    img_name = img_name.split(\"?\")[0]\n                with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n                    f.write(img_data)\n                logger.info(f\"Extracted the HTML file from {img_url} to images\")\n                node[\"node_properties\"][\n                    \"img_path\"\n                ] = f\"{self.output_dir}/images/{img_name}\"\n        # if it is a table tag, then extract the table and save it to the output directory\n        if soup.name == \"table\":\n            rows = []\n            for row in soup.find_all(\"tr\"):\n                cells = [\n                    cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n                ]\n                rows.append(cells)\n            df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n            csv_filename = f\"{self.output_dir}/tables/{node['uuid']}.csv\"\n            df.to_csv(csv_filename, index=False)\n            logger.info(\"Extracted the HTML file from to tables\")\n            node[\"node_properties\"][\"table_path\"] = csv_filename\n        # remove the node from soup after extracting the image and table\n        soup.extract()\n\n        if node_type == \"email\":\n            # also add the metadata to the node properties\n            with open(f\"{self.output_dir}/metadata.json\", \"r\") as f:\n                metadata = json.load(f)\n            node[\"node_properties\"] = {**node[\"node_properties\"], **metadata}\n\n            # add all the attachments to children\n            for _, attachment in self.attachments_df.iterrows():\n                attachment_node = {\n                    \"uuid\": str(uuid4()),\n                    \"node_type\": \"attachment\",\n                    \"node_properties\": attachment.to_dict(),\n                    \"children\": [],\n                }\n                node[\"children\"].append(attachment_node)\n        return node\n\n    def export_kg(self) -&gt; None:\n        \"\"\"\n        Export the knowledge graph to json file\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n            json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.EmailLayoutKG.__init__","title":"<code>__init__(output_dir=None)</code>","text":"<p>Initialize the WebParserBase class</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path</code> <p>Path to the output directory where the converted files will be saved</p> <code>None</code> Source code in <code>Docs2KG/kg/email_layout_kg.py</code> <pre><code>def __init__(self, output_dir: Path = None) -&gt; None:\n    \"\"\"\n    Initialize the WebParserBase class\n\n    Args:\n        output_dir (Path): Path to the output directory where the converted files will be saved\n\n    \"\"\"\n\n    self.output_dir = output_dir\n\n    self.kg_json = {}\n    self.kg_folder = self.output_dir / \"kg\"\n    self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n    # image and table output directories\n    self.image_output_dir = self.output_dir / \"images\"\n    self.image_output_dir.mkdir(parents=True, exist_ok=True)\n    # attachment output directories\n    self.attachment_output_dir = self.output_dir / \"attachments\"\n    self.attachment_output_dir.mkdir(parents=True, exist_ok=True)\n\n    self.images_df = pd.read_csv(f\"{self.image_output_dir}/images.csv\")\n    self.attachments_df = pd.read_csv(\n        f\"{self.attachment_output_dir}/attachments.csv\"\n    )\n</code></pre>"},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.EmailLayoutKG.create_kg","title":"<code>create_kg()</code>","text":"<p>Create the knowledge graph from the HTML file</p> Source code in <code>Docs2KG/kg/email_layout_kg.py</code> <pre><code>def create_kg(self):\n    \"\"\"\n    Create the knowledge graph from the HTML file\n\n    \"\"\"\n    with open(f\"{self.output_dir}/email.html\", \"r\") as f:\n        html_content = f.read()\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    \"\"\"\n    Loop and extract the whole soup into a tree\n    Each node will have\n\n    ```\n    {\n        \"uuid\": str,\n        \"node_type\": str,\n        \"node_properties\": {\n            \"content\": str,\n            // all other stuff\n        },\n        \"children\": List[Node]\n    }\n    ```\n    \"\"\"\n    self.kg_json = self.extract_kg(soup)\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.EmailLayoutKG.export_kg","title":"<code>export_kg()</code>","text":"<p>Export the knowledge graph to json file</p> Source code in <code>Docs2KG/kg/email_layout_kg.py</code> <pre><code>def export_kg(self) -&gt; None:\n    \"\"\"\n    Export the knowledge graph to json file\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n        json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/email_layout_kg/#Docs2KG.kg.email_layout_kg.EmailLayoutKG.extract_kg","title":"<code>extract_kg(soup)</code>","text":"<p>Extract the knowledge graph from the HTML file</p> <p>Parameters:</p> Name Type Description Default <code>soup</code> <code>BeautifulSoup</code> <p>Parsed HTML content</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Knowledge graph in JSON format</p> Source code in <code>Docs2KG/kg/email_layout_kg.py</code> <pre><code>def extract_kg(self, soup):\n    \"\"\"\n    Extract the knowledge graph from the HTML file\n\n    Args:\n        soup (BeautifulSoup): Parsed HTML content\n\n    Returns:\n        dict: Knowledge graph in JSON format\n\n    \"\"\"\n    node = {\n        \"uuid\": str(uuid4()),\n        \"children\": [],\n    }\n\n    for child in soup.children:\n        if child.name is not None and soup.name != \"table\":\n            child_node = self.extract_kg(child)\n            node[\"children\"].append(child_node)\n    # content should be text if exists, if not, leave \"\"\n    content = str(soup.text) if soup.text is not None else \"\"\n    content = content.strip()\n    logger.info(content)\n    logger.info(soup.name)\n    # if there is no parent, then it is the root node, which we call it document\n    node_type = str(soup.name) if soup.name is not None else \"text\"\n    if \"document\" in node_type:\n        node_type = \"email\"\n    node[\"node_type\"] = node_type\n    soup_attr = soup.attrs\n    copied_soup = deepcopy(soup_attr)\n    for key in copied_soup.keys():\n        if \"-\" in key:\n            soup_attr[key.replace(\"-\", \"_\")] = copied_soup[key]\n            del soup_attr[key]\n        if \"$\" in key or \":\" in key:\n            del soup_attr[key]\n    node[\"node_properties\"] = {\"content\": content, **soup_attr}\n    # if it is an image tag, then extract the image and save it to the output directory\n    if soup.name == \"img\":\n        img_url = soup.get(\"src\")\n\n        if img_url.startswith(\"cid:\"):\n            image_cid = img_url.split(\":\")[1]\n            logger.info(image_cid)\n            image_file_path = self.images_df[\n                self.images_df[\"cid\"] == f\"&lt;{image_cid}&gt;\"\n            ][\"path\"].values[0]\n            logger.info(image_file_path)\n            node[\"node_properties\"][\"img_path\"] = image_file_path\n        else:\n            img_data = requests.get(img_url).content\n            img_name = img_url.split(\"/\")[-1]\n            logger.info(\"image_url\")\n            logger.info(img_url)\n            if \"?\" in img_name:\n                img_name = img_name.split(\"?\")[0]\n            with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n                f.write(img_data)\n            logger.info(f\"Extracted the HTML file from {img_url} to images\")\n            node[\"node_properties\"][\n                \"img_path\"\n            ] = f\"{self.output_dir}/images/{img_name}\"\n    # if it is a table tag, then extract the table and save it to the output directory\n    if soup.name == \"table\":\n        rows = []\n        for row in soup.find_all(\"tr\"):\n            cells = [\n                cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n            ]\n            rows.append(cells)\n        df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n        csv_filename = f\"{self.output_dir}/tables/{node['uuid']}.csv\"\n        df.to_csv(csv_filename, index=False)\n        logger.info(\"Extracted the HTML file from to tables\")\n        node[\"node_properties\"][\"table_path\"] = csv_filename\n    # remove the node from soup after extracting the image and table\n    soup.extract()\n\n    if node_type == \"email\":\n        # also add the metadata to the node properties\n        with open(f\"{self.output_dir}/metadata.json\", \"r\") as f:\n            metadata = json.load(f)\n        node[\"node_properties\"] = {**node[\"node_properties\"], **metadata}\n\n        # add all the attachments to children\n        for _, attachment in self.attachments_df.iterrows():\n            attachment_node = {\n                \"uuid\": str(uuid4()),\n                \"node_type\": \"attachment\",\n                \"node_properties\": attachment.to_dict(),\n                \"children\": [],\n            }\n            node[\"children\"].append(attachment_node)\n    return node\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/","title":"Excel layout kg","text":""},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG","title":"<code>ExcelLayoutKG</code>","text":"<p>Layout Knowledge Graph For each excel, each sheet will be a node (same a page in pdf) Then we will have the images and tables connect to the sheet, also the summary and description will be added to the sheet node</p> Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>class ExcelLayoutKG:\n    \"\"\"\n    Layout Knowledge Graph\n    For each excel, each sheet will be a node (same a page in pdf)\n    Then we will have the images and tables connect to the sheet, also the\n    summary and description will be added to the sheet node\n\n    \"\"\"\n\n    def __init__(\n        self,\n        folder_path: Path,\n        input_format: str = \"pdf_exported\",\n    ):\n        \"\"\"\n        Initialize the class with the pdf file\n        The goal of this is to construct the layout knowledge graph\n\n        Args:\n            folder_path (Path): The path to the pdf file\n\n        \"\"\"\n        self.folder_path = folder_path\n        self.kg_folder = self.folder_path / \"kg\"\n        if not self.kg_folder.exists():\n            self.kg_folder.mkdir(parents=True, exist_ok=True)\n        self.kg_json = {}\n        self.metadata = json.load((self.folder_path / \"metadata.json\").open())\n        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        self.input_format = input_format\n\n    def create_kg(self):\n        \"\"\"\n        Create the layout knowledge graph\n        \"\"\"\n        self.document_kg()\n\n    def document_kg(self):\n        \"\"\"\n        Construct the layout knowledge graph skeleton first\n\n        We will require the md.json.csv file with the following columns:\n\n        - layout_json\n        \"\"\"\n        # 1. add the document node\n\n        self.kg_json = {\n            \"node_type\": \"excel\",\n            \"uuid\": str(uuid4()),\n            \"node_properties\": self.metadata,\n            \"children\": [],\n        }\n\n        # 2. add page nodes\n        pages_json = []\n        text_folder = self.folder_path / \"texts\"\n        md_json_csv = text_folder / \"md.json.csv\"\n        summary_and_desc_df = pd.read_csv(md_json_csv)\n        columns = summary_and_desc_df.columns.tolist()\n        logger.debug(f\"Columns: {columns}\")\n        # we will focus on the layout json\n        table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n        image_df = pd.read_csv(self.folder_path / \"images\" / \"images.csv\")\n\n        for index, row in summary_and_desc_df.iterrows():\n            logger.info(f\"Processing page_index {index}\")\n            try:\n                # get table_item\n                table_item = table_df[table_df[\"page_index\"] == index]\n                image_item = image_df[image_df[\"page_index\"] == index]\n                page_json = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": row[\"page_number\"],\n                        \"page_text\": row[\"text\"],\n                        \"sheet_name\": row[\"sheet_name\"],\n                        \"summary\": row[\"summary\"] if pd.notna(row[\"summary\"]) else \"\",\n                        \"content\": row[\"desc\"] if pd.notna(row[\"desc\"]) else \"\",\n                    },\n                    \"children\": [\n                        # we will have image node, table node\n                        {\n                            \"node_type\": \"table_csv\",\n                            \"uuid\": str(uuid4()),\n                            \"node_properties\": {\n                                \"table_path\": table_item[\"file_path\"].values[0],\n                                \"table_index\": int(table_item[\"table_index\"].values[0]),\n                            },\n                            \"children\": [],\n                        },\n                        {\n                            \"node_type\": \"image\",\n                            \"uuid\": str(uuid4()),\n                            \"node_properties\": {\n                                \"image_path\": image_item[\"file_path\"].values[0],\n                                \"filename\": image_item[\"filename\"].values[0],\n                            },\n                            \"children\": [],\n                        },\n                    ],\n                }\n                pages_json.append(page_json)\n            except Exception as e:\n                logger.error(f\"Error in row {index}: {e}\")\n\n                logger.exception(e)\n                # if this is an unhandled error\n                # we should still keep all data for this page, so we will construct a page with everything we have\n                page_json = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": row[\"page_number\"],\n                        \"page_text\": row[\"text\"],\n                        \"sheet_name\": row[\"sheet_name\"],\n                        \"summary\": row[\"summary\"] if pd.notna(row[\"summary\"]) else \"\",\n                        \"content\": row[\"desc\"] if pd.notna(row[\"desc\"]) else \"\",\n                    },\n                    \"children\": [],\n                }\n                pages_json.append(page_json)\n\n        self.kg_json[\"children\"] = pages_json\n        self.export_kg()\n\n    def export_kg(self) -&gt; None:\n        \"\"\"\n        Export the knowledge graph to json file\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n            json.dump(self.kg_json, f, indent=2)\n\n    def load_kg(self):\n        \"\"\"\n        Load the knowledge graph from JSON\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n            self.kg_json = json.load(f)\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG.__init__","title":"<code>__init__(folder_path, input_format='pdf_exported')</code>","text":"<p>Initialize the class with the pdf file The goal of this is to construct the layout knowledge graph</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>Path</code> <p>The path to the pdf file</p> required Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>def __init__(\n    self,\n    folder_path: Path,\n    input_format: str = \"pdf_exported\",\n):\n    \"\"\"\n    Initialize the class with the pdf file\n    The goal of this is to construct the layout knowledge graph\n\n    Args:\n        folder_path (Path): The path to the pdf file\n\n    \"\"\"\n    self.folder_path = folder_path\n    self.kg_folder = self.folder_path / \"kg\"\n    if not self.kg_folder.exists():\n        self.kg_folder.mkdir(parents=True, exist_ok=True)\n    self.kg_json = {}\n    self.metadata = json.load((self.folder_path / \"metadata.json\").open())\n    self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    self.input_format = input_format\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG.create_kg","title":"<code>create_kg()</code>","text":"<p>Create the layout knowledge graph</p> Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>def create_kg(self):\n    \"\"\"\n    Create the layout knowledge graph\n    \"\"\"\n    self.document_kg()\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG.document_kg","title":"<code>document_kg()</code>","text":"<p>Construct the layout knowledge graph skeleton first</p> <p>We will require the md.json.csv file with the following columns:</p> <ul> <li>layout_json</li> </ul> Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>def document_kg(self):\n    \"\"\"\n    Construct the layout knowledge graph skeleton first\n\n    We will require the md.json.csv file with the following columns:\n\n    - layout_json\n    \"\"\"\n    # 1. add the document node\n\n    self.kg_json = {\n        \"node_type\": \"excel\",\n        \"uuid\": str(uuid4()),\n        \"node_properties\": self.metadata,\n        \"children\": [],\n    }\n\n    # 2. add page nodes\n    pages_json = []\n    text_folder = self.folder_path / \"texts\"\n    md_json_csv = text_folder / \"md.json.csv\"\n    summary_and_desc_df = pd.read_csv(md_json_csv)\n    columns = summary_and_desc_df.columns.tolist()\n    logger.debug(f\"Columns: {columns}\")\n    # we will focus on the layout json\n    table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n    image_df = pd.read_csv(self.folder_path / \"images\" / \"images.csv\")\n\n    for index, row in summary_and_desc_df.iterrows():\n        logger.info(f\"Processing page_index {index}\")\n        try:\n            # get table_item\n            table_item = table_df[table_df[\"page_index\"] == index]\n            image_item = image_df[image_df[\"page_index\"] == index]\n            page_json = {\n                \"node_type\": \"page\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"page_number\": row[\"page_number\"],\n                    \"page_text\": row[\"text\"],\n                    \"sheet_name\": row[\"sheet_name\"],\n                    \"summary\": row[\"summary\"] if pd.notna(row[\"summary\"]) else \"\",\n                    \"content\": row[\"desc\"] if pd.notna(row[\"desc\"]) else \"\",\n                },\n                \"children\": [\n                    # we will have image node, table node\n                    {\n                        \"node_type\": \"table_csv\",\n                        \"uuid\": str(uuid4()),\n                        \"node_properties\": {\n                            \"table_path\": table_item[\"file_path\"].values[0],\n                            \"table_index\": int(table_item[\"table_index\"].values[0]),\n                        },\n                        \"children\": [],\n                    },\n                    {\n                        \"node_type\": \"image\",\n                        \"uuid\": str(uuid4()),\n                        \"node_properties\": {\n                            \"image_path\": image_item[\"file_path\"].values[0],\n                            \"filename\": image_item[\"filename\"].values[0],\n                        },\n                        \"children\": [],\n                    },\n                ],\n            }\n            pages_json.append(page_json)\n        except Exception as e:\n            logger.error(f\"Error in row {index}: {e}\")\n\n            logger.exception(e)\n            # if this is an unhandled error\n            # we should still keep all data for this page, so we will construct a page with everything we have\n            page_json = {\n                \"node_type\": \"page\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"page_number\": row[\"page_number\"],\n                    \"page_text\": row[\"text\"],\n                    \"sheet_name\": row[\"sheet_name\"],\n                    \"summary\": row[\"summary\"] if pd.notna(row[\"summary\"]) else \"\",\n                    \"content\": row[\"desc\"] if pd.notna(row[\"desc\"]) else \"\",\n                },\n                \"children\": [],\n            }\n            pages_json.append(page_json)\n\n    self.kg_json[\"children\"] = pages_json\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG.export_kg","title":"<code>export_kg()</code>","text":"<p>Export the knowledge graph to json file</p> Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>def export_kg(self) -&gt; None:\n    \"\"\"\n    Export the knowledge graph to json file\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n        json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/excel_layout_kg/#Docs2KG.kg.excel_layout_kg.ExcelLayoutKG.load_kg","title":"<code>load_kg()</code>","text":"<p>Load the knowledge graph from JSON</p> Source code in <code>Docs2KG/kg/excel_layout_kg.py</code> <pre><code>def load_kg(self):\n    \"\"\"\n    Load the knowledge graph from JSON\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n        self.kg_json = json.load(f)\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/","title":"Pdf layout kg","text":""},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG","title":"<code>PDFLayoutKG</code>","text":"<p>Layout Knowledge Graph</p> <p>This is for one pdf file</p> <p>What we will link in the layout knowledge graph:</p> <ul> <li>Document KG<ul> <li>Input is the Markdown JSON file</li> <li>The context order will be preserved within the Tree</li> </ul> </li> <li>Link Image to Page</li> <li>Link Table to Page</li> <li>Link Image to Context (Find Nearby Context, then Map back to the Tree)</li> <li>Link Table to Context (Same, Find Caption, Nearby Context)</li> </ul> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>class PDFLayoutKG:\n    \"\"\"\n    Layout Knowledge Graph\n\n    This is for one pdf file\n\n\n    What we will link in the layout knowledge graph:\n\n    - Document KG\n        - Input is the Markdown JSON file\n        - The context order will be preserved within the Tree\n    - Link Image to Page\n    - Link Table to Page\n    - Link Image to Context (Find Nearby Context, then Map back to the Tree)\n    - Link Table to Context (Same, Find Caption, Nearby Context)\n    \"\"\"\n\n    def __init__(\n        self,\n        folder_path: Path,\n        input_format: str = \"pdf_exported\",\n    ):\n        \"\"\"\n        Initialize the class with the pdf file\n\n\n        Args:\n            folder_path (Path): The path to the pdf file\n\n        \"\"\"\n        self.folder_path = folder_path\n        self.kg_folder = self.folder_path / \"kg\"\n        if not self.kg_folder.exists():\n            self.kg_folder.mkdir(parents=True, exist_ok=True)\n        self.kg_json = {}\n        self.metadata = json.load((self.folder_path / \"metadata.json\").open())\n        self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        self.input_format = input_format\n\n    def create_kg(self):\n        \"\"\"\n        Create the layout knowledge graph\n        \"\"\"\n        self.document_kg()\n        if self.input_format == \"pdf_exported\":\n            self.link_image_to_page()\n            self.link_table_to_page()\n            self.link_image_to_context()\n            self.link_table_to_context()\n        if self.input_format == \"pdf_scanned\":\n            # add page image\n            self.link_page_image_to_page()\n\n    def document_kg(self):\n        \"\"\"\n        Construct the layout knowledge graph skeleton first\n\n        We will require the md.json.csv file with the following columns:\n\n        - layout_json\n        \"\"\"\n        # 1. add the document node\n\n        self.kg_json = {\n            \"node_type\": \"document\",\n            \"uuid\": str(uuid4()),\n            \"node_properties\": self.metadata,\n            \"children\": [],\n        }\n\n        # 2. add page nodes\n        pages_json = []\n        text_folder = self.folder_path / \"texts\"\n        md_json_csv = text_folder / \"md.json.csv\"\n        texts_json_df = pd.read_csv(md_json_csv)\n        columns = texts_json_df.columns.tolist()\n        logger.debug(f\"Columns: {columns}\")\n        # we will focus on the layout json\n\n        for index, row in texts_json_df.iterrows():\n            logger.info(f\"Processing page_index {index}\")\n            logger.debug(row[\"layout_json\"])\n            try:\n                layout_json = json.loads(row[\"layout_json\"])\n                # recursively decompose the layout json and add to proper level children\n\n                page_json = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": row[\"page_number\"],\n                        \"page_text\": row[\"text\"],\n                    },\n                    \"children\": [self.recursive_layout_json(layout_json)],\n                }\n                pages_json.append(page_json)\n            except Exception as e:\n                logger.error(f\"Error in row {index}: {e}\")\n                logger.error(row[\"layout_json\"])\n                logger.exception(e)\n                # if this is an unhandled error\n                # we should still keep all data for this page, so we will construct a page with everything we have\n                page_json = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": row[\"page_number\"],\n                        \"page_text\": row[\"text\"],\n                    },\n                    \"children\": [],\n                }\n                pages_json.append(page_json)\n\n        self.kg_json[\"children\"] = pages_json\n        self.export_kg()\n\n    def link_image_to_page(self):\n        \"\"\"\n        Loop the image, assign it under the proper page\n        If the page not exist, then add a page node\n        \"\"\"\n        block_images = self.folder_path / \"images\" / \"blocks_images.csv\"\n        if empty_check(block_images):\n            return\n        images_df = pd.read_csv(self.folder_path / \"images\" / \"blocks_images.csv\")\n        for index, row in images_df.iterrows():\n            page_number = row[\"page_number\"]\n            page_node = self.get_page_node(page_number)\n            if not page_node:\n                logger.info(f\"Page {page_number} not found, adding a new page node\")\n                page_node = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": page_number,\n                        \"page_text\": \"\",\n                    },\n                    \"children\": [],\n                }\n                self.kg_json[\"children\"].append(page_node)\n            image_node = {\n                \"node_type\": \"image\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"image_path\": row[\"image_path\"],\n                    \"image_block_number\": row[\"block_number\"],\n                    \"bbox\": row[\"bbox\"],\n                },\n                \"children\": [],\n            }\n\n            page_node[\"children\"].append(image_node)\n\n        self.export_kg()\n\n    def link_page_image_to_page(self):\n        page_images_file = self.folder_path / \"images\" / \"page_images.csv\"\n        page_images_df = pd.read_csv(page_images_file)\n        for index, row in page_images_df.iterrows():\n            page_number = row[\"page_number\"]\n            page_node = self.get_page_node(page_number)\n            if not page_node:\n                logger.info(f\"Page {page_number} not found, adding a new page node\")\n                page_node = {\n                    \"node_type\": \"page\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"page_number\": page_number,\n                        \"page_text\": \"\",\n                    },\n                    \"children\": [],\n                }\n                self.kg_json[\"children\"].append(page_node)\n            image_node = {\n                \"node_type\": \"page_image\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"image_path\": row[\"image_path\"],\n                },\n                \"children\": [],\n            }\n            page_node[\"children\"].append(image_node)\n\n    def link_table_to_page(self):\n        \"\"\"\n        Link the table file to proper page.\n\n        Link to proper position in the page will be in function\n\n        `link_table_to_context`\n\n        \"\"\"\n        tables = self.folder_path / \"tables\" / \"tables.csv\"\n        if empty_check(tables):\n            return\n        table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n        for index, row in table_df.iterrows():\n            logger.info(f\"Processing table {index}\")\n            page_node = self.get_page_node(row[\"page_index\"])\n            page_node[\"children\"].append(\n                {\n                    \"node_type\": \"table_csv\",\n                    \"uuid\": str(uuid4()),\n                    \"node_properties\": {\n                        \"table_path\": row[\"file_path\"],\n                        \"table_index\": row[\"table_index\"],\n                        \"bbox\": row[\"bbox\"],\n                    },\n                }\n            )\n\n        self.export_kg()\n\n    def link_image_to_context(self):\n        \"\"\"\n        Construct the image knowledge graph\n        \"\"\"\n        block_images = self.folder_path / \"images\" / \"blocks_images.csv\"\n        if empty_check(block_images):\n            return\n        images_df = pd.read_csv(self.folder_path / \"images\" / \"blocks_images.csv\")\n        text_block_df = pd.read_csv(self.folder_path / \"texts\" / \"blocks_texts.csv\")\n        logger.debug(text_block_df.columns.tolist())\n        for index, row in images_df.iterrows():\n            page_number = row[\"page_number\"]\n\n            logger.info(f\"Processing image {index} in page {page_number}\")\n            page_node = self.get_page_node(page_number)\n            # get the text blocks that are in the same page\n            text_blocks = text_block_df[\n                text_block_df[\"page_number\"] == page_number\n            ].copy(deep=True)\n            # clean the text_block without text after text clean all space\n            text_blocks = text_blocks[\n                text_blocks[\"text\"].str.strip() != \"\"\n            ].reset_index()\n            image_bbox = row[\"bbox\"]\n            logger.debug(f\"Image bbox: {image_bbox}\")\n            text_blocks_bbox = text_blocks[\"bbox\"].tolist()\n            nearby_text_blocks = BlockFinder.find_closest_blocks(\n                image_bbox, text_blocks_bbox\n            )\n            nearby_info = []\n            nearby_info_dict = {}\n            for key, value in nearby_text_blocks.items():\n                if value is not None:\n                    text_block = text_blocks.loc[value]\n                    logger.debug(text_block)\n                    nearby_info.append(\n                        {\n                            \"node_type\": \"text_block\",\n                            \"uuid\": str(uuid4()),\n                            \"node_properties\": {\n                                \"text_block_bbox\": text_block[\"bbox\"],\n                                \"content\": text_block[\"text\"],\n                                \"position\": key,\n                                \"text_block_number\": int(text_block[\"block_number\"]),\n                            },\n                            \"children\": [],\n                        }\n                    )\n                    nearby_info_dict[key] = {\"content\": text_block[\"text\"], \"uuids\": []}\n            \"\"\"\n            We also need to loop the nodes within this page\n            if the text block is highly similar to a content node, then we can link them together\n\n            How we solve this problem?\n\n            Recursively loop the children of the page node, if the text block is highly similar to the content\n            then we can link them together\n\n            So the function input should be the page_node dict, and the nearby_info_dict\n            Output should be the updated nearby_info_dict with the linked uuid\n            \"\"\"\n            nearby_info_dict = self.link_image_to_tree_node(page_node, nearby_info_dict)\n            logger.info(nearby_info_dict)\n\n            for item in nearby_info:\n                key = item[\"node_properties\"][\"position\"]\n                item[\"linkage\"] = nearby_info_dict[key][\"uuids\"]\n\n            \"\"\"\n            find the image node\n            add the nearby_info to the children\n            the image node will have the image_block_number to identify it\n            \"\"\"\n            for child in page_node[\"children\"]:\n                if (\n                    child[\"node_type\"] == \"image\"\n                    and child[\"node_properties\"][\"image_block_number\"]\n                    == row[\"block_number\"]\n                ):\n                    child[\"children\"] = nearby_info\n                    break\n\n        self.export_kg()\n\n    def link_table_to_context(self):\n        \"\"\"\n        Link the table to the context\n\n        We have two ways to make it work\n\n        1. Loop the table, and for tree leaf within the page node, if it is tagged as table, then link them together\n        2. We have bbox of the table, so we can find the nearby text block, and link them together\n\n        \"\"\"\n        tables = self.folder_path / \"tables\" / \"tables.csv\"\n        if empty_check(tables):\n            return\n        table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n        text_block_df = pd.read_csv(self.folder_path / \"texts\" / \"blocks_texts.csv\")\n        for index, row in table_df.iterrows():\n            page_number = row[\"page_index\"]\n            page_node = self.get_page_node(page_number)\n            table_bbox = row[\"bbox\"]\n            text_blocks = text_block_df[\n                text_block_df[\"page_number\"] == page_number\n            ].copy(deep=True)\n            text_blocks = text_blocks[\n                text_blocks[\"text\"].str.strip() != \"\"\n            ].reset_index()\n            text_blocks_bbox = text_blocks[\"bbox\"].tolist()\n            nearby_text_blocks = BlockFinder.find_closest_blocks(\n                table_bbox, text_blocks_bbox\n            )\n            nearby_info = []\n            nearby_info_dict = {}\n            for key, value in nearby_text_blocks.items():\n                if value is not None:\n                    text_block = text_blocks.loc[value]\n                    nearby_info.append(\n                        {\n                            \"node_type\": \"text_block\",\n                            \"uuid\": str(uuid4()),\n                            \"node_properties\": {\n                                \"text_block_bbox\": text_block[\"bbox\"],\n                                \"content\": text_block[\"text\"],\n                                \"position\": key,\n                                \"text_block_number\": int(text_block[\"block_number\"]),\n                            },\n                            \"children\": [],\n                        }\n                    )\n                    nearby_info_dict[key] = {\"content\": text_block[\"text\"], \"uuids\": []}\n            nearby_info_dict = self.link_image_to_tree_node(page_node, nearby_info_dict)\n            for item in nearby_info:\n                key = item[\"node_properties\"][\"position\"]\n                item[\"linkage\"] = nearby_info_dict[key][\"uuids\"]\n\n            # the second matching method, loop the tree node of the page\n            table_nodes = self.get_specific_tag_nodes(page_node, \"table\")\n            page_tree_table_node = None\n            # matched table nodes\n            table_index = row[\"table_index\"]\n            if len(table_nodes) &gt;= table_index:\n                page_tree_table_node = table_nodes[table_index - 1]\n\n            # give table node a linkage to the table_node\n            for child in page_node[\"children\"]:\n                if (\n                    child[\"node_type\"] == \"table_csv\"\n                    and child[\"node_properties\"][\"table_index\"] == row[\"table_index\"]\n                ):\n                    child[\"children\"] = nearby_info\n                    if page_tree_table_node:\n                        # add the linkage from table_csv to table_tree_node\n                        child[\"linkage\"] = [page_tree_table_node[\"uuid\"]]\n                    break\n\n        self.export_kg()\n\n    def export_kg(self) -&gt; None:\n        \"\"\"\n        Export the knowledge graph to json file\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n            json.dump(self.kg_json, f, indent=2)\n\n    def load_kg(self):\n        \"\"\"\n        Load the knowledge graph from JSON\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n            self.kg_json = json.load(f)\n\n    def get_page_node(self, page_number: int) -&gt; Optional[dict]:\n        \"\"\"\n        Get the page node\n\n        Args:\n            page_number (int): The page number\n\n        Returns:\n            page_node (dict): The page node\n        \"\"\"\n        for page in self.kg_json[\"children\"]:\n            if str(page[\"node_properties\"][\"page_number\"]) == str(page_number):\n                return page\n        logger.error(f\"Page {page_number} not found\")\n        return None\n\n    def get_specific_tag_nodes(self, tree_json: dict, tag: str) -&gt; list:\n        \"\"\"\n        Get the specific tag nodes from the page node\n\n        Args:\n            tree_json (dict): The tree_json\n            tag (str): The tag to find\n\n        Returns:\n            list: The list of nodes with the specific tag\n        \"\"\"\n        nodes = []\n        if \"children\" not in tree_json:\n            return nodes\n        for child in tree_json[\"children\"]:\n            if child[\"node_type\"] == tag:\n                nodes.append(child)\n            nodes.extend(self.get_specific_tag_nodes(child, tag))\n        return nodes\n\n    @classmethod\n    def recursive_layout_json(cls, layout_json: dict) -&gt; dict:\n        \"\"\"\n        Recursively processes layout JSON to construct a tree structure, annotating each node with\n        a unique identifier and handling specific HTML structures like tables.\n\n        Args:\n            layout_json (dict): The layout JSON object to process.\n\n        Returns:\n            dict: A tree-like JSON object with added metadata.\n        \"\"\"\n        try:\n            return cls._process_node(layout_json)\n        except Exception as e:\n            logger.exception(\"Failed to process layout JSON\")\n            return cls._error_node(layout_json, str(e))\n\n    @classmethod\n    def _process_node(cls, node: dict) -&gt; dict:\n        \"\"\"\n        Process a single node in the layout JSON.\n\n        Args:\n            node (dict): The node to process.\n\n        Returns:\n            dict: The processed node.\n        \"\"\"\n        tag = node.get(\"tag\")\n        if tag in HTML_TAGS:\n            return cls._create_tree_node(tag, node)\n\n        # If 'tag' is missing, attempt to find a valid HTML tag in the keys\n        for key in node:\n            if key.strip() in HTML_TAGS:\n                return cls._create_tree_node(key, node)\n\n        # If no valid tag is found, handle as an untagged node\n        return cls._untagged_node(node)\n\n    @classmethod\n    def _create_tree_node(cls, tag: str, node: dict) -&gt; dict:\n        \"\"\"\n        Create a tree node for the JSON structure.\n\n        Args:\n            tag (str): The HTML tag of the node.\n            node (dict): The original node data.\n\n        Returns:\n            dict: A structured tree node.\n        \"\"\"\n        node_uuid = str(uuid4())\n        node_properties = {\n            \"content\": node.get(\"content\", \"\"),\n            \"text\": json.dumps(node) if tag == \"table\" else \"\",\n            \"records\": node.get(\"children\", []) if tag == \"table\" else [],\n        }\n        children = [cls._process_node(child) for child in node.get(\"children\", [])]\n\n        return {\n            \"node_type\": tag,\n            \"uuid\": node_uuid,\n            \"node_properties\": node_properties,\n            \"children\": children,\n        }\n\n    @classmethod\n    def _untagged_node(cls, node: dict) -&gt; dict:\n        \"\"\"\n        Handles nodes without a recognized HTML tag.\n\n        Args:\n            node (dict): The node to handle.\n\n        Returns:\n            dict: A default structured node indicating an untagged element.\n        \"\"\"\n        return {\n            \"node_type\": \"untagged\",\n            \"uuid\": str(uuid4()),\n            \"node_properties\": {\"content\": json.dumps(node)},\n            \"children\": [],\n        }\n\n    @classmethod\n    def _error_node(cls, node: dict, error_message: str) -&gt; dict:\n        \"\"\"\n        Create an error node when processing fails.\n\n        Args:\n            node (dict): The node that caused the error.\n            error_message (str): A message describing the error.\n\n        Returns:\n            dict: An error node.\n        \"\"\"\n        return {\n            \"node_type\": \"unknown\",\n            \"uuid\": str(uuid4()),\n            \"node_properties\": {\"content\": json.dumps(node), \"error\": error_message},\n            \"children\": [],\n        }\n\n    def link_image_to_tree_node(self, page_node: dict, nearby_info_dict: dict) -&gt; dict:\n        \"\"\"\n        Link the image to the tree node\n\n        - Loop the children of the page node\n        - If the text block is highly similar to the content, add the uuid to the nearby_info_dict\n\n        Match method:\n            \u2212 exact match\n            - fuzzy match\n\n        Args:\n            page_node (dict): The page node\n            nearby_info_dict (dict): The nearby info dict\n\n        Returns:\n            nearby_info_dict (dict): The updated nearby info dict\n        \"\"\"\n\n        if \"children\" not in page_node:\n            return nearby_info_dict\n        for child in page_node[\"children\"]:\n            # get the text\n            content = child[\"node_properties\"].get(\"content\", \"\")\n            nearby_info_dict = self.link_image_to_tree_node(child, nearby_info_dict)\n            if content.strip() == \"\":\n                continue\n            for key, value in nearby_info_dict.items():\n                if content.strip() == value[\"content\"].strip():\n                    value[\"uuids\"].append(child[\"uuid\"])\n                elif self.text_bert_match(content, value[\"content\"]):\n                    value[\"uuids\"].append(child[\"uuid\"])\n\n        return nearby_info_dict\n\n    def text_bert_match(\n        self, text1: str, text2: str, threshold_value: float = 0.8\n    ) -&gt; bool:\n        \"\"\"\n        Fuzzy match the text\n\n        Args:\n            text1 (str): The first text\n            text2 (str): The second text\n            threshold_value (float): The threshold value\n\n        Returns:\n            bool: Whether the text is similar\n        \"\"\"\n        embedding1 = self.sentence_transformer.encode([text1])\n        embedding2 = self.sentence_transformer.encode([text2])\n        similarity = self.sentence_transformer.similarity(embedding1, embedding2)\n\n        # get the first value from the similarity matrix, and to float\n        similarity = similarity[0].item()\n        matched = similarity &gt; threshold_value\n\n        if matched:\n            logger.debug(f\"Matched: {text1} | {text2}\")\n            logger.debug(f\"Similarity: {similarity}\")\n        return matched\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.__init__","title":"<code>__init__(folder_path, input_format='pdf_exported')</code>","text":"<p>Initialize the class with the pdf file</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>Path</code> <p>The path to the pdf file</p> required Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def __init__(\n    self,\n    folder_path: Path,\n    input_format: str = \"pdf_exported\",\n):\n    \"\"\"\n    Initialize the class with the pdf file\n\n\n    Args:\n        folder_path (Path): The path to the pdf file\n\n    \"\"\"\n    self.folder_path = folder_path\n    self.kg_folder = self.folder_path / \"kg\"\n    if not self.kg_folder.exists():\n        self.kg_folder.mkdir(parents=True, exist_ok=True)\n    self.kg_json = {}\n    self.metadata = json.load((self.folder_path / \"metadata.json\").open())\n    self.sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    self.input_format = input_format\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.create_kg","title":"<code>create_kg()</code>","text":"<p>Create the layout knowledge graph</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def create_kg(self):\n    \"\"\"\n    Create the layout knowledge graph\n    \"\"\"\n    self.document_kg()\n    if self.input_format == \"pdf_exported\":\n        self.link_image_to_page()\n        self.link_table_to_page()\n        self.link_image_to_context()\n        self.link_table_to_context()\n    if self.input_format == \"pdf_scanned\":\n        # add page image\n        self.link_page_image_to_page()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.document_kg","title":"<code>document_kg()</code>","text":"<p>Construct the layout knowledge graph skeleton first</p> <p>We will require the md.json.csv file with the following columns:</p> <ul> <li>layout_json</li> </ul> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def document_kg(self):\n    \"\"\"\n    Construct the layout knowledge graph skeleton first\n\n    We will require the md.json.csv file with the following columns:\n\n    - layout_json\n    \"\"\"\n    # 1. add the document node\n\n    self.kg_json = {\n        \"node_type\": \"document\",\n        \"uuid\": str(uuid4()),\n        \"node_properties\": self.metadata,\n        \"children\": [],\n    }\n\n    # 2. add page nodes\n    pages_json = []\n    text_folder = self.folder_path / \"texts\"\n    md_json_csv = text_folder / \"md.json.csv\"\n    texts_json_df = pd.read_csv(md_json_csv)\n    columns = texts_json_df.columns.tolist()\n    logger.debug(f\"Columns: {columns}\")\n    # we will focus on the layout json\n\n    for index, row in texts_json_df.iterrows():\n        logger.info(f\"Processing page_index {index}\")\n        logger.debug(row[\"layout_json\"])\n        try:\n            layout_json = json.loads(row[\"layout_json\"])\n            # recursively decompose the layout json and add to proper level children\n\n            page_json = {\n                \"node_type\": \"page\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"page_number\": row[\"page_number\"],\n                    \"page_text\": row[\"text\"],\n                },\n                \"children\": [self.recursive_layout_json(layout_json)],\n            }\n            pages_json.append(page_json)\n        except Exception as e:\n            logger.error(f\"Error in row {index}: {e}\")\n            logger.error(row[\"layout_json\"])\n            logger.exception(e)\n            # if this is an unhandled error\n            # we should still keep all data for this page, so we will construct a page with everything we have\n            page_json = {\n                \"node_type\": \"page\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"page_number\": row[\"page_number\"],\n                    \"page_text\": row[\"text\"],\n                },\n                \"children\": [],\n            }\n            pages_json.append(page_json)\n\n    self.kg_json[\"children\"] = pages_json\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.export_kg","title":"<code>export_kg()</code>","text":"<p>Export the knowledge graph to json file</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def export_kg(self) -&gt; None:\n    \"\"\"\n    Export the knowledge graph to json file\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n        json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.get_page_node","title":"<code>get_page_node(page_number)</code>","text":"<p>Get the page node</p> <p>Parameters:</p> Name Type Description Default <code>page_number</code> <code>int</code> <p>The page number</p> required <p>Returns:</p> Name Type Description <code>page_node</code> <code>dict</code> <p>The page node</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def get_page_node(self, page_number: int) -&gt; Optional[dict]:\n    \"\"\"\n    Get the page node\n\n    Args:\n        page_number (int): The page number\n\n    Returns:\n        page_node (dict): The page node\n    \"\"\"\n    for page in self.kg_json[\"children\"]:\n        if str(page[\"node_properties\"][\"page_number\"]) == str(page_number):\n            return page\n    logger.error(f\"Page {page_number} not found\")\n    return None\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.get_specific_tag_nodes","title":"<code>get_specific_tag_nodes(tree_json, tag)</code>","text":"<p>Get the specific tag nodes from the page node</p> <p>Parameters:</p> Name Type Description Default <code>tree_json</code> <code>dict</code> <p>The tree_json</p> required <code>tag</code> <code>str</code> <p>The tag to find</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of nodes with the specific tag</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def get_specific_tag_nodes(self, tree_json: dict, tag: str) -&gt; list:\n    \"\"\"\n    Get the specific tag nodes from the page node\n\n    Args:\n        tree_json (dict): The tree_json\n        tag (str): The tag to find\n\n    Returns:\n        list: The list of nodes with the specific tag\n    \"\"\"\n    nodes = []\n    if \"children\" not in tree_json:\n        return nodes\n    for child in tree_json[\"children\"]:\n        if child[\"node_type\"] == tag:\n            nodes.append(child)\n        nodes.extend(self.get_specific_tag_nodes(child, tag))\n    return nodes\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.link_image_to_context","title":"<code>link_image_to_context()</code>","text":"<p>Construct the image knowledge graph</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def link_image_to_context(self):\n    \"\"\"\n    Construct the image knowledge graph\n    \"\"\"\n    block_images = self.folder_path / \"images\" / \"blocks_images.csv\"\n    if empty_check(block_images):\n        return\n    images_df = pd.read_csv(self.folder_path / \"images\" / \"blocks_images.csv\")\n    text_block_df = pd.read_csv(self.folder_path / \"texts\" / \"blocks_texts.csv\")\n    logger.debug(text_block_df.columns.tolist())\n    for index, row in images_df.iterrows():\n        page_number = row[\"page_number\"]\n\n        logger.info(f\"Processing image {index} in page {page_number}\")\n        page_node = self.get_page_node(page_number)\n        # get the text blocks that are in the same page\n        text_blocks = text_block_df[\n            text_block_df[\"page_number\"] == page_number\n        ].copy(deep=True)\n        # clean the text_block without text after text clean all space\n        text_blocks = text_blocks[\n            text_blocks[\"text\"].str.strip() != \"\"\n        ].reset_index()\n        image_bbox = row[\"bbox\"]\n        logger.debug(f\"Image bbox: {image_bbox}\")\n        text_blocks_bbox = text_blocks[\"bbox\"].tolist()\n        nearby_text_blocks = BlockFinder.find_closest_blocks(\n            image_bbox, text_blocks_bbox\n        )\n        nearby_info = []\n        nearby_info_dict = {}\n        for key, value in nearby_text_blocks.items():\n            if value is not None:\n                text_block = text_blocks.loc[value]\n                logger.debug(text_block)\n                nearby_info.append(\n                    {\n                        \"node_type\": \"text_block\",\n                        \"uuid\": str(uuid4()),\n                        \"node_properties\": {\n                            \"text_block_bbox\": text_block[\"bbox\"],\n                            \"content\": text_block[\"text\"],\n                            \"position\": key,\n                            \"text_block_number\": int(text_block[\"block_number\"]),\n                        },\n                        \"children\": [],\n                    }\n                )\n                nearby_info_dict[key] = {\"content\": text_block[\"text\"], \"uuids\": []}\n        \"\"\"\n        We also need to loop the nodes within this page\n        if the text block is highly similar to a content node, then we can link them together\n\n        How we solve this problem?\n\n        Recursively loop the children of the page node, if the text block is highly similar to the content\n        then we can link them together\n\n        So the function input should be the page_node dict, and the nearby_info_dict\n        Output should be the updated nearby_info_dict with the linked uuid\n        \"\"\"\n        nearby_info_dict = self.link_image_to_tree_node(page_node, nearby_info_dict)\n        logger.info(nearby_info_dict)\n\n        for item in nearby_info:\n            key = item[\"node_properties\"][\"position\"]\n            item[\"linkage\"] = nearby_info_dict[key][\"uuids\"]\n\n        \"\"\"\n        find the image node\n        add the nearby_info to the children\n        the image node will have the image_block_number to identify it\n        \"\"\"\n        for child in page_node[\"children\"]:\n            if (\n                child[\"node_type\"] == \"image\"\n                and child[\"node_properties\"][\"image_block_number\"]\n                == row[\"block_number\"]\n            ):\n                child[\"children\"] = nearby_info\n                break\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.link_image_to_page","title":"<code>link_image_to_page()</code>","text":"<p>Loop the image, assign it under the proper page If the page not exist, then add a page node</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def link_image_to_page(self):\n    \"\"\"\n    Loop the image, assign it under the proper page\n    If the page not exist, then add a page node\n    \"\"\"\n    block_images = self.folder_path / \"images\" / \"blocks_images.csv\"\n    if empty_check(block_images):\n        return\n    images_df = pd.read_csv(self.folder_path / \"images\" / \"blocks_images.csv\")\n    for index, row in images_df.iterrows():\n        page_number = row[\"page_number\"]\n        page_node = self.get_page_node(page_number)\n        if not page_node:\n            logger.info(f\"Page {page_number} not found, adding a new page node\")\n            page_node = {\n                \"node_type\": \"page\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"page_number\": page_number,\n                    \"page_text\": \"\",\n                },\n                \"children\": [],\n            }\n            self.kg_json[\"children\"].append(page_node)\n        image_node = {\n            \"node_type\": \"image\",\n            \"uuid\": str(uuid4()),\n            \"node_properties\": {\n                \"image_path\": row[\"image_path\"],\n                \"image_block_number\": row[\"block_number\"],\n                \"bbox\": row[\"bbox\"],\n            },\n            \"children\": [],\n        }\n\n        page_node[\"children\"].append(image_node)\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.link_image_to_tree_node","title":"<code>link_image_to_tree_node(page_node, nearby_info_dict)</code>","text":"<p>Link the image to the tree node</p> <ul> <li>Loop the children of the page node</li> <li>If the text block is highly similar to the content, add the uuid to the nearby_info_dict</li> </ul> Match method <p>\u2212 exact match - fuzzy match</p> <p>Parameters:</p> Name Type Description Default <code>page_node</code> <code>dict</code> <p>The page node</p> required <code>nearby_info_dict</code> <code>dict</code> <p>The nearby info dict</p> required <p>Returns:</p> Name Type Description <code>nearby_info_dict</code> <code>dict</code> <p>The updated nearby info dict</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def link_image_to_tree_node(self, page_node: dict, nearby_info_dict: dict) -&gt; dict:\n    \"\"\"\n    Link the image to the tree node\n\n    - Loop the children of the page node\n    - If the text block is highly similar to the content, add the uuid to the nearby_info_dict\n\n    Match method:\n        \u2212 exact match\n        - fuzzy match\n\n    Args:\n        page_node (dict): The page node\n        nearby_info_dict (dict): The nearby info dict\n\n    Returns:\n        nearby_info_dict (dict): The updated nearby info dict\n    \"\"\"\n\n    if \"children\" not in page_node:\n        return nearby_info_dict\n    for child in page_node[\"children\"]:\n        # get the text\n        content = child[\"node_properties\"].get(\"content\", \"\")\n        nearby_info_dict = self.link_image_to_tree_node(child, nearby_info_dict)\n        if content.strip() == \"\":\n            continue\n        for key, value in nearby_info_dict.items():\n            if content.strip() == value[\"content\"].strip():\n                value[\"uuids\"].append(child[\"uuid\"])\n            elif self.text_bert_match(content, value[\"content\"]):\n                value[\"uuids\"].append(child[\"uuid\"])\n\n    return nearby_info_dict\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.link_table_to_context","title":"<code>link_table_to_context()</code>","text":"<p>Link the table to the context</p> <p>We have two ways to make it work</p> <ol> <li>Loop the table, and for tree leaf within the page node, if it is tagged as table, then link them together</li> <li>We have bbox of the table, so we can find the nearby text block, and link them together</li> </ol> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def link_table_to_context(self):\n    \"\"\"\n    Link the table to the context\n\n    We have two ways to make it work\n\n    1. Loop the table, and for tree leaf within the page node, if it is tagged as table, then link them together\n    2. We have bbox of the table, so we can find the nearby text block, and link them together\n\n    \"\"\"\n    tables = self.folder_path / \"tables\" / \"tables.csv\"\n    if empty_check(tables):\n        return\n    table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n    text_block_df = pd.read_csv(self.folder_path / \"texts\" / \"blocks_texts.csv\")\n    for index, row in table_df.iterrows():\n        page_number = row[\"page_index\"]\n        page_node = self.get_page_node(page_number)\n        table_bbox = row[\"bbox\"]\n        text_blocks = text_block_df[\n            text_block_df[\"page_number\"] == page_number\n        ].copy(deep=True)\n        text_blocks = text_blocks[\n            text_blocks[\"text\"].str.strip() != \"\"\n        ].reset_index()\n        text_blocks_bbox = text_blocks[\"bbox\"].tolist()\n        nearby_text_blocks = BlockFinder.find_closest_blocks(\n            table_bbox, text_blocks_bbox\n        )\n        nearby_info = []\n        nearby_info_dict = {}\n        for key, value in nearby_text_blocks.items():\n            if value is not None:\n                text_block = text_blocks.loc[value]\n                nearby_info.append(\n                    {\n                        \"node_type\": \"text_block\",\n                        \"uuid\": str(uuid4()),\n                        \"node_properties\": {\n                            \"text_block_bbox\": text_block[\"bbox\"],\n                            \"content\": text_block[\"text\"],\n                            \"position\": key,\n                            \"text_block_number\": int(text_block[\"block_number\"]),\n                        },\n                        \"children\": [],\n                    }\n                )\n                nearby_info_dict[key] = {\"content\": text_block[\"text\"], \"uuids\": []}\n        nearby_info_dict = self.link_image_to_tree_node(page_node, nearby_info_dict)\n        for item in nearby_info:\n            key = item[\"node_properties\"][\"position\"]\n            item[\"linkage\"] = nearby_info_dict[key][\"uuids\"]\n\n        # the second matching method, loop the tree node of the page\n        table_nodes = self.get_specific_tag_nodes(page_node, \"table\")\n        page_tree_table_node = None\n        # matched table nodes\n        table_index = row[\"table_index\"]\n        if len(table_nodes) &gt;= table_index:\n            page_tree_table_node = table_nodes[table_index - 1]\n\n        # give table node a linkage to the table_node\n        for child in page_node[\"children\"]:\n            if (\n                child[\"node_type\"] == \"table_csv\"\n                and child[\"node_properties\"][\"table_index\"] == row[\"table_index\"]\n            ):\n                child[\"children\"] = nearby_info\n                if page_tree_table_node:\n                    # add the linkage from table_csv to table_tree_node\n                    child[\"linkage\"] = [page_tree_table_node[\"uuid\"]]\n                break\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.link_table_to_page","title":"<code>link_table_to_page()</code>","text":"<p>Link the table file to proper page.</p> <p>Link to proper position in the page will be in function</p> <p><code>link_table_to_context</code></p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def link_table_to_page(self):\n    \"\"\"\n    Link the table file to proper page.\n\n    Link to proper position in the page will be in function\n\n    `link_table_to_context`\n\n    \"\"\"\n    tables = self.folder_path / \"tables\" / \"tables.csv\"\n    if empty_check(tables):\n        return\n    table_df = pd.read_csv(self.folder_path / \"tables\" / \"tables.csv\")\n    for index, row in table_df.iterrows():\n        logger.info(f\"Processing table {index}\")\n        page_node = self.get_page_node(row[\"page_index\"])\n        page_node[\"children\"].append(\n            {\n                \"node_type\": \"table_csv\",\n                \"uuid\": str(uuid4()),\n                \"node_properties\": {\n                    \"table_path\": row[\"file_path\"],\n                    \"table_index\": row[\"table_index\"],\n                    \"bbox\": row[\"bbox\"],\n                },\n            }\n        )\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.load_kg","title":"<code>load_kg()</code>","text":"<p>Load the knowledge graph from JSON</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def load_kg(self):\n    \"\"\"\n    Load the knowledge graph from JSON\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n        self.kg_json = json.load(f)\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.recursive_layout_json","title":"<code>recursive_layout_json(layout_json)</code>  <code>classmethod</code>","text":"<p>Recursively processes layout JSON to construct a tree structure, annotating each node with a unique identifier and handling specific HTML structures like tables.</p> <p>Parameters:</p> Name Type Description Default <code>layout_json</code> <code>dict</code> <p>The layout JSON object to process.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A tree-like JSON object with added metadata.</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>@classmethod\ndef recursive_layout_json(cls, layout_json: dict) -&gt; dict:\n    \"\"\"\n    Recursively processes layout JSON to construct a tree structure, annotating each node with\n    a unique identifier and handling specific HTML structures like tables.\n\n    Args:\n        layout_json (dict): The layout JSON object to process.\n\n    Returns:\n        dict: A tree-like JSON object with added metadata.\n    \"\"\"\n    try:\n        return cls._process_node(layout_json)\n    except Exception as e:\n        logger.exception(\"Failed to process layout JSON\")\n        return cls._error_node(layout_json, str(e))\n</code></pre>"},{"location":"sources/kg/pdf_layout_kg/#Docs2KG.kg.pdf_layout_kg.PDFLayoutKG.text_bert_match","title":"<code>text_bert_match(text1, text2, threshold_value=0.8)</code>","text":"<p>Fuzzy match the text</p> <p>Parameters:</p> Name Type Description Default <code>text1</code> <code>str</code> <p>The first text</p> required <code>text2</code> <code>str</code> <p>The second text</p> required <code>threshold_value</code> <code>float</code> <p>The threshold value</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the text is similar</p> Source code in <code>Docs2KG/kg/pdf_layout_kg.py</code> <pre><code>def text_bert_match(\n    self, text1: str, text2: str, threshold_value: float = 0.8\n) -&gt; bool:\n    \"\"\"\n    Fuzzy match the text\n\n    Args:\n        text1 (str): The first text\n        text2 (str): The second text\n        threshold_value (float): The threshold value\n\n    Returns:\n        bool: Whether the text is similar\n    \"\"\"\n    embedding1 = self.sentence_transformer.encode([text1])\n    embedding2 = self.sentence_transformer.encode([text2])\n    similarity = self.sentence_transformer.similarity(embedding1, embedding2)\n\n    # get the first value from the similarity matrix, and to float\n    similarity = similarity[0].item()\n    matched = similarity &gt; threshold_value\n\n    if matched:\n        logger.debug(f\"Matched: {text1} | {text2}\")\n        logger.debug(f\"Similarity: {similarity}\")\n    return matched\n</code></pre>"},{"location":"sources/kg/semantic_kg/","title":"Semantic kg","text":""},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG","title":"<code>SemanticKG</code>","text":"<p>The plan is</p> <ul> <li>We keep the layout_kg.json, and use this as the base</li> <li>Then we start to extract the linkage</li> <li>And then we have a semantic_kg.json</li> </ul> <p>Within this one we will have</p> <ul> <li>source_uuid</li> <li>source_semantic</li> <li>predicate</li> <li>target_uuid</li> <li>target_semantic</li> <li>extraction_method</li> </ul> <p>What we want to link:</p> <ul> <li>Table to Content<ul> <li>Where this table is mentioned, which is actually finding the reference point</li> </ul> </li> <li>Image to Content<ul> <li>Same as table</li> </ul> </li> </ul> <p>Discussion</p> <ul> <li>Within Page<ul> <li>Text2KG with Named Entity Recognition</li> </ul> </li> <li>Across Pages<ul> <li>Summary Linkage?</li> </ul> </li> </ul> <p>Alerts: Some of the functions will require the help of LLM</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>class SemanticKG:\n    \"\"\"\n    The plan is\n\n    - We keep the layout_kg.json, and use this as the base\n    - Then we start to extract the linkage\n    - And then we have a semantic_kg.json\n\n    Within this one we will have\n\n    - source_uuid\n    - source_semantic\n    - predicate\n    - target_uuid\n    - target_semantic\n    - extraction_method\n\n    What we want to link:\n\n    - Table to Content\n        - Where this table is mentioned, which is actually finding the reference point\n    - Image to Content\n        - Same as table\n\n    Discussion\n\n    - Within Page\n        - Text2KG with Named Entity Recognition\n    - Across Pages\n        - Summary Linkage?\n\n    Alerts: Some of the functions will require the help of LLM\n    \"\"\"\n\n    def __init__(\n        self,\n        folder_path: Path,\n        llm_enabled: bool = False,\n        input_format: str = \"pdf_exported\",\n    ):\n        \"\"\"\n        Initialize the SemanticKG class\n        Args:\n            folder_path (Path): The path to the pdf file\n            llm_enabled (bool, optional): Whether to use LLM. Defaults to False.\n\n        \"\"\"\n        self.folder_path = folder_path\n        self.llm_enabled = llm_enabled\n        self.cost = 0\n        logger.info(\"LLM is enabled\" if self.llm_enabled else \"LLM is disabled\")\n        self.kg_folder = self.folder_path / \"kg\"\n        if not self.kg_folder.exists():\n            self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n        self.layout_kg_file = self.kg_folder / \"layout_kg.json\"\n        # if layout_kg does not exist, then raise an error\n        if not self.layout_kg_file.exists():\n            raise FileNotFoundError(f\"{self.layout_kg_file} does not exist\")\n        # load layout_kg\n        self.layout_kg = self.load_kg(self.layout_kg_file)\n        self.input_format = input_format\n\n    def add_semantic_kg(self):\n        \"\"\"\n        As discussed in the plan, we will add the semantic knowledge graph based on the layout knowledge graph\n\n        Returns:\n\n        \"\"\"\n        # we will start with the image to content\n        if self.input_format == \"pdf_exported\":\n            self.semantic_link_image_to_content()\n            self.semantic_link_table_to_content()\n        if self.input_format != \"excel\":\n            self.semantic_page_summary()\n        self.semantic_text2kg()\n\n    def semantic_link_image_to_content(self):\n        \"\"\"\n        Link the image to the content\n\n        1. We will need to extract the image's caption and reference point\n        2. Use this caption or 1.1 to search the context, link the image to where the image is mentioned\n\n        Returns:\n\n        \"\"\"\n\n        # first locate the image caption\n        for page in self.layout_kg[\"children\"]:\n            # within the page node, then it should have the children start with the image node\n            for child in page[\"children\"]:\n                if child[\"node_type\"] == \"image\":\n                    # child now is the image node\n                    # if this child do not have children, then we will skip\n                    if \"children\" not in child or len(child[\"children\"]) == 0:\n                        continue\n                    # logger.info(child)\n                    for item in child[\"children\"]:\n                        # if this is the caption, then we will extract the text\n                        text = item[\"node_properties\"][\"content\"]\n                        if self.util_caption_detection(text):\n                            logger.info(f\"Figure/Caption detected: {text}\")\n                            # we will use this\n                            child[\"node_properties\"][\"caption\"] = text\n                            \"\"\"\n                            Link the caption to where it is mentioned\n\n                            For example, if the caption is \"Figure 1.1: The distribution of the population\",\n                            then we will search the context\n                            And found out a place indicate that: as shown in Figure 1.1,\n                            the distribution of the population is ...\n\n                            We need to find a way to match it back to the content\n\n                            Current plan of attack, we use rule based way.\n\n                            If there is a Figure XX, then we will search the context for Figure XX,\n                            and link it back to the content\n                            Because the content have not been\n                            \"\"\"\n\n                            uuids, caption = self.util_caption_mentions_detect(\n                                caption=text\n                            )\n                            logger.info(f\"UUIDs: {uuids}\")\n                            child[\"node_properties\"][\"mentioned_in\"] = uuids\n                            if caption:\n                                child[\"node_properties\"][\"unique_description\"] = caption\n                            continue\n\n        self.export_kg()\n\n    def semantic_link_table_to_content(self):\n        \"\"\"\n        Link the table to the content\n\n        So we will do the same thing first for the table\n\n        Returns:\n\n        \"\"\"\n        for page in self.layout_kg[\"children\"]:\n            # within the page node, then it should have the children start with the image node\n            for child in page[\"children\"]:\n                if child[\"node_type\"] == \"table_csv\":\n                    # child now is the image node\n                    # if this child do not have children, then we will skip\n                    if \"children\" not in child or len(child[\"children\"]) == 0:\n                        continue\n                    # logger.info(child)\n                    for item in child[\"children\"]:\n                        # if this is the caption, then we will extract the text\n                        text = item[\"node_properties\"][\"content\"]\n                        if self.util_caption_detection(text):\n                            logger.info(f\"Table/Caption detected: {text}\")\n                            # we will use this\n                            child[\"node_properties\"][\"caption\"] = text\n                            uuids, caption = self.util_caption_mentions_detect(\n                                caption=text\n                            )\n                            logger.info(f\"UUIDs: {uuids}\")\n                            child[\"node_properties\"][\"mentioned_in\"] = uuids\n                            if caption:\n                                child[\"node_properties\"][\"unique_description\"] = caption\n                            continue\n        self.export_kg()\n\n    def semantic_text2kg(self):\n        \"\"\"\n        ## General Goal of this:\n\n        - A list of triplet: (subject, predicate, object)\n        - Triplets will be associated to the tree\n        - Frequent subject will be merged, and linked\n\n        Plan of attack:\n\n        1. We need to do the Named Entity Recognition for each sentence\n        2. Do NER coexist relationship\n        3. Last step will be extracting the semantic NER vs NER relationship\n\n        How to construction the relation?\n\n        - We will grab the entities mapping to text uuid\n        {\n           \"ner_type\": {\n            \"entities\": [uuid1, uuid2]\n           }\n        }\n\n        \"\"\"\n        if self.llm_enabled:\n            current_cost = self.cost\n            # do the triple extraction\n            # self.semantic_triplet_extraction(self.layout_kg)\n            logger.info(\"Start the semantic text2kg extraction\")\n            nodes = self.semantic_triplet_extraction(self.layout_kg, [])\n            # use tqdm to show the progress\n            for node in tqdm(nodes, desc=\"Extracting triplets\"):\n                # extract the triplets from the text\n                text = node[\"node_properties\"][\"content\"]\n                logger.debug(text)\n                if text == \"\":\n                    continue\n                triplets = self.llm_extract_triplet(text)\n                node[\"node_properties\"][\"text2kg\"] = triplets\n            self.export_kg()\n            logger.info(f\"LLM cost: {self.cost - current_cost}\")\n        else:\n            # Hard to do this without LLM\n            logger.info(\"LLM is not enabled, skip the semantic text2kg extraction\")\n\n    def semantic_triplet_extraction(self, node: dict, nodes: List[dict]):\n        \"\"\"\n        Extract tripplets from the text\n\n        It will update the node with the Text2KG field, add a list of triplets\n        Args:\n            node (dict): The node in the layout knowledge graph\n            nodes (List[dict]): The list of nodes\n\n        Returns:\n\n        \"\"\"\n        for child in node[\"children\"]:\n            if \"children\" in child:\n                nodes = self.semantic_triplet_extraction(child, nodes)\n            content = child[\"node_properties\"].get(\"content\", \"\")\n            if not content:\n                continue\n            nodes.append(child)\n        return nodes\n\n    def semantic_page_summary(self):\n        \"\"\"\n        Summary of the page, which will have better understanding of the page.\n\n        Not sure whether this will enhance the RAG or not.\n\n        But will be easier for human to understand the page.\n\n        When doing summary, also need to give up page and later page information\n\n        Returns:\n\n        \"\"\"\n        for page_index, page in enumerate(self.layout_kg[\"children\"]):\n            if page[\"node_type\"] == \"page\":\n                page_content = page[\"node_properties\"][\"page_text\"]\n                logger.debug(page_content)\n                summary = self.llm_page_summary(page_content)\n                page[\"node_properties\"][\"summary\"] = summary\n\n        self.export_kg()\n\n    @staticmethod\n    def load_kg(file_path: Path) -&gt; dict:\n        \"\"\"\n        Load the knowledge graph from JSON\n\n        Args:\n            file_path (Path): The path to the JSON file\n\n        Returns:\n            dict: The knowledge graph\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            kg = json.load(f)\n        return kg\n\n    def export_kg(self):\n        \"\"\"\n        Export the semantic knowledge graph to a JSON file\n        \"\"\"\n\n        with open(self.layout_kg_file, \"w\") as f:\n            json.dump(self.layout_kg, f, indent=4)\n\n    def util_caption_detection(self, text: str) -&gt; bool:  # noqa\n        \"\"\"\n        Give a text, detect if this is a caption for image or table\n\n        If it is LLM enabled, then we will use LLM to detect the caption\n        If it is not LLM enabled, we use keyword match\n            - Currently LLM performance not well\n\n        Returns:\n\n        \"\"\"\n        for keyword in CAPTION_KEYWORDS:\n            if keyword in text.lower():\n                return True\n        # if self.llm_enabled:\n        #     return self.llm_detect_caption(text)\n        return False\n\n    def util_caption_mentions_detect(self, caption: str) -&gt; Tuple[List[str], str]:\n        \"\"\"\n\n        First we need to find the unique description for the caption.\n\n        For example: Plate 1.1: The distribution of the population\n\n        Plate 1.1 is the unique description\n\n        We will need to search the whole document to find the reference point\n\n        Args:\n            caption (str): The caption text\n\n\n        Returns:\n            uuids (List[str]): The list of uuids where the caption is mentioned\n\n        \"\"\"\n        # first extract the unique description\n        # Extract the unique description from the caption\n        keyword_patten = \"|\".join(CAPTION_KEYWORDS)\n        match = re.search(rf\"(\\b({keyword_patten}) \\d+(\\.\\d+)*\\b)\", caption.lower())\n        unique_description = None\n        if match:\n            unique_description = match.group(1)\n        else:\n            if self.llm_enabled:\n                \"\"\"\n                Try to use LLM to do this work\n                \"\"\"\n                unique_description = self.llm_detect_caption_mentions(caption)\n                logger.info(f\"Unique description: {unique_description}\")\n\n        if not unique_description:\n            return []\n        logger.info(f\"Unique description: {unique_description}\")\n        mentioned_uuids = []\n        # search the context\n        mentioned_uuids = self.util_mentioned_uuids(\n            self.layout_kg, unique_description, mentioned_uuids\n        )\n        return mentioned_uuids, unique_description\n\n    def util_mentioned_uuids(\n        self, node: dict, unique_description: str, uuids: List[str]\n    ) -&gt; List[str]:\n        \"\"\"\n        Search the context for the unique description\n\n        Args:\n            node (dict): The node in the layout knowledge graph\n            unique_description (str): The unique description extracted from the caption\n            uuids (List[str]): The list of uuids where the unique description is mentioned\n\n        Returns:\n            uuids (List[str]): The list of uuids where the unique description is mentioned\n        \"\"\"\n        for child in node[\"children\"]:\n            if \"node_properties\" in child:\n                if \"content\" in child[\"node_properties\"]:\n                    if (\n                        unique_description\n                        in child[\"node_properties\"][\"content\"].lower()\n                    ):\n                        uuids.append(child[\"uuid\"])\n            if \"children\" in child:\n                uuids = self.util_mentioned_uuids(child, unique_description, uuids)\n        return uuids\n\n    def llm_detect_caption(self, text: str) -&gt; bool:\n        \"\"\"\n        Use LLM to detect whether the given text is a caption for an image or table.\n\n        Args:\n            text (str): The text to be evaluated.\n\n        Returns:\n            bool: True if the text is identified as a caption, False otherwise.\n        \"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are a system that help to detect if the text is a caption for an image or table.\n                                \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Is the following text a caption for image or table?\n                        You need to detect if the text is a caption for an image or table.\n\n                        Please return the result in JSON format as follows:\n                            - {'is_caption': 1} if it is a caption,\n                            - or {'is_caption': 0} if it is not a caption.\n\n                        Some examples are captions for images or tables:\n                        - \"Figure 1.1: The distribution of the population\"\n                        - \"Table 2.1: The distribution of the population\"\n                        - \"Plate 1.1: The distribution of the population\"\n                        - \"Graph 1.1: The distribution of the population\"\n\n                        \"{text}\"\n                    \"\"\",\n                },\n            ]\n            response, cost = openai_call(messages)\n            self.cost += cost\n            logger.debug(f\"LLM cost: {cost}, response: {response}, text: {text}\")\n            response_dict = json.loads(response)\n            return response_dict.get(\"is_caption\", 0) == 1\n        except Exception as e:\n            logger.error(f\"Error in LLM caption detection: {e}\")\n        return False\n\n    def llm_detect_caption_mentions(self, caption: str) -&gt; Optional[str]:\n        \"\"\"\n        Use LLM to detect the mentions of the given caption in the document.\n\n        Args:\n            caption (str): The caption text.\n\n        Returns:\n            List[str]: The list of uuids where the caption is mentioned.\n        \"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an assistant that can detect the unique description\n                                  of a caption in a document.\n                                \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Please find the unique description of the caption in the document.\n\n                        For example, if the caption is \"Plate 1.1: The distribution of the population\",\n                        the unique description is \"Plate 1.1\".\n\n                        Given caption:\n\n                        \"{caption}\"\n\n                        Return the str within the json with the key \"uid\".\n                    \"\"\",\n                },\n            ]\n            response, cost = openai_call(messages)\n            self.cost += cost\n            logger.debug(f\"LLM cost: {cost}, response: {response}, caption: {caption}\")\n            response_dict = json.loads(response)\n            return response_dict.get(\"uid\", \"\")\n        except Exception as e:\n            logger.error(f\"Error in LLM caption mentions detection: {e}\")\n            logger.exception(e)\n        return None\n\n    def llm_extract_triplet(self, text) -&gt; List[dict]:\n        \"\"\"\n        Extract the triplet from the text\n        Args:\n            text (str): The text to extract the triplets from\n\n\n        Returns:\n            triplets (List[dict]): The list of triplets extracted from the text\n        \"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an assistant that can extract the triplets from a given text.\n                                \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Please extract the triplets from the following text:\n\n                        \"{text}\"\n\n                        Return the triplets within the json with the key \"triplets\".\n                        And the triplets should be in the format of a list of dictionaries,\n                        each dictionary should have the following keys:\n                        - subject\n                        - subject_ner_type\n                        - predicate\n                        - object\n                        - object_ner_type\n\n                    \"\"\",\n                },\n            ]\n            response, cost = openai_call(messages)\n            self.cost += cost\n            logger.debug(f\"LLM cost: {cost}, response: {response}, text: {text}\")\n            response_dict = json.loads(response)\n            return response_dict.get(\"triplets\", [])\n        except Exception as e:\n            logger.error(f\"Error in LLM triplet extraction: {e}\")\n        return []\n\n    def llm_page_summary(self, page_content: str) -&gt; str:\n        \"\"\"\n\n        Args:\n            page_content (str): The content of the page\n\n        Returns:\n            summary (str): The summary of the page\n\n        \"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an assistant that can summarize the content of a page.\n                                \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                        Please summarize the content of the page.\n\n                        \"{page_content}\"\n\n                        Return the summary within the json with the key \"summary\".\n                    \"\"\",\n                },\n            ]\n            response, cost = openai_call(messages)\n            self.cost += cost\n            logger.debug(\n                f\"LLM cost: {cost}, response: {response}, page_content: {page_content}\"\n            )\n            response_dict = json.loads(response)\n            return response_dict.get(\"summary\", \"\")\n        except Exception as e:\n            logger.error(f\"Error in LLM page summary: {e}\")\n        return \"\"\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.__init__","title":"<code>__init__(folder_path, llm_enabled=False, input_format='pdf_exported')</code>","text":"<p>Initialize the SemanticKG class Args:     folder_path (Path): The path to the pdf file     llm_enabled (bool, optional): Whether to use LLM. Defaults to False.</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def __init__(\n    self,\n    folder_path: Path,\n    llm_enabled: bool = False,\n    input_format: str = \"pdf_exported\",\n):\n    \"\"\"\n    Initialize the SemanticKG class\n    Args:\n        folder_path (Path): The path to the pdf file\n        llm_enabled (bool, optional): Whether to use LLM. Defaults to False.\n\n    \"\"\"\n    self.folder_path = folder_path\n    self.llm_enabled = llm_enabled\n    self.cost = 0\n    logger.info(\"LLM is enabled\" if self.llm_enabled else \"LLM is disabled\")\n    self.kg_folder = self.folder_path / \"kg\"\n    if not self.kg_folder.exists():\n        self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n    self.layout_kg_file = self.kg_folder / \"layout_kg.json\"\n    # if layout_kg does not exist, then raise an error\n    if not self.layout_kg_file.exists():\n        raise FileNotFoundError(f\"{self.layout_kg_file} does not exist\")\n    # load layout_kg\n    self.layout_kg = self.load_kg(self.layout_kg_file)\n    self.input_format = input_format\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.add_semantic_kg","title":"<code>add_semantic_kg()</code>","text":"<p>As discussed in the plan, we will add the semantic knowledge graph based on the layout knowledge graph</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def add_semantic_kg(self):\n    \"\"\"\n    As discussed in the plan, we will add the semantic knowledge graph based on the layout knowledge graph\n\n    Returns:\n\n    \"\"\"\n    # we will start with the image to content\n    if self.input_format == \"pdf_exported\":\n        self.semantic_link_image_to_content()\n        self.semantic_link_table_to_content()\n    if self.input_format != \"excel\":\n        self.semantic_page_summary()\n    self.semantic_text2kg()\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.export_kg","title":"<code>export_kg()</code>","text":"<p>Export the semantic knowledge graph to a JSON file</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def export_kg(self):\n    \"\"\"\n    Export the semantic knowledge graph to a JSON file\n    \"\"\"\n\n    with open(self.layout_kg_file, \"w\") as f:\n        json.dump(self.layout_kg, f, indent=4)\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.llm_detect_caption","title":"<code>llm_detect_caption(text)</code>","text":"<p>Use LLM to detect whether the given text is a caption for an image or table.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be evaluated.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the text is identified as a caption, False otherwise.</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def llm_detect_caption(self, text: str) -&gt; bool:\n    \"\"\"\n    Use LLM to detect whether the given text is a caption for an image or table.\n\n    Args:\n        text (str): The text to be evaluated.\n\n    Returns:\n        bool: True if the text is identified as a caption, False otherwise.\n    \"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a system that help to detect if the text is a caption for an image or table.\n                            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Is the following text a caption for image or table?\n                    You need to detect if the text is a caption for an image or table.\n\n                    Please return the result in JSON format as follows:\n                        - {'is_caption': 1} if it is a caption,\n                        - or {'is_caption': 0} if it is not a caption.\n\n                    Some examples are captions for images or tables:\n                    - \"Figure 1.1: The distribution of the population\"\n                    - \"Table 2.1: The distribution of the population\"\n                    - \"Plate 1.1: The distribution of the population\"\n                    - \"Graph 1.1: The distribution of the population\"\n\n                    \"{text}\"\n                \"\"\",\n            },\n        ]\n        response, cost = openai_call(messages)\n        self.cost += cost\n        logger.debug(f\"LLM cost: {cost}, response: {response}, text: {text}\")\n        response_dict = json.loads(response)\n        return response_dict.get(\"is_caption\", 0) == 1\n    except Exception as e:\n        logger.error(f\"Error in LLM caption detection: {e}\")\n    return False\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.llm_detect_caption_mentions","title":"<code>llm_detect_caption_mentions(caption)</code>","text":"<p>Use LLM to detect the mentions of the given caption in the document.</p> <p>Parameters:</p> Name Type Description Default <code>caption</code> <code>str</code> <p>The caption text.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>List[str]: The list of uuids where the caption is mentioned.</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def llm_detect_caption_mentions(self, caption: str) -&gt; Optional[str]:\n    \"\"\"\n    Use LLM to detect the mentions of the given caption in the document.\n\n    Args:\n        caption (str): The caption text.\n\n    Returns:\n        List[str]: The list of uuids where the caption is mentioned.\n    \"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an assistant that can detect the unique description\n                              of a caption in a document.\n                            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Please find the unique description of the caption in the document.\n\n                    For example, if the caption is \"Plate 1.1: The distribution of the population\",\n                    the unique description is \"Plate 1.1\".\n\n                    Given caption:\n\n                    \"{caption}\"\n\n                    Return the str within the json with the key \"uid\".\n                \"\"\",\n            },\n        ]\n        response, cost = openai_call(messages)\n        self.cost += cost\n        logger.debug(f\"LLM cost: {cost}, response: {response}, caption: {caption}\")\n        response_dict = json.loads(response)\n        return response_dict.get(\"uid\", \"\")\n    except Exception as e:\n        logger.error(f\"Error in LLM caption mentions detection: {e}\")\n        logger.exception(e)\n    return None\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.llm_extract_triplet","title":"<code>llm_extract_triplet(text)</code>","text":"<p>Extract the triplet from the text Args:     text (str): The text to extract the triplets from</p> <p>Returns:</p> Name Type Description <code>triplets</code> <code>List[dict]</code> <p>The list of triplets extracted from the text</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def llm_extract_triplet(self, text) -&gt; List[dict]:\n    \"\"\"\n    Extract the triplet from the text\n    Args:\n        text (str): The text to extract the triplets from\n\n\n    Returns:\n        triplets (List[dict]): The list of triplets extracted from the text\n    \"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an assistant that can extract the triplets from a given text.\n                            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Please extract the triplets from the following text:\n\n                    \"{text}\"\n\n                    Return the triplets within the json with the key \"triplets\".\n                    And the triplets should be in the format of a list of dictionaries,\n                    each dictionary should have the following keys:\n                    - subject\n                    - subject_ner_type\n                    - predicate\n                    - object\n                    - object_ner_type\n\n                \"\"\",\n            },\n        ]\n        response, cost = openai_call(messages)\n        self.cost += cost\n        logger.debug(f\"LLM cost: {cost}, response: {response}, text: {text}\")\n        response_dict = json.loads(response)\n        return response_dict.get(\"triplets\", [])\n    except Exception as e:\n        logger.error(f\"Error in LLM triplet extraction: {e}\")\n    return []\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.llm_page_summary","title":"<code>llm_page_summary(page_content)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>page_content</code> <code>str</code> <p>The content of the page</p> required <p>Returns:</p> Name Type Description <code>summary</code> <code>str</code> <p>The summary of the page</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def llm_page_summary(self, page_content: str) -&gt; str:\n    \"\"\"\n\n    Args:\n        page_content (str): The content of the page\n\n    Returns:\n        summary (str): The summary of the page\n\n    \"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an assistant that can summarize the content of a page.\n                            \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Please summarize the content of the page.\n\n                    \"{page_content}\"\n\n                    Return the summary within the json with the key \"summary\".\n                \"\"\",\n            },\n        ]\n        response, cost = openai_call(messages)\n        self.cost += cost\n        logger.debug(\n            f\"LLM cost: {cost}, response: {response}, page_content: {page_content}\"\n        )\n        response_dict = json.loads(response)\n        return response_dict.get(\"summary\", \"\")\n    except Exception as e:\n        logger.error(f\"Error in LLM page summary: {e}\")\n    return \"\"\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.load_kg","title":"<code>load_kg(file_path)</code>  <code>staticmethod</code>","text":"<p>Load the knowledge graph from JSON</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The path to the JSON file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The knowledge graph</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>@staticmethod\ndef load_kg(file_path: Path) -&gt; dict:\n    \"\"\"\n    Load the knowledge graph from JSON\n\n    Args:\n        file_path (Path): The path to the JSON file\n\n    Returns:\n        dict: The knowledge graph\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        kg = json.load(f)\n    return kg\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_link_image_to_content","title":"<code>semantic_link_image_to_content()</code>","text":"<p>Link the image to the content</p> <ol> <li>We will need to extract the image's caption and reference point</li> <li>Use this caption or 1.1 to search the context, link the image to where the image is mentioned</li> </ol> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def semantic_link_image_to_content(self):\n    \"\"\"\n    Link the image to the content\n\n    1. We will need to extract the image's caption and reference point\n    2. Use this caption or 1.1 to search the context, link the image to where the image is mentioned\n\n    Returns:\n\n    \"\"\"\n\n    # first locate the image caption\n    for page in self.layout_kg[\"children\"]:\n        # within the page node, then it should have the children start with the image node\n        for child in page[\"children\"]:\n            if child[\"node_type\"] == \"image\":\n                # child now is the image node\n                # if this child do not have children, then we will skip\n                if \"children\" not in child or len(child[\"children\"]) == 0:\n                    continue\n                # logger.info(child)\n                for item in child[\"children\"]:\n                    # if this is the caption, then we will extract the text\n                    text = item[\"node_properties\"][\"content\"]\n                    if self.util_caption_detection(text):\n                        logger.info(f\"Figure/Caption detected: {text}\")\n                        # we will use this\n                        child[\"node_properties\"][\"caption\"] = text\n                        \"\"\"\n                        Link the caption to where it is mentioned\n\n                        For example, if the caption is \"Figure 1.1: The distribution of the population\",\n                        then we will search the context\n                        And found out a place indicate that: as shown in Figure 1.1,\n                        the distribution of the population is ...\n\n                        We need to find a way to match it back to the content\n\n                        Current plan of attack, we use rule based way.\n\n                        If there is a Figure XX, then we will search the context for Figure XX,\n                        and link it back to the content\n                        Because the content have not been\n                        \"\"\"\n\n                        uuids, caption = self.util_caption_mentions_detect(\n                            caption=text\n                        )\n                        logger.info(f\"UUIDs: {uuids}\")\n                        child[\"node_properties\"][\"mentioned_in\"] = uuids\n                        if caption:\n                            child[\"node_properties\"][\"unique_description\"] = caption\n                        continue\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_link_table_to_content","title":"<code>semantic_link_table_to_content()</code>","text":"<p>Link the table to the content</p> <p>So we will do the same thing first for the table</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def semantic_link_table_to_content(self):\n    \"\"\"\n    Link the table to the content\n\n    So we will do the same thing first for the table\n\n    Returns:\n\n    \"\"\"\n    for page in self.layout_kg[\"children\"]:\n        # within the page node, then it should have the children start with the image node\n        for child in page[\"children\"]:\n            if child[\"node_type\"] == \"table_csv\":\n                # child now is the image node\n                # if this child do not have children, then we will skip\n                if \"children\" not in child or len(child[\"children\"]) == 0:\n                    continue\n                # logger.info(child)\n                for item in child[\"children\"]:\n                    # if this is the caption, then we will extract the text\n                    text = item[\"node_properties\"][\"content\"]\n                    if self.util_caption_detection(text):\n                        logger.info(f\"Table/Caption detected: {text}\")\n                        # we will use this\n                        child[\"node_properties\"][\"caption\"] = text\n                        uuids, caption = self.util_caption_mentions_detect(\n                            caption=text\n                        )\n                        logger.info(f\"UUIDs: {uuids}\")\n                        child[\"node_properties\"][\"mentioned_in\"] = uuids\n                        if caption:\n                            child[\"node_properties\"][\"unique_description\"] = caption\n                        continue\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_page_summary","title":"<code>semantic_page_summary()</code>","text":"<p>Summary of the page, which will have better understanding of the page.</p> <p>Not sure whether this will enhance the RAG or not.</p> <p>But will be easier for human to understand the page.</p> <p>When doing summary, also need to give up page and later page information</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def semantic_page_summary(self):\n    \"\"\"\n    Summary of the page, which will have better understanding of the page.\n\n    Not sure whether this will enhance the RAG or not.\n\n    But will be easier for human to understand the page.\n\n    When doing summary, also need to give up page and later page information\n\n    Returns:\n\n    \"\"\"\n    for page_index, page in enumerate(self.layout_kg[\"children\"]):\n        if page[\"node_type\"] == \"page\":\n            page_content = page[\"node_properties\"][\"page_text\"]\n            logger.debug(page_content)\n            summary = self.llm_page_summary(page_content)\n            page[\"node_properties\"][\"summary\"] = summary\n\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_text2kg","title":"<code>semantic_text2kg()</code>","text":""},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_text2kg--general-goal-of-this","title":"General Goal of this:","text":"<ul> <li>A list of triplet: (subject, predicate, object)</li> <li>Triplets will be associated to the tree</li> <li>Frequent subject will be merged, and linked</li> </ul> <p>Plan of attack:</p> <ol> <li>We need to do the Named Entity Recognition for each sentence</li> <li>Do NER coexist relationship</li> <li>Last step will be extracting the semantic NER vs NER relationship</li> </ol> <p>How to construction the relation?</p> <ul> <li>We will grab the entities mapping to text uuid {    \"ner_type\": {     \"entities\": [uuid1, uuid2]    } }</li> </ul> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def semantic_text2kg(self):\n    \"\"\"\n    ## General Goal of this:\n\n    - A list of triplet: (subject, predicate, object)\n    - Triplets will be associated to the tree\n    - Frequent subject will be merged, and linked\n\n    Plan of attack:\n\n    1. We need to do the Named Entity Recognition for each sentence\n    2. Do NER coexist relationship\n    3. Last step will be extracting the semantic NER vs NER relationship\n\n    How to construction the relation?\n\n    - We will grab the entities mapping to text uuid\n    {\n       \"ner_type\": {\n        \"entities\": [uuid1, uuid2]\n       }\n    }\n\n    \"\"\"\n    if self.llm_enabled:\n        current_cost = self.cost\n        # do the triple extraction\n        # self.semantic_triplet_extraction(self.layout_kg)\n        logger.info(\"Start the semantic text2kg extraction\")\n        nodes = self.semantic_triplet_extraction(self.layout_kg, [])\n        # use tqdm to show the progress\n        for node in tqdm(nodes, desc=\"Extracting triplets\"):\n            # extract the triplets from the text\n            text = node[\"node_properties\"][\"content\"]\n            logger.debug(text)\n            if text == \"\":\n                continue\n            triplets = self.llm_extract_triplet(text)\n            node[\"node_properties\"][\"text2kg\"] = triplets\n        self.export_kg()\n        logger.info(f\"LLM cost: {self.cost - current_cost}\")\n    else:\n        # Hard to do this without LLM\n        logger.info(\"LLM is not enabled, skip the semantic text2kg extraction\")\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.semantic_triplet_extraction","title":"<code>semantic_triplet_extraction(node, nodes)</code>","text":"<p>Extract tripplets from the text</p> <p>It will update the node with the Text2KG field, add a list of triplets Args:     node (dict): The node in the layout knowledge graph     nodes (List[dict]): The list of nodes</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def semantic_triplet_extraction(self, node: dict, nodes: List[dict]):\n    \"\"\"\n    Extract tripplets from the text\n\n    It will update the node with the Text2KG field, add a list of triplets\n    Args:\n        node (dict): The node in the layout knowledge graph\n        nodes (List[dict]): The list of nodes\n\n    Returns:\n\n    \"\"\"\n    for child in node[\"children\"]:\n        if \"children\" in child:\n            nodes = self.semantic_triplet_extraction(child, nodes)\n        content = child[\"node_properties\"].get(\"content\", \"\")\n        if not content:\n            continue\n        nodes.append(child)\n    return nodes\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.util_caption_detection","title":"<code>util_caption_detection(text)</code>","text":"<p>Give a text, detect if this is a caption for image or table</p> <p>If it is LLM enabled, then we will use LLM to detect the caption If it is not LLM enabled, we use keyword match     - Currently LLM performance not well</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def util_caption_detection(self, text: str) -&gt; bool:  # noqa\n    \"\"\"\n    Give a text, detect if this is a caption for image or table\n\n    If it is LLM enabled, then we will use LLM to detect the caption\n    If it is not LLM enabled, we use keyword match\n        - Currently LLM performance not well\n\n    Returns:\n\n    \"\"\"\n    for keyword in CAPTION_KEYWORDS:\n        if keyword in text.lower():\n            return True\n    # if self.llm_enabled:\n    #     return self.llm_detect_caption(text)\n    return False\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.util_caption_mentions_detect","title":"<code>util_caption_mentions_detect(caption)</code>","text":"<p>First we need to find the unique description for the caption.</p> <p>For example: Plate 1.1: The distribution of the population</p> <p>Plate 1.1 is the unique description</p> <p>We will need to search the whole document to find the reference point</p> <p>Parameters:</p> Name Type Description Default <code>caption</code> <code>str</code> <p>The caption text</p> required <p>Returns:</p> Name Type Description <code>uuids</code> <code>List[str]</code> <p>The list of uuids where the caption is mentioned</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def util_caption_mentions_detect(self, caption: str) -&gt; Tuple[List[str], str]:\n    \"\"\"\n\n    First we need to find the unique description for the caption.\n\n    For example: Plate 1.1: The distribution of the population\n\n    Plate 1.1 is the unique description\n\n    We will need to search the whole document to find the reference point\n\n    Args:\n        caption (str): The caption text\n\n\n    Returns:\n        uuids (List[str]): The list of uuids where the caption is mentioned\n\n    \"\"\"\n    # first extract the unique description\n    # Extract the unique description from the caption\n    keyword_patten = \"|\".join(CAPTION_KEYWORDS)\n    match = re.search(rf\"(\\b({keyword_patten}) \\d+(\\.\\d+)*\\b)\", caption.lower())\n    unique_description = None\n    if match:\n        unique_description = match.group(1)\n    else:\n        if self.llm_enabled:\n            \"\"\"\n            Try to use LLM to do this work\n            \"\"\"\n            unique_description = self.llm_detect_caption_mentions(caption)\n            logger.info(f\"Unique description: {unique_description}\")\n\n    if not unique_description:\n        return []\n    logger.info(f\"Unique description: {unique_description}\")\n    mentioned_uuids = []\n    # search the context\n    mentioned_uuids = self.util_mentioned_uuids(\n        self.layout_kg, unique_description, mentioned_uuids\n    )\n    return mentioned_uuids, unique_description\n</code></pre>"},{"location":"sources/kg/semantic_kg/#Docs2KG.kg.semantic_kg.SemanticKG.util_mentioned_uuids","title":"<code>util_mentioned_uuids(node, unique_description, uuids)</code>","text":"<p>Search the context for the unique description</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>dict</code> <p>The node in the layout knowledge graph</p> required <code>unique_description</code> <code>str</code> <p>The unique description extracted from the caption</p> required <code>uuids</code> <code>List[str]</code> <p>The list of uuids where the unique description is mentioned</p> required <p>Returns:</p> Name Type Description <code>uuids</code> <code>List[str]</code> <p>The list of uuids where the unique description is mentioned</p> Source code in <code>Docs2KG/kg/semantic_kg.py</code> <pre><code>def util_mentioned_uuids(\n    self, node: dict, unique_description: str, uuids: List[str]\n) -&gt; List[str]:\n    \"\"\"\n    Search the context for the unique description\n\n    Args:\n        node (dict): The node in the layout knowledge graph\n        unique_description (str): The unique description extracted from the caption\n        uuids (List[str]): The list of uuids where the unique description is mentioned\n\n    Returns:\n        uuids (List[str]): The list of uuids where the unique description is mentioned\n    \"\"\"\n    for child in node[\"children\"]:\n        if \"node_properties\" in child:\n            if \"content\" in child[\"node_properties\"]:\n                if (\n                    unique_description\n                    in child[\"node_properties\"][\"content\"].lower()\n                ):\n                    uuids.append(child[\"uuid\"])\n        if \"children\" in child:\n            uuids = self.util_mentioned_uuids(child, unique_description, uuids)\n    return uuids\n</code></pre>"},{"location":"sources/kg/web_layout_kg/","title":"Web layout kg","text":""},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.logger","title":"<code>logger = get_logger(__name__)</code>  <code>module-attribute</code>","text":"<p>TODO:</p> <ul> <li>Try to extract the image and file captions</li> </ul>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG","title":"<code>WebLayoutKG</code>","text":"Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>class WebLayoutKG:\n    def __init__(\n        self, url: str, output_dir: Path = None, input_dir: Path = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize the WebParserBase class\n\n        Args:\n            url (str): URL to download the HTML files\n            output_dir (Path): Path to the output directory where the converted files will be saved\n            input_dir (Path): Path to the input directory where the html files will be downloaded\n        \"\"\"\n        self.url = url\n        # extract the domain from the url, if it is http://example.com/sss, then the domain is https://example.com\n        self.domain = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n\n        self.output_dir = output_dir\n        self.input_dir = input_dir\n        self.quoted_url = quote(url, \"\")\n        if self.output_dir is None:\n            self.output_dir = DATA_OUTPUT_DIR / self.quoted_url\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.download_html_file()\n\n        self.kg_json = {}\n        self.kg_folder = self.output_dir / \"kg\"\n        self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n        # image and table output directories\n        self.image_output_dir = self.output_dir / \"images\"\n        self.image_output_dir.mkdir(parents=True, exist_ok=True)\n        self.table_output_dir = self.output_dir / \"tables\"\n        self.table_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def download_html_file(self):\n        \"\"\"\n        Download the html file from the url and save it to the input directory\n\n        \"\"\"\n        response = requests.get(self.url)\n        if response.status_code == 200:\n            with open(f\"{DATA_INPUT_DIR}/index.html\", \"wb\") as f:\n                f.write(response.content)\n            logger.info(f\"Downloaded the HTML file from {self.url}\")\n        else:\n            logger.error(f\"Failed to download the HTML file from {self.url}\")\n\n    def create_kg(self):\n        \"\"\"\n        Create the knowledge graph from the HTML file\n\n        \"\"\"\n        with open(f\"{DATA_INPUT_DIR}/index.html\", \"r\") as f:\n            html_content = f.read()\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        \"\"\"\n        Loop and extract the whole soup into a tree\n        Each node will have\n\n        ```\n        {\n            \"uuid\": str,\n            \"node_type\": str,\n            \"node_properties\": {\n                \"content\": str,\n                // all other stuff\n            },\n            \"children\": List[Node]\n        }\n        ```\n        \"\"\"\n        self.kg_json = self.extract_kg(soup)\n        self.export_kg()\n\n    def extract_kg(self, soup):\n        \"\"\"\n        Extract the knowledge graph from the HTML file\n\n        Args:\n            soup (BeautifulSoup): Parsed HTML content\n\n        Returns:\n            dict: Knowledge graph in JSON format\n\n        \"\"\"\n        # FIXME: still not working properly\n        node = {\n            \"uuid\": str(uuid4()),\n            \"children\": [],\n        }\n\n        for child in soup.children:\n            if child.name is not None and soup.name != \"table\":\n                child_node = self.extract_kg(child)\n                node[\"children\"].append(child_node)\n        # content should be text if exists, if not, leave \"\"\n        content = str(soup.text) if soup.text is not None else \"\"\n        content = content.strip()\n        logger.info(content)\n        logger.info(soup.name)\n        # if there is no parent, then it is the root node, which we call it document\n        node_type = str(soup.name) if soup.name is not None else \"text\"\n        if \"document\" in node_type:\n            node_type = \"document\"\n\n        node[\"node_type\"] = node_type\n        soup_attr = soup.attrs\n        copied_soup = deepcopy(soup_attr)\n        for key in copied_soup.keys():\n            if \"-\" in key:\n                soup_attr[key.replace(\"-\", \"_\")] = copied_soup[key]\n                del soup_attr[key]\n            if \"$\" in key or \":\" in key:\n                del soup_attr[key]\n        node[\"node_properties\"] = {\"content\": content, **soup_attr}\n        # if it is an image tag, then extract the image and save it to the output directory\n        if soup.name == \"img\":\n            img_url = soup.get(\"src\")\n            if not img_url.startswith(\"http\"):\n                img_url = self.domain + img_url\n            img_data = requests.get(img_url).content\n            img_name = img_url.split(\"/\")[-1]\n            logger.info(\"image_url\")\n            logger.info(img_url)\n            if \"?\" in img_name:\n                img_name = img_name.split(\"?\")[0]\n            with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n                f.write(img_data)\n            logger.info(f\"Extracted the HTML file from {self.url} to images\")\n            node[\"node_properties\"][\"img_path\"] = f\"{self.output_dir}/images/{img_name}\"\n        # if it is a table tag, then extract the table and save it to the output directory\n        if soup.name == \"table\":\n            rows = []\n            for row in soup.find_all(\"tr\"):\n                cells = [\n                    cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n                ]\n                rows.append(cells)\n            df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n            csv_filename = f\"{self.output_dir}/tables/{node['uuid']}.csv\"\n            df.to_csv(csv_filename, index=False)\n            logger.info(f\"Extracted the HTML file from {self.url} to tables\")\n            node[\"node_properties\"][\"table_path\"] = csv_filename\n        # remove the node from soup after extracting the image and table\n        soup.extract()\n        return node\n\n    def export_kg(self) -&gt; None:\n        \"\"\"\n        Export the knowledge graph to json file\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n            json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG.__init__","title":"<code>__init__(url, output_dir=None, input_dir=None)</code>","text":"<p>Initialize the WebParserBase class</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to download the HTML files</p> required <code>output_dir</code> <code>Path</code> <p>Path to the output directory where the converted files will be saved</p> <code>None</code> <code>input_dir</code> <code>Path</code> <p>Path to the input directory where the html files will be downloaded</p> <code>None</code> Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>def __init__(\n    self, url: str, output_dir: Path = None, input_dir: Path = None\n) -&gt; None:\n    \"\"\"\n    Initialize the WebParserBase class\n\n    Args:\n        url (str): URL to download the HTML files\n        output_dir (Path): Path to the output directory where the converted files will be saved\n        input_dir (Path): Path to the input directory where the html files will be downloaded\n    \"\"\"\n    self.url = url\n    # extract the domain from the url, if it is http://example.com/sss, then the domain is https://example.com\n    self.domain = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n\n    self.output_dir = output_dir\n    self.input_dir = input_dir\n    self.quoted_url = quote(url, \"\")\n    if self.output_dir is None:\n        self.output_dir = DATA_OUTPUT_DIR / self.quoted_url\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    self.download_html_file()\n\n    self.kg_json = {}\n    self.kg_folder = self.output_dir / \"kg\"\n    self.kg_folder.mkdir(parents=True, exist_ok=True)\n\n    # image and table output directories\n    self.image_output_dir = self.output_dir / \"images\"\n    self.image_output_dir.mkdir(parents=True, exist_ok=True)\n    self.table_output_dir = self.output_dir / \"tables\"\n    self.table_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG.create_kg","title":"<code>create_kg()</code>","text":"<p>Create the knowledge graph from the HTML file</p> Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>def create_kg(self):\n    \"\"\"\n    Create the knowledge graph from the HTML file\n\n    \"\"\"\n    with open(f\"{DATA_INPUT_DIR}/index.html\", \"r\") as f:\n        html_content = f.read()\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    \"\"\"\n    Loop and extract the whole soup into a tree\n    Each node will have\n\n    ```\n    {\n        \"uuid\": str,\n        \"node_type\": str,\n        \"node_properties\": {\n            \"content\": str,\n            // all other stuff\n        },\n        \"children\": List[Node]\n    }\n    ```\n    \"\"\"\n    self.kg_json = self.extract_kg(soup)\n    self.export_kg()\n</code></pre>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG.download_html_file","title":"<code>download_html_file()</code>","text":"<p>Download the html file from the url and save it to the input directory</p> Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>def download_html_file(self):\n    \"\"\"\n    Download the html file from the url and save it to the input directory\n\n    \"\"\"\n    response = requests.get(self.url)\n    if response.status_code == 200:\n        with open(f\"{DATA_INPUT_DIR}/index.html\", \"wb\") as f:\n            f.write(response.content)\n        logger.info(f\"Downloaded the HTML file from {self.url}\")\n    else:\n        logger.error(f\"Failed to download the HTML file from {self.url}\")\n</code></pre>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG.export_kg","title":"<code>export_kg()</code>","text":"<p>Export the knowledge graph to json file</p> Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>def export_kg(self) -&gt; None:\n    \"\"\"\n    Export the knowledge graph to json file\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"w\") as f:\n        json.dump(self.kg_json, f, indent=2)\n</code></pre>"},{"location":"sources/kg/web_layout_kg/#Docs2KG.kg.web_layout_kg.WebLayoutKG.extract_kg","title":"<code>extract_kg(soup)</code>","text":"<p>Extract the knowledge graph from the HTML file</p> <p>Parameters:</p> Name Type Description Default <code>soup</code> <code>BeautifulSoup</code> <p>Parsed HTML content</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Knowledge graph in JSON format</p> Source code in <code>Docs2KG/kg/web_layout_kg.py</code> <pre><code>def extract_kg(self, soup):\n    \"\"\"\n    Extract the knowledge graph from the HTML file\n\n    Args:\n        soup (BeautifulSoup): Parsed HTML content\n\n    Returns:\n        dict: Knowledge graph in JSON format\n\n    \"\"\"\n    # FIXME: still not working properly\n    node = {\n        \"uuid\": str(uuid4()),\n        \"children\": [],\n    }\n\n    for child in soup.children:\n        if child.name is not None and soup.name != \"table\":\n            child_node = self.extract_kg(child)\n            node[\"children\"].append(child_node)\n    # content should be text if exists, if not, leave \"\"\n    content = str(soup.text) if soup.text is not None else \"\"\n    content = content.strip()\n    logger.info(content)\n    logger.info(soup.name)\n    # if there is no parent, then it is the root node, which we call it document\n    node_type = str(soup.name) if soup.name is not None else \"text\"\n    if \"document\" in node_type:\n        node_type = \"document\"\n\n    node[\"node_type\"] = node_type\n    soup_attr = soup.attrs\n    copied_soup = deepcopy(soup_attr)\n    for key in copied_soup.keys():\n        if \"-\" in key:\n            soup_attr[key.replace(\"-\", \"_\")] = copied_soup[key]\n            del soup_attr[key]\n        if \"$\" in key or \":\" in key:\n            del soup_attr[key]\n    node[\"node_properties\"] = {\"content\": content, **soup_attr}\n    # if it is an image tag, then extract the image and save it to the output directory\n    if soup.name == \"img\":\n        img_url = soup.get(\"src\")\n        if not img_url.startswith(\"http\"):\n            img_url = self.domain + img_url\n        img_data = requests.get(img_url).content\n        img_name = img_url.split(\"/\")[-1]\n        logger.info(\"image_url\")\n        logger.info(img_url)\n        if \"?\" in img_name:\n            img_name = img_name.split(\"?\")[0]\n        with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n            f.write(img_data)\n        logger.info(f\"Extracted the HTML file from {self.url} to images\")\n        node[\"node_properties\"][\"img_path\"] = f\"{self.output_dir}/images/{img_name}\"\n    # if it is a table tag, then extract the table and save it to the output directory\n    if soup.name == \"table\":\n        rows = []\n        for row in soup.find_all(\"tr\"):\n            cells = [\n                cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n            ]\n            rows.append(cells)\n        df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n        csv_filename = f\"{self.output_dir}/tables/{node['uuid']}.csv\"\n        df.to_csv(csv_filename, index=False)\n        logger.info(f\"Extracted the HTML file from {self.url} to tables\")\n        node[\"node_properties\"][\"table_path\"] = csv_filename\n    # remove the node from soup after extracting the image and table\n    soup.extract()\n    return node\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/","title":"Json2triplets","text":""},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets","title":"<code>JSON2Triplets</code>","text":"<p>Convert JSON to triplets</p> <p>A JSON for all nodes:</p> <p>{     \"nodes\": [         {             \"uuid\": uuid1             \"labels\": [\"label1\", \"label2\"],             \"properties\": {                 \"prop1\": \"value1\",                 \"prop2\": \"value2\"             }         },         {             \"uuid\": uuid2             \"labels\": [\"label3\"],             \"properties\": {                 \"prop3\": \"value3\",                 \"prop4\": \"value4\"             }         }     ],     \"relationships\": [         {             \"start_node\": uuid1,             \"end_node\": uuid2,             \"type\": \"type1\",             \"properties\": {                 \"prop5\": \"value5\",                 \"prop6\": \"value6\"             }         }     ] }</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>class JSON2Triplets:\n    \"\"\"\n    Convert JSON to triplets\n\n    A JSON for all nodes:\n\n    {\n        \"nodes\": [\n            {\n                \"uuid\": uuid1\n                \"labels\": [\"label1\", \"label2\"],\n                \"properties\": {\n                    \"prop1\": \"value1\",\n                    \"prop2\": \"value2\"\n                }\n            },\n            {\n                \"uuid\": uuid2\n                \"labels\": [\"label3\"],\n                \"properties\": {\n                    \"prop3\": \"value3\",\n                    \"prop4\": \"value4\"\n                }\n            }\n        ],\n        \"relationships\": [\n            {\n                \"start_node\": uuid1,\n                \"end_node\": uuid2,\n                \"type\": \"type1\",\n                \"properties\": {\n                    \"prop5\": \"value5\",\n                    \"prop6\": \"value6\"\n                }\n            }\n        ]\n    }\n\n    \"\"\"\n\n    def __init__(self, folder_path: Path):\n        self.folder_path = folder_path\n        self.kg_folder = folder_path / \"kg\"\n        self.kg_json = self.load_kg()\n        self.triplets_json = {\"nodes\": [], \"relationships\": []}\n        self.entities_mapping = {}\n\n    def transform(self):\n        \"\"\"\n        Transform the JSON to triplets\n        \"\"\"\n\n        self.transform_node(self.kg_json)\n\n        self.transform_images()\n        self.transform_tables()\n        self.transform_text2kg(self.kg_json)\n\n        self.export_json()\n\n    def transform_node(self, node: dict, parent_uuid: str = None):\n        \"\"\"\n        Transform the node to triplets\n\n        For the relationship part.\n\n        Also need to consider the relation within the for loop.\n\n        Before and After\n\n        Args:\n            node (dict): The node\n            parent_uuid (str): The UUID of the node\n\n        Returns:\n\n        \"\"\"\n        labels = [node[\"node_type\"].upper()]\n        uuid = node[\"uuid\"]\n        properties = node[\"node_properties\"]\n        # deep copy the properties\n        copied_properties = self.clean_nested_properties(properties)\n        entity = {\"uuid\": uuid, \"labels\": labels, \"properties\": copied_properties}\n        self.triplets_json[\"nodes\"].append(entity)\n        rel = {\n            \"start_node\": parent_uuid,\n            \"end_node\": uuid,\n            \"type\": \"HAS_CHILD\",\n        }\n        self.triplets_json[\"relationships\"].append(rel)\n        for index, child in enumerate(node[\"children\"]):\n            # if the children is text_block, then stop here\n            before_node_uuid = None\n            if index &gt; 0:\n                before_node_uuid = node[\"children\"][index - 1][\"uuid\"]\n\n            if before_node_uuid is not None:\n                before_rel = {\n                    \"start_node\": before_node_uuid,\n                    \"end_node\": child[\"uuid\"],\n                    \"type\": \"BEFORE\",\n                }\n                self.triplets_json[\"relationships\"].append(before_rel)\n\n            if child[\"node_type\"] == \"text_block\":\n                continue\n            self.transform_node(child, parent_uuid=uuid)\n\n    @staticmethod\n    def clean_nested_properties(properties: dict):\n        \"\"\"\n        Clean the nested properties\n        Args:\n            properties:\n\n        Returns:\n\n        \"\"\"\n        copied_properties = deepcopy(properties)\n        if \"text2kg\" in copied_properties:\n            copied_properties.pop(\"text2kg\")\n        return copied_properties\n\n    def transform_images(self):\n        \"\"\"\n        Connect the image to nearby text\n        \"\"\"\n        for page in self.kg_json[\"children\"]:\n            for node in page[\"children\"]:\n                if node[\"node_type\"] == \"image\":\n                    image_uuid = node[\"uuid\"]\n                    # add text_block node and relationship\n                    # first add where the image is mentioned\n                    mentioned_in = node[\"node_properties\"].get(\"mentioned_in\", [])\n                    for mention_uuid in mentioned_in:\n                        mention_rel = {\n                            \"start_node\": image_uuid,\n                            \"end_node\": mention_uuid,\n                            \"type\": \"MENTIONED_IN\",\n                        }\n                        self.triplets_json[\"relationships\"].append(mention_rel)\n                    if \"children\" not in node:\n                        continue\n                    # then add the nearby text block\n                    for child in node[\"children\"]:\n                        if child[\"node_type\"] == \"text_block\":\n                            text_block_uuid = child[\"uuid\"]\n                            copied_properties = self.clean_nested_properties(\n                                child[\"node_properties\"]\n                            )\n                            self.triplets_json[\"nodes\"].append(\n                                {\n                                    \"uuid\": text_block_uuid,\n                                    \"labels\": [\"TEXT_BLOCK\"],\n                                    \"properties\": copied_properties,\n                                }\n                            )\n\n                            rel = {\n                                \"start_node\": image_uuid,\n                                \"end_node\": text_block_uuid,\n                                \"type\": \"NEARBY_TEXT\",\n                            }\n                            self.triplets_json[\"relationships\"].append(rel)\n                            # include where the text block belong to the tree\n                            text_block_linkage = child.get(\"linkage\", [])\n                            for linkage_uuid in text_block_linkage:\n                                linkage_rel = {\n                                    \"start_node\": text_block_uuid,\n                                    \"end_node\": linkage_uuid,\n                                    \"type\": \"TEXT_LINKAGE\",\n                                }\n                                self.triplets_json[\"relationships\"].append(linkage_rel)\n\n    def transform_tables(self):\n        \"\"\"\n        This is to transform the text into a format can be used in neo4j, etc.\n        Returns:\n\n        \"\"\"\n\n        for page in self.kg_json[\"children\"]:\n            for node in page[\"children\"]:\n                if node[\"node_type\"] == \"table_csv\":\n                    table_uuid = node[\"uuid\"]\n                    # add text_block node and relationship\n                    # first add where the table is mentioned\n                    mentioned_in = node[\"node_properties\"].get(\"mentioned_in\", [])\n                    for mention_uuid in mentioned_in:\n                        mention_rel = {\n                            \"start_node\": table_uuid,\n                            \"end_node\": mention_uuid,\n                            \"type\": \"MENTIONED_IN\",\n                        }\n                        self.triplets_json[\"relationships\"].append(mention_rel)\n                    if \"children\" not in node:\n                        continue\n                    # then add the nearby text block\n                    for child in node[\"children\"]:\n                        if child[\"node_type\"] == \"text_block\":\n                            text_block_uuid = child[\"uuid\"]\n                            copied_properties = self.clean_nested_properties(\n                                child[\"node_properties\"]\n                            )\n                            self.triplets_json[\"nodes\"].append(\n                                {\n                                    \"uuid\": text_block_uuid,\n                                    \"labels\": [\"TEXT_BLOCK\"],\n                                    \"properties\": copied_properties,\n                                }\n                            )\n\n                            rel = {\n                                \"start_node\": table_uuid,\n                                \"end_node\": text_block_uuid,\n                                \"type\": \"NEARBY_TEXT\",\n                            }\n                            self.triplets_json[\"relationships\"].append(rel)\n                            # include where the text block belong to the tree\n                            text_block_linkage = child.get(\"linkage\", [])\n                            for linkage_uuid in text_block_linkage:\n                                linkage_rel = {\n                                    \"start_node\": text_block_uuid,\n                                    \"end_node\": linkage_uuid,\n                                    \"type\": \"TEXT_LINKAGE\",\n                                }\n                                self.triplets_json[\"relationships\"].append(linkage_rel)\n\n    def transform_text2kg(self, node: dict):\n        \"\"\"\n\n        Loop through the kg, and then figure out the Text2KG part, get them into the triplets\n\n        However, before that we will need to give each Text2KG node an uuid\n        And if they are the same content, they should have the same uuid\n\n        Returns:\n\n        \"\"\"\n        for child in node[\"children\"]:\n            if \"children\" in child:\n                self.transform_text2kg(child)\n            text2kg_list = child[\"node_properties\"].get(\"text2kg\", [])\n            if len(text2kg_list) == 0:\n                continue\n            for text2kg in text2kg_list:\n\n                subject = text2kg.get(\"subject\", None)\n                subject_ner_type = text2kg.get(\"subject_ner_type\", None)\n                predicate = text2kg.get(\"predicate\", None)\n                object_ent = text2kg.get(\"object\", None)\n                object_ner_type = text2kg.get(\"object_ner_type\", None)\n                if any(\n                    [\n                        subject is None,\n                        predicate is None,\n                        object_ent is None,\n                        subject_ner_type is None,\n                        object_ner_type is None,\n                        subject == \"\",\n                        object_ent == \"\",\n                        predicate == \"\",\n                    ]\n                ):\n                    continue\n                # strip the text and then clean again\n                subject = subject.strip()\n                object_ent = object_ent.strip()\n                predicate = predicate.strip()\n                subject_ner_type = subject_ner_type.strip()\n                object_ner_type = object_ner_type.strip()\n                predicate = \"\".join([i for i in predicate if i.isalnum() or i == \" \"])\n                # should not start with number\n                if predicate and predicate[0].isdigit():\n                    continue\n                predicate = predicate.replace(\" \", \"_\")\n                if any(\n                    [\n                        subject == \"\",\n                        object_ent == \"\",\n                        predicate == \"\",\n                        subject_ner_type == \"\",\n                        object_ner_type == \"\",\n                    ]\n                ):\n                    continue\n                logger.info(f\"Text2KG: {text2kg}\")\n                # check if the subject is in the entities_mapping\n                if subject not in self.entities_mapping:\n                    self.entities_mapping[subject] = str(uuid4())\n                    # add the subject rel to the parent\n                    self.triplets_json[\"relationships\"].append(\n                        {\n                            \"start_node\": node[\"uuid\"],\n                            \"end_node\": self.entities_mapping[subject],\n                            \"type\": \"HAS_ENTITY\",\n                        }\n                    )\n                if object_ent not in self.entities_mapping:\n                    self.entities_mapping[object_ent] = str(uuid4())\n                    # add the object rel to the parent\n                    self.triplets_json[\"relationships\"].append(\n                        {\n                            \"start_node\": node[\"uuid\"],\n                            \"end_node\": self.entities_mapping[object_ent],\n                            \"type\": \"HAS_ENTITY\",\n                        }\n                    )\n                subject_uuid = self.entities_mapping[subject]\n                object_uuid = self.entities_mapping[object_ent]\n                # add the subject\n                subject_ner_type = \"\".join(\n                    [i for i in subject_ner_type if i.isalnum() or i == \" \"]\n                )\n                subject_ner_type = subject_ner_type.replace(\" \", \"_\")\n                # object_ner_type and subject_ner_type can not start with number\n                if subject_ner_type and subject_ner_type[0].isdigit():\n                    continue\n                self.triplets_json[\"nodes\"].append(\n                    {\n                        \"uuid\": subject_uuid,\n                        \"labels\": [\n                            \"ENTITY\",\n                            subject_ner_type.upper(),\n                            \"TEXT2KG\",\n                        ],\n                        \"properties\": {\"text\": subject},\n                    }\n                )\n                # add the object\n                # replace object_ner_type, clean all special characters, only keep the letters and numbers\n                object_ner_type = \"\".join(\n                    [i for i in object_ner_type if i.isalnum() or i == \" \"]\n                )\n                object_ner_type = object_ner_type.replace(\" \", \"_\")\n                if object_ner_type and object_ner_type[0].isdigit():\n                    continue\n                self.triplets_json[\"nodes\"].append(\n                    {\n                        \"uuid\": object_uuid,\n                        \"labels\": [\n                            \"ENTITY\",\n                            object_ner_type.upper(),\n                            \"TEXT2KG\",\n                        ],\n                        \"properties\": {\"text\": object_ent},\n                    }\n                )\n                # do same to the predicate\n\n                # add the relationship\n                rel = {\n                    \"start_node\": subject_uuid,\n                    \"end_node\": object_uuid,\n                    \"type\": predicate,\n                    \"properties\": {\"source\": \"TEXT2KG\"},\n                }\n                self.triplets_json[\"relationships\"].append(rel)\n\n    def load_kg(self) -&gt; dict:\n        \"\"\"\n        Load the layout knowledge graph from JSON\n        \"\"\"\n        with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n            kg_json = json.load(f)\n        return kg_json\n\n    def export_json(self):\n        \"\"\"\n        Export the triplets JSON\n        \"\"\"\n        # how many nodes\n        logger.info(f\"Number of nodes: {len(self.triplets_json['nodes'])}\")\n        # how many relationships\n        logger.info(\n            f\"Number of relationships: {len(self.triplets_json['relationships'])}\"\n        )\n        with open(self.kg_folder / \"triplets_kg.json\", \"w\") as f:\n            json.dump(self.triplets_json, f, indent=4)\n        logger.info(f\"Triplets JSON exported to {self.kg_folder / 'triplets_kg.json'}\")\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.clean_nested_properties","title":"<code>clean_nested_properties(properties)</code>  <code>staticmethod</code>","text":"<p>Clean the nested properties Args:     properties:</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>@staticmethod\ndef clean_nested_properties(properties: dict):\n    \"\"\"\n    Clean the nested properties\n    Args:\n        properties:\n\n    Returns:\n\n    \"\"\"\n    copied_properties = deepcopy(properties)\n    if \"text2kg\" in copied_properties:\n        copied_properties.pop(\"text2kg\")\n    return copied_properties\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.export_json","title":"<code>export_json()</code>","text":"<p>Export the triplets JSON</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def export_json(self):\n    \"\"\"\n    Export the triplets JSON\n    \"\"\"\n    # how many nodes\n    logger.info(f\"Number of nodes: {len(self.triplets_json['nodes'])}\")\n    # how many relationships\n    logger.info(\n        f\"Number of relationships: {len(self.triplets_json['relationships'])}\"\n    )\n    with open(self.kg_folder / \"triplets_kg.json\", \"w\") as f:\n        json.dump(self.triplets_json, f, indent=4)\n    logger.info(f\"Triplets JSON exported to {self.kg_folder / 'triplets_kg.json'}\")\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.load_kg","title":"<code>load_kg()</code>","text":"<p>Load the layout knowledge graph from JSON</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def load_kg(self) -&gt; dict:\n    \"\"\"\n    Load the layout knowledge graph from JSON\n    \"\"\"\n    with open(self.kg_folder / \"layout_kg.json\", \"r\") as f:\n        kg_json = json.load(f)\n    return kg_json\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.transform","title":"<code>transform()</code>","text":"<p>Transform the JSON to triplets</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def transform(self):\n    \"\"\"\n    Transform the JSON to triplets\n    \"\"\"\n\n    self.transform_node(self.kg_json)\n\n    self.transform_images()\n    self.transform_tables()\n    self.transform_text2kg(self.kg_json)\n\n    self.export_json()\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.transform_images","title":"<code>transform_images()</code>","text":"<p>Connect the image to nearby text</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def transform_images(self):\n    \"\"\"\n    Connect the image to nearby text\n    \"\"\"\n    for page in self.kg_json[\"children\"]:\n        for node in page[\"children\"]:\n            if node[\"node_type\"] == \"image\":\n                image_uuid = node[\"uuid\"]\n                # add text_block node and relationship\n                # first add where the image is mentioned\n                mentioned_in = node[\"node_properties\"].get(\"mentioned_in\", [])\n                for mention_uuid in mentioned_in:\n                    mention_rel = {\n                        \"start_node\": image_uuid,\n                        \"end_node\": mention_uuid,\n                        \"type\": \"MENTIONED_IN\",\n                    }\n                    self.triplets_json[\"relationships\"].append(mention_rel)\n                if \"children\" not in node:\n                    continue\n                # then add the nearby text block\n                for child in node[\"children\"]:\n                    if child[\"node_type\"] == \"text_block\":\n                        text_block_uuid = child[\"uuid\"]\n                        copied_properties = self.clean_nested_properties(\n                            child[\"node_properties\"]\n                        )\n                        self.triplets_json[\"nodes\"].append(\n                            {\n                                \"uuid\": text_block_uuid,\n                                \"labels\": [\"TEXT_BLOCK\"],\n                                \"properties\": copied_properties,\n                            }\n                        )\n\n                        rel = {\n                            \"start_node\": image_uuid,\n                            \"end_node\": text_block_uuid,\n                            \"type\": \"NEARBY_TEXT\",\n                        }\n                        self.triplets_json[\"relationships\"].append(rel)\n                        # include where the text block belong to the tree\n                        text_block_linkage = child.get(\"linkage\", [])\n                        for linkage_uuid in text_block_linkage:\n                            linkage_rel = {\n                                \"start_node\": text_block_uuid,\n                                \"end_node\": linkage_uuid,\n                                \"type\": \"TEXT_LINKAGE\",\n                            }\n                            self.triplets_json[\"relationships\"].append(linkage_rel)\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.transform_node","title":"<code>transform_node(node, parent_uuid=None)</code>","text":"<p>Transform the node to triplets</p> <p>For the relationship part.</p> <p>Also need to consider the relation within the for loop.</p> <p>Before and After</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>dict</code> <p>The node</p> required <code>parent_uuid</code> <code>str</code> <p>The UUID of the node</p> <code>None</code> <p>Returns:</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def transform_node(self, node: dict, parent_uuid: str = None):\n    \"\"\"\n    Transform the node to triplets\n\n    For the relationship part.\n\n    Also need to consider the relation within the for loop.\n\n    Before and After\n\n    Args:\n        node (dict): The node\n        parent_uuid (str): The UUID of the node\n\n    Returns:\n\n    \"\"\"\n    labels = [node[\"node_type\"].upper()]\n    uuid = node[\"uuid\"]\n    properties = node[\"node_properties\"]\n    # deep copy the properties\n    copied_properties = self.clean_nested_properties(properties)\n    entity = {\"uuid\": uuid, \"labels\": labels, \"properties\": copied_properties}\n    self.triplets_json[\"nodes\"].append(entity)\n    rel = {\n        \"start_node\": parent_uuid,\n        \"end_node\": uuid,\n        \"type\": \"HAS_CHILD\",\n    }\n    self.triplets_json[\"relationships\"].append(rel)\n    for index, child in enumerate(node[\"children\"]):\n        # if the children is text_block, then stop here\n        before_node_uuid = None\n        if index &gt; 0:\n            before_node_uuid = node[\"children\"][index - 1][\"uuid\"]\n\n        if before_node_uuid is not None:\n            before_rel = {\n                \"start_node\": before_node_uuid,\n                \"end_node\": child[\"uuid\"],\n                \"type\": \"BEFORE\",\n            }\n            self.triplets_json[\"relationships\"].append(before_rel)\n\n        if child[\"node_type\"] == \"text_block\":\n            continue\n        self.transform_node(child, parent_uuid=uuid)\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.transform_tables","title":"<code>transform_tables()</code>","text":"<p>This is to transform the text into a format can be used in neo4j, etc. Returns:</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def transform_tables(self):\n    \"\"\"\n    This is to transform the text into a format can be used in neo4j, etc.\n    Returns:\n\n    \"\"\"\n\n    for page in self.kg_json[\"children\"]:\n        for node in page[\"children\"]:\n            if node[\"node_type\"] == \"table_csv\":\n                table_uuid = node[\"uuid\"]\n                # add text_block node and relationship\n                # first add where the table is mentioned\n                mentioned_in = node[\"node_properties\"].get(\"mentioned_in\", [])\n                for mention_uuid in mentioned_in:\n                    mention_rel = {\n                        \"start_node\": table_uuid,\n                        \"end_node\": mention_uuid,\n                        \"type\": \"MENTIONED_IN\",\n                    }\n                    self.triplets_json[\"relationships\"].append(mention_rel)\n                if \"children\" not in node:\n                    continue\n                # then add the nearby text block\n                for child in node[\"children\"]:\n                    if child[\"node_type\"] == \"text_block\":\n                        text_block_uuid = child[\"uuid\"]\n                        copied_properties = self.clean_nested_properties(\n                            child[\"node_properties\"]\n                        )\n                        self.triplets_json[\"nodes\"].append(\n                            {\n                                \"uuid\": text_block_uuid,\n                                \"labels\": [\"TEXT_BLOCK\"],\n                                \"properties\": copied_properties,\n                            }\n                        )\n\n                        rel = {\n                            \"start_node\": table_uuid,\n                            \"end_node\": text_block_uuid,\n                            \"type\": \"NEARBY_TEXT\",\n                        }\n                        self.triplets_json[\"relationships\"].append(rel)\n                        # include where the text block belong to the tree\n                        text_block_linkage = child.get(\"linkage\", [])\n                        for linkage_uuid in text_block_linkage:\n                            linkage_rel = {\n                                \"start_node\": text_block_uuid,\n                                \"end_node\": linkage_uuid,\n                                \"type\": \"TEXT_LINKAGE\",\n                            }\n                            self.triplets_json[\"relationships\"].append(linkage_rel)\n</code></pre>"},{"location":"sources/kg/utils/json2triplets/#Docs2KG.kg.utils.json2triplets.JSON2Triplets.transform_text2kg","title":"<code>transform_text2kg(node)</code>","text":"<p>Loop through the kg, and then figure out the Text2KG part, get them into the triplets</p> <p>However, before that we will need to give each Text2KG node an uuid And if they are the same content, they should have the same uuid</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/utils/json2triplets.py</code> <pre><code>def transform_text2kg(self, node: dict):\n    \"\"\"\n\n    Loop through the kg, and then figure out the Text2KG part, get them into the triplets\n\n    However, before that we will need to give each Text2KG node an uuid\n    And if they are the same content, they should have the same uuid\n\n    Returns:\n\n    \"\"\"\n    for child in node[\"children\"]:\n        if \"children\" in child:\n            self.transform_text2kg(child)\n        text2kg_list = child[\"node_properties\"].get(\"text2kg\", [])\n        if len(text2kg_list) == 0:\n            continue\n        for text2kg in text2kg_list:\n\n            subject = text2kg.get(\"subject\", None)\n            subject_ner_type = text2kg.get(\"subject_ner_type\", None)\n            predicate = text2kg.get(\"predicate\", None)\n            object_ent = text2kg.get(\"object\", None)\n            object_ner_type = text2kg.get(\"object_ner_type\", None)\n            if any(\n                [\n                    subject is None,\n                    predicate is None,\n                    object_ent is None,\n                    subject_ner_type is None,\n                    object_ner_type is None,\n                    subject == \"\",\n                    object_ent == \"\",\n                    predicate == \"\",\n                ]\n            ):\n                continue\n            # strip the text and then clean again\n            subject = subject.strip()\n            object_ent = object_ent.strip()\n            predicate = predicate.strip()\n            subject_ner_type = subject_ner_type.strip()\n            object_ner_type = object_ner_type.strip()\n            predicate = \"\".join([i for i in predicate if i.isalnum() or i == \" \"])\n            # should not start with number\n            if predicate and predicate[0].isdigit():\n                continue\n            predicate = predicate.replace(\" \", \"_\")\n            if any(\n                [\n                    subject == \"\",\n                    object_ent == \"\",\n                    predicate == \"\",\n                    subject_ner_type == \"\",\n                    object_ner_type == \"\",\n                ]\n            ):\n                continue\n            logger.info(f\"Text2KG: {text2kg}\")\n            # check if the subject is in the entities_mapping\n            if subject not in self.entities_mapping:\n                self.entities_mapping[subject] = str(uuid4())\n                # add the subject rel to the parent\n                self.triplets_json[\"relationships\"].append(\n                    {\n                        \"start_node\": node[\"uuid\"],\n                        \"end_node\": self.entities_mapping[subject],\n                        \"type\": \"HAS_ENTITY\",\n                    }\n                )\n            if object_ent not in self.entities_mapping:\n                self.entities_mapping[object_ent] = str(uuid4())\n                # add the object rel to the parent\n                self.triplets_json[\"relationships\"].append(\n                    {\n                        \"start_node\": node[\"uuid\"],\n                        \"end_node\": self.entities_mapping[object_ent],\n                        \"type\": \"HAS_ENTITY\",\n                    }\n                )\n            subject_uuid = self.entities_mapping[subject]\n            object_uuid = self.entities_mapping[object_ent]\n            # add the subject\n            subject_ner_type = \"\".join(\n                [i for i in subject_ner_type if i.isalnum() or i == \" \"]\n            )\n            subject_ner_type = subject_ner_type.replace(\" \", \"_\")\n            # object_ner_type and subject_ner_type can not start with number\n            if subject_ner_type and subject_ner_type[0].isdigit():\n                continue\n            self.triplets_json[\"nodes\"].append(\n                {\n                    \"uuid\": subject_uuid,\n                    \"labels\": [\n                        \"ENTITY\",\n                        subject_ner_type.upper(),\n                        \"TEXT2KG\",\n                    ],\n                    \"properties\": {\"text\": subject},\n                }\n            )\n            # add the object\n            # replace object_ner_type, clean all special characters, only keep the letters and numbers\n            object_ner_type = \"\".join(\n                [i for i in object_ner_type if i.isalnum() or i == \" \"]\n            )\n            object_ner_type = object_ner_type.replace(\" \", \"_\")\n            if object_ner_type and object_ner_type[0].isdigit():\n                continue\n            self.triplets_json[\"nodes\"].append(\n                {\n                    \"uuid\": object_uuid,\n                    \"labels\": [\n                        \"ENTITY\",\n                        object_ner_type.upper(),\n                        \"TEXT2KG\",\n                    ],\n                    \"properties\": {\"text\": object_ent},\n                }\n            )\n            # do same to the predicate\n\n            # add the relationship\n            rel = {\n                \"start_node\": subject_uuid,\n                \"end_node\": object_uuid,\n                \"type\": predicate,\n                \"properties\": {\"source\": \"TEXT2KG\"},\n            }\n            self.triplets_json[\"relationships\"].append(rel)\n</code></pre>"},{"location":"sources/kg/utils/neo4j_connector/","title":"Neo4j connector","text":""},{"location":"sources/kg/utils/neo4j_connector/#Docs2KG.kg.utils.neo4j_connector.Neo4jLoader","title":"<code>Neo4jLoader</code>","text":"Source code in <code>Docs2KG/kg/utils/neo4j_connector.py</code> <pre><code>class Neo4jLoader:\n    def __init__(\n        self,\n        uri: str,\n        username: str,\n        password: str,\n        json_file_path: Path,\n        clean: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            uri (str): URI of the Neo4j database\n            username (str): Username of the Neo4j database\n            password (str): Password of the Neo4j database\n            json_file_path (Path): Path to the JSON file containing the data\n            clean (bool): Whether to clean the database before loading the data\n        \"\"\"\n        self.uri = uri\n        self.username = username\n        self.password = password\n        self.json_file_path = json_file_path\n        self.driver = GraphDatabase.driver(\n            self.uri, auth=(self.username, self.password)\n        )\n        self.clean = clean\n\n        if self.clean:\n            self.clean_database()\n\n    def clean_database(self):\n        with self.driver.session() as session:\n            session.run(\"MATCH (n) DETACH DELETE n\")\n            logger.info(\"Database cleaned successfully\")\n\n    def close(self):\n        self.driver.close()\n\n    def load_json_data(self):\n        with open(self.json_file_path, \"r\") as file:\n            return json.load(file)\n\n    def load_nodes(self, nodes):\n        \"\"\"\n        It can be any type of node, not just Person\n\n        One example node is like this:\n\n        ```\n           {\n            \"uuid\": \"6cedef4a-52d1-4ff2-8fc8-644ad5de8c49\",\n            \"labels\": [\n                \"text_block\"\n            ],\n            \"properties\": {\n                \"text_block_bbox\": \"(373.5598449707031, 667.95703125, 490.0483093261719, 679.9588623046875)\",\n                \"content\": \"B.Sc., Geol, Grad Dip (GIS) \",\n                \"position\": \"right\",\n                \"text_block_number\": 9,\n                \"text2kg\": [\n                    {\n                        \"subject\": \"B.Sc.\",\n                        \"subject_ner_type\": \"Degree\",\n                        \"predicate\": \"has\",\n                        \"object\": \"Geol\",\n                        \"object_ner_type\": \"Subject\"\n                    },\n                    {\n                        \"subject\": \"B.Sc.\",\n                        \"subject_ner_type\": \"Degree\",\n                        \"predicate\": \"has\",\n                        \"object\": \"Grad Dip (GIS)\",\n                        \"object_ner_type\": \"Certificate\"\n                    }\n                ]\n            }\n        }\n        ```\n\n        Args:\n            nodes:\n\n        Returns:\n\n        \"\"\"\n        for node in tqdm(nodes, desc=\"Loading Nodes\"):\n            labels = \":\".join(node[\"labels\"])\n            properties = node[\"properties\"]\n\n            properties[\"uuid\"] = node[\"uuid\"]\n            properties[\"labels\"] = labels\n\n            # if the value of the property is a dictionary or a list, remove it\n            keys_to_remove = [\n                key\n                for key, value in properties.items()\n                if isinstance(value, (dict, list))\n            ]\n            for key in keys_to_remove:\n                properties.pop(key)\n\n            properties_query = \", \".join(\n                [\n                    f\"{key.replace('.', '_')}: ${key}\"\n                    for key in node[\"properties\"].keys()\n                ]\n            )\n            node_query = f\"\"\"\n              CREATE (n:{labels} {{ {properties_query} }})\n            \"\"\"\n            logger.debug(node_query)\n            logger.debug(properties)\n\n            with self.driver.session() as session:\n                session.run(node_query, **properties)\n\n    def load_relationships(self, relationships):\n        \"\"\"\n        Example like this:\n\n        ```\n        {\n            \"start_node\": \"49547ed0-0f86-418e-8dea-a269f7b002f6\",\n            \"end_node\": \"d9efb3d3-7b5c-49af-83b6-1d39b3f63912\",\n            \"type\": \"was issued in\",\n            \"properties\": {\n                \"source\": \"TEXT2KG\"\n            }\n        }\n        ```\n        Args:\n            relationships:\n\n        Returns:\n\n        \"\"\"\n        for relationship in tqdm(relationships, desc=\"Loading Relationships\"):\n            start_node = relationship[\"start_node\"]\n            end_node = relationship[\"end_node\"]\n            relationship_type = relationship[\"type\"]\n            properties = relationship.get(\"properties\", {})\n            properties_query = \", \".join(\n                [f\"{key}: ${key}\" for key in properties.keys()]\n            )\n            relationship_query = f\"\"\"\n            MATCH (start_node {{uuid: $start_node}}), (end_node {{uuid: $end_node}})\n            MERGE (start_node)-[r:{relationship_type}]-&gt;(end_node)\n            ON CREATE SET r += {{{properties_query}}}\n            \"\"\"\n\n            with self.driver.session() as session:\n                session.run(\n                    relationship_query,\n                    start_node=start_node,\n                    end_node=end_node,\n                    **properties,\n                )\n\n    def load_data(self):\n        data = self.load_json_data()\n        nodes = data[\"nodes\"]\n        relationships = data[\"relationships\"]\n        self.load_nodes(nodes)\n        self.load_relationships(relationships)\n        logger.info(\"Data loaded successfully to Neo4j\")\n</code></pre>"},{"location":"sources/kg/utils/neo4j_connector/#Docs2KG.kg.utils.neo4j_connector.Neo4jLoader.__init__","title":"<code>__init__(uri, username, password, json_file_path, clean=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>URI of the Neo4j database</p> required <code>username</code> <code>str</code> <p>Username of the Neo4j database</p> required <code>password</code> <code>str</code> <p>Password of the Neo4j database</p> required <code>json_file_path</code> <code>Path</code> <p>Path to the JSON file containing the data</p> required <code>clean</code> <code>bool</code> <p>Whether to clean the database before loading the data</p> <code>False</code> Source code in <code>Docs2KG/kg/utils/neo4j_connector.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    username: str,\n    password: str,\n    json_file_path: Path,\n    clean: bool = False,\n):\n    \"\"\"\n\n    Args:\n        uri (str): URI of the Neo4j database\n        username (str): Username of the Neo4j database\n        password (str): Password of the Neo4j database\n        json_file_path (Path): Path to the JSON file containing the data\n        clean (bool): Whether to clean the database before loading the data\n    \"\"\"\n    self.uri = uri\n    self.username = username\n    self.password = password\n    self.json_file_path = json_file_path\n    self.driver = GraphDatabase.driver(\n        self.uri, auth=(self.username, self.password)\n    )\n    self.clean = clean\n\n    if self.clean:\n        self.clean_database()\n</code></pre>"},{"location":"sources/kg/utils/neo4j_connector/#Docs2KG.kg.utils.neo4j_connector.Neo4jLoader.load_nodes","title":"<code>load_nodes(nodes)</code>","text":"<p>It can be any type of node, not just Person</p> <p>One example node is like this:</p> <pre><code>   {\n    \"uuid\": \"6cedef4a-52d1-4ff2-8fc8-644ad5de8c49\",\n    \"labels\": [\n        \"text_block\"\n    ],\n    \"properties\": {\n        \"text_block_bbox\": \"(373.5598449707031, 667.95703125, 490.0483093261719, 679.9588623046875)\",\n        \"content\": \"B.Sc., Geol, Grad Dip (GIS) \",\n        \"position\": \"right\",\n        \"text_block_number\": 9,\n        \"text2kg\": [\n            {\n                \"subject\": \"B.Sc.\",\n                \"subject_ner_type\": \"Degree\",\n                \"predicate\": \"has\",\n                \"object\": \"Geol\",\n                \"object_ner_type\": \"Subject\"\n            },\n            {\n                \"subject\": \"B.Sc.\",\n                \"subject_ner_type\": \"Degree\",\n                \"predicate\": \"has\",\n                \"object\": \"Grad Dip (GIS)\",\n                \"object_ner_type\": \"Certificate\"\n            }\n        ]\n    }\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>nodes</code> required <p>Returns:</p> Source code in <code>Docs2KG/kg/utils/neo4j_connector.py</code> <pre><code>def load_nodes(self, nodes):\n    \"\"\"\n    It can be any type of node, not just Person\n\n    One example node is like this:\n\n    ```\n       {\n        \"uuid\": \"6cedef4a-52d1-4ff2-8fc8-644ad5de8c49\",\n        \"labels\": [\n            \"text_block\"\n        ],\n        \"properties\": {\n            \"text_block_bbox\": \"(373.5598449707031, 667.95703125, 490.0483093261719, 679.9588623046875)\",\n            \"content\": \"B.Sc., Geol, Grad Dip (GIS) \",\n            \"position\": \"right\",\n            \"text_block_number\": 9,\n            \"text2kg\": [\n                {\n                    \"subject\": \"B.Sc.\",\n                    \"subject_ner_type\": \"Degree\",\n                    \"predicate\": \"has\",\n                    \"object\": \"Geol\",\n                    \"object_ner_type\": \"Subject\"\n                },\n                {\n                    \"subject\": \"B.Sc.\",\n                    \"subject_ner_type\": \"Degree\",\n                    \"predicate\": \"has\",\n                    \"object\": \"Grad Dip (GIS)\",\n                    \"object_ner_type\": \"Certificate\"\n                }\n            ]\n        }\n    }\n    ```\n\n    Args:\n        nodes:\n\n    Returns:\n\n    \"\"\"\n    for node in tqdm(nodes, desc=\"Loading Nodes\"):\n        labels = \":\".join(node[\"labels\"])\n        properties = node[\"properties\"]\n\n        properties[\"uuid\"] = node[\"uuid\"]\n        properties[\"labels\"] = labels\n\n        # if the value of the property is a dictionary or a list, remove it\n        keys_to_remove = [\n            key\n            for key, value in properties.items()\n            if isinstance(value, (dict, list))\n        ]\n        for key in keys_to_remove:\n            properties.pop(key)\n\n        properties_query = \", \".join(\n            [\n                f\"{key.replace('.', '_')}: ${key}\"\n                for key in node[\"properties\"].keys()\n            ]\n        )\n        node_query = f\"\"\"\n          CREATE (n:{labels} {{ {properties_query} }})\n        \"\"\"\n        logger.debug(node_query)\n        logger.debug(properties)\n\n        with self.driver.session() as session:\n            session.run(node_query, **properties)\n</code></pre>"},{"location":"sources/kg/utils/neo4j_connector/#Docs2KG.kg.utils.neo4j_connector.Neo4jLoader.load_relationships","title":"<code>load_relationships(relationships)</code>","text":"<p>Example like this:</p> <pre><code>{\n    \"start_node\": \"49547ed0-0f86-418e-8dea-a269f7b002f6\",\n    \"end_node\": \"d9efb3d3-7b5c-49af-83b6-1d39b3f63912\",\n    \"type\": \"was issued in\",\n    \"properties\": {\n        \"source\": \"TEXT2KG\"\n    }\n}\n</code></pre> <p>Args:     relationships:</p> <p>Returns:</p> Source code in <code>Docs2KG/kg/utils/neo4j_connector.py</code> <pre><code>def load_relationships(self, relationships):\n    \"\"\"\n    Example like this:\n\n    ```\n    {\n        \"start_node\": \"49547ed0-0f86-418e-8dea-a269f7b002f6\",\n        \"end_node\": \"d9efb3d3-7b5c-49af-83b6-1d39b3f63912\",\n        \"type\": \"was issued in\",\n        \"properties\": {\n            \"source\": \"TEXT2KG\"\n        }\n    }\n    ```\n    Args:\n        relationships:\n\n    Returns:\n\n    \"\"\"\n    for relationship in tqdm(relationships, desc=\"Loading Relationships\"):\n        start_node = relationship[\"start_node\"]\n        end_node = relationship[\"end_node\"]\n        relationship_type = relationship[\"type\"]\n        properties = relationship.get(\"properties\", {})\n        properties_query = \", \".join(\n            [f\"{key}: ${key}\" for key in properties.keys()]\n        )\n        relationship_query = f\"\"\"\n        MATCH (start_node {{uuid: $start_node}}), (end_node {{uuid: $end_node}})\n        MERGE (start_node)-[r:{relationship_type}]-&gt;(end_node)\n        ON CREATE SET r += {{{properties_query}}}\n        \"\"\"\n\n        with self.driver.session() as session:\n            session.run(\n                relationship_query,\n                start_node=start_node,\n                end_node=end_node,\n                **properties,\n            )\n</code></pre>"},{"location":"sources/modules/llm/image2description/","title":"Image2description","text":""},{"location":"sources/modules/llm/image2description/#Docs2KG.modules.llm.image2description.Image2Description","title":"<code>Image2Description</code>","text":"Source code in <code>Docs2KG/modules/llm/image2description.py</code> <pre><code>class Image2Description:\n    def __init__(self, image_path: Path):\n        \"\"\"\n        Extract Semantic Description from Image\n        \"\"\"\n        self.image_path = image_path\n\n    def get_image_description(self):\n        \"\"\"\n        Get the description of the image\n\n        Read the image, and then pass to the OpenAI API to get the description\n        \"\"\"\n        pass\n</code></pre>"},{"location":"sources/modules/llm/image2description/#Docs2KG.modules.llm.image2description.Image2Description.__init__","title":"<code>__init__(image_path)</code>","text":"<p>Extract Semantic Description from Image</p> Source code in <code>Docs2KG/modules/llm/image2description.py</code> <pre><code>def __init__(self, image_path: Path):\n    \"\"\"\n    Extract Semantic Description from Image\n    \"\"\"\n    self.image_path = image_path\n</code></pre>"},{"location":"sources/modules/llm/image2description/#Docs2KG.modules.llm.image2description.Image2Description.get_image_description","title":"<code>get_image_description()</code>","text":"<p>Get the description of the image</p> <p>Read the image, and then pass to the OpenAI API to get the description</p> Source code in <code>Docs2KG/modules/llm/image2description.py</code> <pre><code>def get_image_description(self):\n    \"\"\"\n    Get the description of the image\n\n    Read the image, and then pass to the OpenAI API to get the description\n    \"\"\"\n    pass\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/","title":"Markdown2json","text":""},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json","title":"<code>LLMMarkdown2Json</code>","text":"Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>class LLMMarkdown2Json:\n    def __init__(self, markdown_file: Path, llm_model_name: str = \"gpt-3.5-turbo-0125\"):\n        \"\"\"\n        Convert markdown to json using OpenAI LLM\n\n        There are two types of JSON (Structured format)\n\n        - JSON for layout structure\n        - JSON for content structure\n\n        It will be interesting to compare and see the difference between the two\n\n        Args:\n            markdown_file (Path): The path to the markdown file\n            llm_model_name (str): The OpenAI LLM model name\n        \"\"\"\n        self.markdown_file = markdown_file\n        if self.markdown_file.suffix != \".csv\":\n            raise ValueError(\"Only support csv\")\n        self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n        self.llm_model_name = llm_model_name\n        self.cost = 0\n\n    def clean_markdown(self) -&gt; Optional[str]:\n        \"\"\"\n        Prompt will give the LLM Markdown text\n\n        Ask it clean it, and then get the Markdown into proper format\n\n        Returns:\n            str: The cleaned Markdown text\n        \"\"\"\n        current_cost = self.cost\n        cleaned_markdown_csv = self.markdown_file.with_suffix(\".cleaned.csv\")\n        if cleaned_markdown_csv.exists():\n            logger.info(f\"{cleaned_markdown_csv} already exists\")\n            return\n        df = pd.read_csv(self.markdown_file)\n        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Layout JSON\"):\n            cleaned_markdown = self.openai_clean_markdown(row[\"text\"])\n            df.at[index, \"text\"] = cleaned_markdown\n        df.to_csv(cleaned_markdown_csv, index=False)\n        logger.info(f\"Cost: {self.cost - current_cost}\")\n\n    def extract2json(self):\n        if self.json_csv_file.exists():\n            logger.info(f\"{self.json_csv_file} already exists\")\n            return\n        logger.info(self.markdown_file)\n        current_cost = self.cost\n        df = pd.read_csv(self.markdown_file)\n\n        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Layout JSON\"):\n            df.at[index, \"layout_json\"] = self.openai_layout_json(row[\"text\"])\n        for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Content JSON\"):\n            df.at[index, \"content_json\"] = self.openai_content_json(row[\"text\"])\n\n        df.to_csv(self.json_csv_file, index=False)\n        logger.info(f\"Cost: {self.cost - current_cost}\")\n\n    def openai_clean_markdown(self, markdown):\n        \"\"\"\n        Use OpenAI LLM to clean the markdown\n\n        The markdown will be cleaned using the OpenAI LLM\n\n        Args:\n            markdown (str): The Markdown text\n\n        Returns:\n            str: The cleaned Markdown text\n        \"\"\"\n        try:\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are a helpful assistant to clean the markdown text.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                                You task is to clean the markdown.\n\n                                Steps include\n\n                                - Remove the content which is not meaningful, such as a single I character, etc\n                                - Do not need to keep special characters, such as `#`, `*`, etc, only keep\n                                    meaningful content\n                                - Make sure the markdown is fit with the markdown format\n                                - Only do remove for the noise characters, do not change any content\n\n                                Output should be a cleaned markdown text, which in str format\n\n                                It will stay in the response in json format\n                                with a key \"cleaned_markdown\"\n\n                                Clean the following markdown text:\\n\\n{markdown}\n                                \"\"\",\n                },\n            ]\n            markdown_json = self.llm_openai_call(messages)\n            logger.debug(markdown_json)\n            return json.loads(markdown_json).get(\"cleaned_markdown\", None)\n        except Exception as e:\n            logger.exception(e)\n            return None\n\n    def openai_layout_json(self, markdown):\n        \"\"\"\n        Use OpenAI LLM to convert markdown to json\n\n        The markdown will be converted to json using the OpenAI LLM\n\n        Output format should be like\n\n        Examples:\n            {\n            \"tag\": \"root\",\n            \"content\": {title},\n            \"children\": [\n                {\n                \"tag\": \"h1\",\n                \"content\": \"This is the header 1\",\n                \"children\": [\n                    {\n                    \"tag\": \"h2\",\n                    \"content\": \"This is the header 2\",\n                    \"children\": [\n                        ....\n                        {\n                        \"tag\": \"p\",\n                        \"content\": \"This is the paragraph\",\n                        \"children\": []\n                        }\n                    ]\n                    }\n                ]\n                }\n            ]\n            }\n\n        For Example:\n\n        ```markdown\n        # Title\n        ## Subtitle\n        ### Subtitle 2\n        - Item 1\n        - Item 2\n        - Item 3\n\n        ## Subtitle 3\n        - Item 1\n        - Item 2\n\n        This is a paragraph\n\n        ## Subtitle 4\n        - Item 1\n        - Item 2\n        ```\n\n        Should output as\n\n        ```json\n        {\n        \"tag\": \"h1\",\n        \"content\": \"Title\",\n        \"children\": [\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle\",\n                \"children\": [\n                    {\n                        \"tag\": \"h3\",\n                        \"content\": \"Subtitle 2\",\n                        \"children\": [\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 1\",\n                                \"children\": []\n                            },\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 2\",\n                                \"children\": []\n                            },\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 3\",\n                                \"children\": []\n                            }\n                        ]\n                    }\n                ]\n            },\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle 3\",\n                \"children\": [\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 1\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 2\",\n                        \"children\": []\n                    }\n                ]\n            },\n            {\n                \"tag\": \"p\",\n                \"content\": \"This is a paragraph\",\n                \"children\": []\n            },\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle 4\",\n                \"children\": [\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 1\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 2\",\n                        \"children\": []\n                    }\n                ]\n            }\n        ]\n        }\n        ```\n\n        Args:\n            markdown (str): The Markdown text\n\n        Returns:\n\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a helpful assistant to convert markdown to JSON format.\n\n        The markdown given to you will have some noise/not meaningful characters or information, you need to think\n        about cleaning the markdown into a cleaned version.\n\n        Then convert the markdown to json\n\n        For Example:\n\n        ```markdown\n        # Title\n        ## Subtitle\n        ### Subtitle 2\n        - Item 1\n        - Item 2\n        - Item 3\n\n        ## Subtitle 3\n        - Item 1\n        - Item 2\n\n        This is a paragraph\n\n        ## Subtitle 4\n        - Item 1\n        - Item 2\n        ```\n\n        Should output as\n\n        ```json\n        {\n        \"tag\": \"h1\",\n        \"content\": \"Title\",\n        \"children\": [\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle\",\n                \"children\": [\n                    {\n                        \"tag\": \"h3\",\n                        \"content\": \"Subtitle 2\",\n                        \"children\": [\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 1\",\n                                \"children\": []\n                            },\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 2\",\n                                \"children\": []\n                            },\n                            {\n                                \"tag\": \"li\",\n                                \"content\": \"Item 3\",\n                                \"children\": []\n                            }\n                        ]\n                    }\n                ]\n            },\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle 3\",\n                \"children\": [\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 1\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 2\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"p\",\n                        \"content\": \"This is a paragraph\",\n                        \"children\": []\n                    },\n                ]\n            },\n            {\n                \"tag\": \"h2\",\n                \"content\": \"Subtitle 4\",\n                \"children\": [\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 1\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 2\",\n                        \"children\": []\n                    }\n                ]\n            }\n        ]\n        }\n        ```\n\n        tag will be from the html convention like h1, h2, h3, p, li, etc.\n\n        Keep the meaningful hierarchy information within markdown via the html h1/h2/... tags\n        Get them proper in json format.\n\n        If it is a table, leave it as\n        {\n            \"tag\": \"table\",\n            \"content\": \"\",\n            \"children\": []\n        }\n        Content should the full content of the table, do not decompose further into tr/td/th, etc\n\n        One example can be\n        {\n            \"tag\": \"table\",\n            \"content\": \",header,header,\n                        value,value....\n                        \",\n            \"children\": []\n        }\n\n        \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Convert the following markdown to JSON format:\\n\\n{markdown}\",\n            },\n        ]\n        return self.llm_openai_call(messages)\n\n    def openai_content_json(self, markdown: str):\n        \"\"\"\n        Use OpenAI LLM to convert markdown to json\n\n        The markdown will be converted to json using the OpenAI LLM\n        Main focus here is to convert the content to json\n        Args:\n            markdown:\n\n        Returns:\n\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                    You are a helpful assistant to convert markdown to JSON format.\n                    You will be focusing on extracting meaningful key-value pairs from the markdown text.\n                    \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Convert the following markdown to JSON format:\\n\\n{markdown}\",\n            },\n        ]\n        return self.llm_openai_call(messages)\n\n    def llm_openai_call(self, messages: List[dict]) -&gt; str:\n        \"\"\"\n        Call the OpenAI API to get the response\n        Args:\n            messages (List[dict]): The messages to send to the OpenAI API\n\n\n        Returns:\n            response_json_str (str): The response from the OpenAI API\n        \"\"\"\n        result_json_str, cost = openai_call(messages, self.llm_model_name)\n        self.cost += cost\n        return result_json_str\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.__init__","title":"<code>__init__(markdown_file, llm_model_name='gpt-3.5-turbo-0125')</code>","text":"<p>Convert markdown to json using OpenAI LLM</p> <p>There are two types of JSON (Structured format)</p> <ul> <li>JSON for layout structure</li> <li>JSON for content structure</li> </ul> <p>It will be interesting to compare and see the difference between the two</p> <p>Parameters:</p> Name Type Description Default <code>markdown_file</code> <code>Path</code> <p>The path to the markdown file</p> required <code>llm_model_name</code> <code>str</code> <p>The OpenAI LLM model name</p> <code>'gpt-3.5-turbo-0125'</code> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def __init__(self, markdown_file: Path, llm_model_name: str = \"gpt-3.5-turbo-0125\"):\n    \"\"\"\n    Convert markdown to json using OpenAI LLM\n\n    There are two types of JSON (Structured format)\n\n    - JSON for layout structure\n    - JSON for content structure\n\n    It will be interesting to compare and see the difference between the two\n\n    Args:\n        markdown_file (Path): The path to the markdown file\n        llm_model_name (str): The OpenAI LLM model name\n    \"\"\"\n    self.markdown_file = markdown_file\n    if self.markdown_file.suffix != \".csv\":\n        raise ValueError(\"Only support csv\")\n    self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n    self.llm_model_name = llm_model_name\n    self.cost = 0\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.clean_markdown","title":"<code>clean_markdown()</code>","text":"<p>Prompt will give the LLM Markdown text</p> <p>Ask it clean it, and then get the Markdown into proper format</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The cleaned Markdown text</p> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def clean_markdown(self) -&gt; Optional[str]:\n    \"\"\"\n    Prompt will give the LLM Markdown text\n\n    Ask it clean it, and then get the Markdown into proper format\n\n    Returns:\n        str: The cleaned Markdown text\n    \"\"\"\n    current_cost = self.cost\n    cleaned_markdown_csv = self.markdown_file.with_suffix(\".cleaned.csv\")\n    if cleaned_markdown_csv.exists():\n        logger.info(f\"{cleaned_markdown_csv} already exists\")\n        return\n    df = pd.read_csv(self.markdown_file)\n    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Layout JSON\"):\n        cleaned_markdown = self.openai_clean_markdown(row[\"text\"])\n        df.at[index, \"text\"] = cleaned_markdown\n    df.to_csv(cleaned_markdown_csv, index=False)\n    logger.info(f\"Cost: {self.cost - current_cost}\")\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.llm_openai_call","title":"<code>llm_openai_call(messages)</code>","text":"<p>Call the OpenAI API to get the response Args:     messages (List[dict]): The messages to send to the OpenAI API</p> <p>Returns:</p> Name Type Description <code>response_json_str</code> <code>str</code> <p>The response from the OpenAI API</p> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def llm_openai_call(self, messages: List[dict]) -&gt; str:\n    \"\"\"\n    Call the OpenAI API to get the response\n    Args:\n        messages (List[dict]): The messages to send to the OpenAI API\n\n\n    Returns:\n        response_json_str (str): The response from the OpenAI API\n    \"\"\"\n    result_json_str, cost = openai_call(messages, self.llm_model_name)\n    self.cost += cost\n    return result_json_str\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.openai_clean_markdown","title":"<code>openai_clean_markdown(markdown)</code>","text":"<p>Use OpenAI LLM to clean the markdown</p> <p>The markdown will be cleaned using the OpenAI LLM</p> <p>Parameters:</p> Name Type Description Default <code>markdown</code> <code>str</code> <p>The Markdown text</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The cleaned Markdown text</p> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def openai_clean_markdown(self, markdown):\n    \"\"\"\n    Use OpenAI LLM to clean the markdown\n\n    The markdown will be cleaned using the OpenAI LLM\n\n    Args:\n        markdown (str): The Markdown text\n\n    Returns:\n        str: The cleaned Markdown text\n    \"\"\"\n    try:\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a helpful assistant to clean the markdown text.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                            You task is to clean the markdown.\n\n                            Steps include\n\n                            - Remove the content which is not meaningful, such as a single I character, etc\n                            - Do not need to keep special characters, such as `#`, `*`, etc, only keep\n                                meaningful content\n                            - Make sure the markdown is fit with the markdown format\n                            - Only do remove for the noise characters, do not change any content\n\n                            Output should be a cleaned markdown text, which in str format\n\n                            It will stay in the response in json format\n                            with a key \"cleaned_markdown\"\n\n                            Clean the following markdown text:\\n\\n{markdown}\n                            \"\"\",\n            },\n        ]\n        markdown_json = self.llm_openai_call(messages)\n        logger.debug(markdown_json)\n        return json.loads(markdown_json).get(\"cleaned_markdown\", None)\n    except Exception as e:\n        logger.exception(e)\n        return None\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.openai_content_json","title":"<code>openai_content_json(markdown)</code>","text":"<p>Use OpenAI LLM to convert markdown to json</p> <p>The markdown will be converted to json using the OpenAI LLM Main focus here is to convert the content to json Args:     markdown:</p> <p>Returns:</p> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def openai_content_json(self, markdown: str):\n    \"\"\"\n    Use OpenAI LLM to convert markdown to json\n\n    The markdown will be converted to json using the OpenAI LLM\n    Main focus here is to convert the content to json\n    Args:\n        markdown:\n\n    Returns:\n\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n                You are a helpful assistant to convert markdown to JSON format.\n                You will be focusing on extracting meaningful key-value pairs from the markdown text.\n                \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Convert the following markdown to JSON format:\\n\\n{markdown}\",\n        },\n    ]\n    return self.llm_openai_call(messages)\n</code></pre>"},{"location":"sources/modules/llm/markdown2json/#Docs2KG.modules.llm.markdown2json.LLMMarkdown2Json.openai_layout_json","title":"<code>openai_layout_json(markdown)</code>","text":"<p>Use OpenAI LLM to convert markdown to json</p> <p>The markdown will be converted to json using the OpenAI LLM</p> <p>Output format should be like</p> <p>Examples:</p> <p>{ \"tag\": \"root\", \"content\": {title}, \"children\": [     {     \"tag\": \"h1\",     \"content\": \"This is the header 1\",     \"children\": [         {         \"tag\": \"h2\",         \"content\": \"This is the header 2\",         \"children\": [             ....             {             \"tag\": \"p\",             \"content\": \"This is the paragraph\",             \"children\": []             }         ]         }     ]     } ] }</p> <p>For Example:</p> <pre><code># Title\n## Subtitle\n### Subtitle 2\n- Item 1\n- Item 2\n- Item 3\n\n## Subtitle 3\n- Item 1\n- Item 2\n\nThis is a paragraph\n\n## Subtitle 4\n- Item 1\n- Item 2\n</code></pre> <p>Should output as</p> <pre><code>{\n\"tag\": \"h1\",\n\"content\": \"Title\",\n\"children\": [\n    {\n        \"tag\": \"h2\",\n        \"content\": \"Subtitle\",\n        \"children\": [\n            {\n                \"tag\": \"h3\",\n                \"content\": \"Subtitle 2\",\n                \"children\": [\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 1\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 2\",\n                        \"children\": []\n                    },\n                    {\n                        \"tag\": \"li\",\n                        \"content\": \"Item 3\",\n                        \"children\": []\n                    }\n                ]\n            }\n        ]\n    },\n    {\n        \"tag\": \"h2\",\n        \"content\": \"Subtitle 3\",\n        \"children\": [\n            {\n                \"tag\": \"li\",\n                \"content\": \"Item 1\",\n                \"children\": []\n            },\n            {\n                \"tag\": \"li\",\n                \"content\": \"Item 2\",\n                \"children\": []\n            }\n        ]\n    },\n    {\n        \"tag\": \"p\",\n        \"content\": \"This is a paragraph\",\n        \"children\": []\n    },\n    {\n        \"tag\": \"h2\",\n        \"content\": \"Subtitle 4\",\n        \"children\": [\n            {\n                \"tag\": \"li\",\n                \"content\": \"Item 1\",\n                \"children\": []\n            },\n            {\n                \"tag\": \"li\",\n                \"content\": \"Item 2\",\n                \"children\": []\n            }\n        ]\n    }\n]\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>markdown</code> <code>str</code> <p>The Markdown text</p> required <p>Returns:</p> Source code in <code>Docs2KG/modules/llm/markdown2json.py</code> <pre><code>def openai_layout_json(self, markdown):\n    \"\"\"\n    Use OpenAI LLM to convert markdown to json\n\n    The markdown will be converted to json using the OpenAI LLM\n\n    Output format should be like\n\n    Examples:\n        {\n        \"tag\": \"root\",\n        \"content\": {title},\n        \"children\": [\n            {\n            \"tag\": \"h1\",\n            \"content\": \"This is the header 1\",\n            \"children\": [\n                {\n                \"tag\": \"h2\",\n                \"content\": \"This is the header 2\",\n                \"children\": [\n                    ....\n                    {\n                    \"tag\": \"p\",\n                    \"content\": \"This is the paragraph\",\n                    \"children\": []\n                    }\n                ]\n                }\n            ]\n            }\n        ]\n        }\n\n    For Example:\n\n    ```markdown\n    # Title\n    ## Subtitle\n    ### Subtitle 2\n    - Item 1\n    - Item 2\n    - Item 3\n\n    ## Subtitle 3\n    - Item 1\n    - Item 2\n\n    This is a paragraph\n\n    ## Subtitle 4\n    - Item 1\n    - Item 2\n    ```\n\n    Should output as\n\n    ```json\n    {\n    \"tag\": \"h1\",\n    \"content\": \"Title\",\n    \"children\": [\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle\",\n            \"children\": [\n                {\n                    \"tag\": \"h3\",\n                    \"content\": \"Subtitle 2\",\n                    \"children\": [\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 1\",\n                            \"children\": []\n                        },\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 2\",\n                            \"children\": []\n                        },\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 3\",\n                            \"children\": []\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle 3\",\n            \"children\": [\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 1\",\n                    \"children\": []\n                },\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 2\",\n                    \"children\": []\n                }\n            ]\n        },\n        {\n            \"tag\": \"p\",\n            \"content\": \"This is a paragraph\",\n            \"children\": []\n        },\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle 4\",\n            \"children\": [\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 1\",\n                    \"children\": []\n                },\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 2\",\n                    \"children\": []\n                }\n            ]\n        }\n    ]\n    }\n    ```\n\n    Args:\n        markdown (str): The Markdown text\n\n    Returns:\n\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"You are a helpful assistant to convert markdown to JSON format.\n\n    The markdown given to you will have some noise/not meaningful characters or information, you need to think\n    about cleaning the markdown into a cleaned version.\n\n    Then convert the markdown to json\n\n    For Example:\n\n    ```markdown\n    # Title\n    ## Subtitle\n    ### Subtitle 2\n    - Item 1\n    - Item 2\n    - Item 3\n\n    ## Subtitle 3\n    - Item 1\n    - Item 2\n\n    This is a paragraph\n\n    ## Subtitle 4\n    - Item 1\n    - Item 2\n    ```\n\n    Should output as\n\n    ```json\n    {\n    \"tag\": \"h1\",\n    \"content\": \"Title\",\n    \"children\": [\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle\",\n            \"children\": [\n                {\n                    \"tag\": \"h3\",\n                    \"content\": \"Subtitle 2\",\n                    \"children\": [\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 1\",\n                            \"children\": []\n                        },\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 2\",\n                            \"children\": []\n                        },\n                        {\n                            \"tag\": \"li\",\n                            \"content\": \"Item 3\",\n                            \"children\": []\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle 3\",\n            \"children\": [\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 1\",\n                    \"children\": []\n                },\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 2\",\n                    \"children\": []\n                },\n                {\n                    \"tag\": \"p\",\n                    \"content\": \"This is a paragraph\",\n                    \"children\": []\n                },\n            ]\n        },\n        {\n            \"tag\": \"h2\",\n            \"content\": \"Subtitle 4\",\n            \"children\": [\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 1\",\n                    \"children\": []\n                },\n                {\n                    \"tag\": \"li\",\n                    \"content\": \"Item 2\",\n                    \"children\": []\n                }\n            ]\n        }\n    ]\n    }\n    ```\n\n    tag will be from the html convention like h1, h2, h3, p, li, etc.\n\n    Keep the meaningful hierarchy information within markdown via the html h1/h2/... tags\n    Get them proper in json format.\n\n    If it is a table, leave it as\n    {\n        \"tag\": \"table\",\n        \"content\": \"\",\n        \"children\": []\n    }\n    Content should the full content of the table, do not decompose further into tr/td/th, etc\n\n    One example can be\n    {\n        \"tag\": \"table\",\n        \"content\": \",header,header,\n                    value,value....\n                    \",\n        \"children\": []\n    }\n\n    \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Convert the following markdown to JSON format:\\n\\n{markdown}\",\n        },\n    ]\n    return self.llm_openai_call(messages)\n</code></pre>"},{"location":"sources/modules/llm/openai_call/","title":"Openai call","text":""},{"location":"sources/modules/llm/openai_call/#Docs2KG.modules.llm.openai_call.openai_call","title":"<code>openai_call(messages, llm_model_name='gpt-3.5-turbo')</code>","text":"<p>Call the OpenAI API to get the response Args:     messages (List[dict]): The messages to send to the OpenAI API     llm_model_name (str): The name of the LLM model</p> <p>Returns:</p> Name Type Description <code>response_json_str</code> <code>str</code> <p>The response from the OpenAI API</p> <code>cost</code> <code>float</code> <p>The cost of the response</p> Source code in <code>Docs2KG/modules/llm/openai_call.py</code> <pre><code>def openai_call(\n    messages: List[dict], llm_model_name: str = \"gpt-3.5-turbo\"\n) -&gt; Tuple[str, float]:\n    \"\"\"\n    Call the OpenAI API to get the response\n    Args:\n        messages (List[dict]): The messages to send to the OpenAI API\n        llm_model_name (str): The name of the LLM model\n\n\n    Returns:\n        response_json_str (str): The response from the OpenAI API\n        cost (float): The cost of the response\n    \"\"\"\n    result_json_str = \"\"\n    cost = 0\n    while True:\n        response = client.chat.completions.create(\n            model=llm_model_name,\n            response_format={\"type\": \"json_object\"},\n            messages=messages,\n            temperature=0.0,\n        )\n        logger.debug(response)\n        content = response.choices[0].message.content\n        logger.debug(content)\n        result_json_str += content\n        cost += track_usage(response)\n        # if finish_reason is length, then it is not complete\n        logger.debug(response.choices[0].finish_reason)\n        if response.choices[0].finish_reason != \"length\":\n            break\n        else:\n            messages.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": content,\n                }\n            )\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Continue the response\",\n                }\n            )\n\n    return result_json_str, cost\n</code></pre>"},{"location":"sources/modules/llm/openai_embedding/","title":"Openai embedding","text":""},{"location":"sources/modules/llm/openai_embedding/#Docs2KG.modules.llm.openai_embedding.get_openai_embedding","title":"<code>get_openai_embedding(text, llm_emb_model='text-embedding-3-small')</code>","text":"<p>Get the embedding from OpenAI API</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to get the embedding</p> required <code>llm_emb_model</code> <code>str</code> <p>The name of the LLM model</p> <code>'text-embedding-3-small'</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The embedding of the text</p> Source code in <code>Docs2KG/modules/llm/openai_embedding.py</code> <pre><code>def get_openai_embedding(text: str, llm_emb_model: str = \"text-embedding-3-small\"):\n    \"\"\"\n    Get the embedding from OpenAI API\n\n    Args:\n        text (str): The text to get the embedding\n        llm_emb_model (str): The name of the LLM model\n\n    Returns:\n        str: The embedding of the text\n    \"\"\"\n    response = client.embeddings.create(input=text, model=llm_emb_model)\n\n    return response.data[0].embedding\n</code></pre>"},{"location":"sources/modules/llm/sheet2metadata/","title":"Sheet2metadata","text":""},{"location":"sources/modules/llm/sheet2metadata/#Docs2KG.modules.llm.sheet2metadata.Sheet2Metadata","title":"<code>Sheet2Metadata</code>","text":"Source code in <code>Docs2KG/modules/llm/sheet2metadata.py</code> <pre><code>class Sheet2Metadata:\n    def __init__(self, markdown_file: Path, llm_model_name: str = \"gpt-3.5-turbo-0125\"):\n        \"\"\"\n        1. Extract the descriptive part of the markdown\n        2. Summary the markdown\n\n        Args:\n            markdown_file (Path): The path to the markdown file\n            llm_model_name (str): The OpenAI LLM model name\n        \"\"\"\n        self.markdown_file = markdown_file\n        if self.markdown_file.suffix != \".csv\":\n            raise ValueError(\"Only support csv\")\n        self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n        self.llm_model_name = llm_model_name\n        self.cost = 0\n\n    def extract_metadata(self):\n        if self.json_csv_file.exists():\n            logger.info(f\"{self.json_csv_file} already exists\")\n            return\n        logger.info(self.markdown_file)\n        current_cost = self.cost\n        df = pd.read_csv(self.markdown_file)\n\n        for index, row in tqdm(\n            df.iterrows(), total=df.shape[0], desc=\"Summary and Description Extraction\"\n        ):\n            try:\n                summary, desc = self.openai_sheet_handler(row[\"text\"])\n                logger.info(summary)\n                logger.info(desc)\n                # if it is list, then we will join them\n                if isinstance(summary, list):\n                    summary = \" \".join(summary)\n                if isinstance(desc, list):\n                    desc = \" \".join(desc)\n                try:\n                    df.loc[index, \"summary\"] = summary\n                    df.loc[index, \"desc\"] = desc\n                except Exception as e:\n                    logger.error(e)\n                    df.loc[index, \"summary\"] = str(summary)\n                    df.loc[index, \"desc\"] = str(desc)\n            except Exception as e:\n                logger.error(e)\n                df.loc[index, \"summary\"] = str()\n                df.loc[index, \"desc\"] = \"\"\n                logger.error(f\"Error at index {index}\")\n\n        df.to_csv(self.json_csv_file, index=False)\n        logger.info(f\"Cost: {self.cost - current_cost}\")\n\n    def openai_sheet_handler(self, markdown: str):\n        \"\"\"\n        1. Use OpenAI to exclude the numerical part of the markdown, only keep the descriptive part\n        2. Summarize the markdown\n\n        Args:\n            markdown:\n\n        Returns:\n\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                    You are a helpful assistant, help us clean and understand the excel data\n                    You will need to first summary the markdwon content, it is normally some descriptive text with\n                    Table information.\n                    \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                            We will need you to do two things:\n                            1. Summarize the following markdown content into a description about the data:\n                                - What the data is about\n                                - What's the main point of the data\n                            2. Exclude the numerical part of the markdown, only keep the descriptive part.\n\n                            The markdown content is: \\n\\n{markdown}\n\n                            Return in JSON format with key \"summary\" and \"desc\"\n                            \"\"\",\n            },\n        ]\n        try:\n            content = self.llm_openai_call(messages)\n            content = json.loads(content)\n            return content[\"summary\"], content[\"desc\"]\n        except Exception as e:\n            logger.error(e)\n            return \"\", \"\"\n\n    def llm_openai_call(self, messages: List[dict]) -&gt; str:\n        \"\"\"\n        Call the OpenAI API to get the response\n        Args:\n            messages (List[dict]): The messages to send to the OpenAI API\n\n\n        Returns:\n            response_json_str (str): The response from the OpenAI API\n        \"\"\"\n        result_json_str, cost = openai_call(messages, self.llm_model_name)\n        self.cost += cost\n        logger.debug(result_json_str)\n        logger.debug(f\"Cost: {self.cost}\")\n        return result_json_str\n</code></pre>"},{"location":"sources/modules/llm/sheet2metadata/#Docs2KG.modules.llm.sheet2metadata.Sheet2Metadata.__init__","title":"<code>__init__(markdown_file, llm_model_name='gpt-3.5-turbo-0125')</code>","text":"<ol> <li>Extract the descriptive part of the markdown</li> <li>Summary the markdown</li> </ol> <p>Parameters:</p> Name Type Description Default <code>markdown_file</code> <code>Path</code> <p>The path to the markdown file</p> required <code>llm_model_name</code> <code>str</code> <p>The OpenAI LLM model name</p> <code>'gpt-3.5-turbo-0125'</code> Source code in <code>Docs2KG/modules/llm/sheet2metadata.py</code> <pre><code>def __init__(self, markdown_file: Path, llm_model_name: str = \"gpt-3.5-turbo-0125\"):\n    \"\"\"\n    1. Extract the descriptive part of the markdown\n    2. Summary the markdown\n\n    Args:\n        markdown_file (Path): The path to the markdown file\n        llm_model_name (str): The OpenAI LLM model name\n    \"\"\"\n    self.markdown_file = markdown_file\n    if self.markdown_file.suffix != \".csv\":\n        raise ValueError(\"Only support csv\")\n    self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n    self.llm_model_name = llm_model_name\n    self.cost = 0\n</code></pre>"},{"location":"sources/modules/llm/sheet2metadata/#Docs2KG.modules.llm.sheet2metadata.Sheet2Metadata.llm_openai_call","title":"<code>llm_openai_call(messages)</code>","text":"<p>Call the OpenAI API to get the response Args:     messages (List[dict]): The messages to send to the OpenAI API</p> <p>Returns:</p> Name Type Description <code>response_json_str</code> <code>str</code> <p>The response from the OpenAI API</p> Source code in <code>Docs2KG/modules/llm/sheet2metadata.py</code> <pre><code>def llm_openai_call(self, messages: List[dict]) -&gt; str:\n    \"\"\"\n    Call the OpenAI API to get the response\n    Args:\n        messages (List[dict]): The messages to send to the OpenAI API\n\n\n    Returns:\n        response_json_str (str): The response from the OpenAI API\n    \"\"\"\n    result_json_str, cost = openai_call(messages, self.llm_model_name)\n    self.cost += cost\n    logger.debug(result_json_str)\n    logger.debug(f\"Cost: {self.cost}\")\n    return result_json_str\n</code></pre>"},{"location":"sources/modules/llm/sheet2metadata/#Docs2KG.modules.llm.sheet2metadata.Sheet2Metadata.openai_sheet_handler","title":"<code>openai_sheet_handler(markdown)</code>","text":"<ol> <li>Use OpenAI to exclude the numerical part of the markdown, only keep the descriptive part</li> <li>Summarize the markdown</li> </ol> <p>Parameters:</p> Name Type Description Default <code>markdown</code> <code>str</code> required <p>Returns:</p> Source code in <code>Docs2KG/modules/llm/sheet2metadata.py</code> <pre><code>def openai_sheet_handler(self, markdown: str):\n    \"\"\"\n    1. Use OpenAI to exclude the numerical part of the markdown, only keep the descriptive part\n    2. Summarize the markdown\n\n    Args:\n        markdown:\n\n    Returns:\n\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n                You are a helpful assistant, help us clean and understand the excel data\n                You will need to first summary the markdwon content, it is normally some descriptive text with\n                Table information.\n                \"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\n                        We will need you to do two things:\n                        1. Summarize the following markdown content into a description about the data:\n                            - What the data is about\n                            - What's the main point of the data\n                        2. Exclude the numerical part of the markdown, only keep the descriptive part.\n\n                        The markdown content is: \\n\\n{markdown}\n\n                        Return in JSON format with key \"summary\" and \"desc\"\n                        \"\"\",\n        },\n    ]\n    try:\n        content = self.llm_openai_call(messages)\n        content = json.loads(content)\n        return content[\"summary\"], content[\"desc\"]\n    except Exception as e:\n        logger.error(e)\n        return \"\", \"\"\n</code></pre>"},{"location":"sources/modules/native/markdown2json/","title":"Markdown2json","text":""},{"location":"sources/modules/native/markdown2json/#Docs2KG.modules.native.markdown2json.Markdown2JSON","title":"<code>Markdown2JSON</code>","text":"<p>We want to make the semi-structured data into structured format, which is JSON format.</p> <p>If it is a well-formatted markdown file, we should be able to get it into JSON format via programmatic methods.</p> <p>However, if it is not a well-formatted markdown file, for example</p> <pre><code># Title\n\n## Subtitle\n\n- Item 1\n\n\n##### I\n\nProsperity Resources - Perth office\n\n##### I\n\n##### I\n##### I\n\n##### I\n##### I\n\n##### I\n##### I\n\n##### I\n</code></pre> <p>We will need to use NLP techniques to extract the structured information from the markdown file, or we need the help from LLMs to generate the structured information from the markdown file.</p> Source code in <code>Docs2KG/modules/native/markdown2json.py</code> <pre><code>class Markdown2JSON:\n    \"\"\"\n    We want to make the semi-structured data into structured format, which is JSON format.\n\n    If it is a well-formatted markdown file, we should be able to get it into JSON format via programmatic methods.\n\n    However, if it is not a well-formatted markdown file, for example\n\n    ```\n    # Title\n\n    ## Subtitle\n\n    - Item 1\n\n\n    ##### I\n\n    Prosperity Resources - Perth office\n\n    ##### I\n\n    ##### I\n    ##### I\n\n    ##### I\n    ##### I\n\n    ##### I\n    ##### I\n\n    ##### I\n    ```\n\n    We will need to use NLP techniques to extract the structured information from the markdown file, or\n    we need the help from LLMs to generate the structured information from the markdown file.\n    \"\"\"\n\n    def __init__(self, markdown_file: Path):\n        \"\"\"\n        Initialize the markdown file and the output JSON file\n        Args:\n            markdown_file (Path): The markdown file to be converted to JSON\n        \"\"\"\n        # if it is ending with .md, then it is a markdown file\n        # if it is ending with .csv, then it is a csv file\n        self.markdown_file = markdown_file\n        self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n\n    @staticmethod\n    def markdown_to_json(markdown):\n        lines = markdown.split(\"\\n\")\n        json_data = {\"tag\": \"root\", \"content\": \"\", \"children\": []}\n\n        stack = [json_data]\n\n        for line in lines:\n            if line.startswith(\"#\"):\n                level = line.count(\"#\")\n                new_tag = \"h\" + str(level)\n                new_content = line.lstrip(\"# \").strip()\n\n                # Find the correct parent for the current tag\n                while len(stack) &gt; level:\n                    stack.pop()\n\n                parent = stack[-1]\n                new_node = {\"tag\": new_tag, \"content\": new_content, \"children\": []}\n                parent[\"children\"].append(new_node)\n                stack.append(new_node)\n            elif line.strip() != \"\":\n                current_node = stack[-1]\n                current_node[\"children\"].append(\n                    {\"tag\": \"p\", \"content\": line.strip(), \"children\": []}\n                )\n        return json_data\n\n    def extract2json_from_csv(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Extract the csv file to JSON and save it to the output directory\n\n        Returns:\n            json_csv (pd.DataFrame): The DataFrame containing the JSON content\n        \"\"\"\n        df = pd.read_csv(self.markdown_file)\n        # loop the df\n        for index, row in df.iterrows():\n            json_content = self.markdown_to_json(row[\"text\"])\n            logger.debug(json_content)\n            df.at[index, \"json\"] = json.dumps(json_content)\n        df.to_csv(self.json_csv_file, index=False)\n        return df\n</code></pre>"},{"location":"sources/modules/native/markdown2json/#Docs2KG.modules.native.markdown2json.Markdown2JSON.__init__","title":"<code>__init__(markdown_file)</code>","text":"<p>Initialize the markdown file and the output JSON file Args:     markdown_file (Path): The markdown file to be converted to JSON</p> Source code in <code>Docs2KG/modules/native/markdown2json.py</code> <pre><code>def __init__(self, markdown_file: Path):\n    \"\"\"\n    Initialize the markdown file and the output JSON file\n    Args:\n        markdown_file (Path): The markdown file to be converted to JSON\n    \"\"\"\n    # if it is ending with .md, then it is a markdown file\n    # if it is ending with .csv, then it is a csv file\n    self.markdown_file = markdown_file\n    self.json_csv_file = markdown_file.with_suffix(\".json.csv\")\n</code></pre>"},{"location":"sources/modules/native/markdown2json/#Docs2KG.modules.native.markdown2json.Markdown2JSON.extract2json_from_csv","title":"<code>extract2json_from_csv()</code>","text":"<p>Extract the csv file to JSON and save it to the output directory</p> <p>Returns:</p> Name Type Description <code>json_csv</code> <code>DataFrame</code> <p>The DataFrame containing the JSON content</p> Source code in <code>Docs2KG/modules/native/markdown2json.py</code> <pre><code>def extract2json_from_csv(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract the csv file to JSON and save it to the output directory\n\n    Returns:\n        json_csv (pd.DataFrame): The DataFrame containing the JSON content\n    \"\"\"\n    df = pd.read_csv(self.markdown_file)\n    # loop the df\n    for index, row in df.iterrows():\n        json_content = self.markdown_to_json(row[\"text\"])\n        logger.debug(json_content)\n        df.at[index, \"json\"] = json.dumps(json_content)\n    df.to_csv(self.json_csv_file, index=False)\n    return df\n</code></pre>"},{"location":"sources/parser/email/base/","title":"Base","text":""},{"location":"sources/parser/email/base/#Docs2KG.parser.email.base.EmailParseBase","title":"<code>EmailParseBase</code>","text":"Source code in <code>Docs2KG/parser/email/base.py</code> <pre><code>class EmailParseBase:\n    def __init__(self, email_file: Path, output_dir: Path = None):\n        \"\"\"\n        Initialize the EmailParseBase class\n\n        Args:\n            email_file (Path): Path to the email file, end with .eml\n            output_dir (Path): Path to the output directory where the converted files will be saved\n        \"\"\"\n        self.email_file = email_file\n\n        self.output_dir = output_dir\n        if self.output_dir is None:\n            email_output_folder = DATA_OUTPUT_DIR / email_file.name\n            email_output_folder.mkdir(parents=True, exist_ok=True)\n\n            self.output_dir = email_output_folder\n</code></pre>"},{"location":"sources/parser/email/base/#Docs2KG.parser.email.base.EmailParseBase.__init__","title":"<code>__init__(email_file, output_dir=None)</code>","text":"<p>Initialize the EmailParseBase class</p> <p>Parameters:</p> Name Type Description Default <code>email_file</code> <code>Path</code> <p>Path to the email file, end with .eml</p> required <code>output_dir</code> <code>Path</code> <p>Path to the output directory where the converted files will be saved</p> <code>None</code> Source code in <code>Docs2KG/parser/email/base.py</code> <pre><code>def __init__(self, email_file: Path, output_dir: Path = None):\n    \"\"\"\n    Initialize the EmailParseBase class\n\n    Args:\n        email_file (Path): Path to the email file, end with .eml\n        output_dir (Path): Path to the output directory where the converted files will be saved\n    \"\"\"\n    self.email_file = email_file\n\n    self.output_dir = output_dir\n    if self.output_dir is None:\n        email_output_folder = DATA_OUTPUT_DIR / email_file.name\n        email_output_folder.mkdir(parents=True, exist_ok=True)\n\n        self.output_dir = email_output_folder\n</code></pre>"},{"location":"sources/parser/email/email_compose/","title":"Email compose","text":""},{"location":"sources/parser/email/email_compose/#Docs2KG.parser.email.email_compose.EmailDecompose","title":"<code>EmailDecompose</code>","text":"<p>               Bases: <code>EmailParseBase</code></p> Source code in <code>Docs2KG/parser/email/email_compose.py</code> <pre><code>class EmailDecompose(EmailParseBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.image_output_dir = self.output_dir / \"images\"\n        self.image_output_dir.mkdir(parents=True, exist_ok=True)\n        self.attachments_output_dir = self.output_dir / \"attachments\"\n        self.attachments_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def decompose_email(self):\n        \"\"\"\n        Decompose the email file to images, attachments, and metadata\n        \"\"\"\n        msg = self.read_email_file()\n        self.download_email_attachment(msg)\n        return msg\n\n    def read_email_file(self):\n        with open(self.email_file, \"rb\") as f:\n            msg = email.message_from_bytes(f.read())\n        return msg\n\n    def download_email_attachment(self, msg):\n        \"\"\"\n        Download the email attachment and save it to the output directory\n        Args:\n            msg:\n\n        Returns:\n\n        \"\"\"\n        images = []\n        attachments = []\n        # extract all the attachments\n        for part in msg.walk():\n            if part.get_content_disposition() == \"attachment\":\n                filename = part.get_filename()\n                if filename:\n                    filename = self.clean_filename(filename, part)\n\n                    filepath = self.attachments_output_dir / filename\n                    with open(filepath, \"wb\") as f:\n                        f.write(part.get_payload(decode=True))\n                    attachments.append(\n                        {\n                            \"name\": filename,\n                            \"path\": filepath,\n                            \"original_filename\": part.get_filename(),\n                        }\n                    )\n            # if content type is image/ , download the image\n            if part.get_content_type().startswith(\"image/\"):\n                img_data = part.get_payload(decode=True)\n                img_name = part.get_filename()\n                if img_name:\n                    img_name = self.clean_filename(img_name, part)\n                    img_path = self.image_output_dir / img_name\n                    with open(img_path, \"wb\") as f:\n                        f.write(img_data)\n                    logger.info(f\"Saved image to: {img_path}\")\n                    images.append(\n                        {\n                            \"name\": img_name,\n                            \"path\": img_path,\n                            \"cid\": part.get(\"Content-ID\", \"\"),\n                        }\n                    )\n            # save content to html or text, end with .html or .txt\n            if part.get_content_type() == \"text/html\":\n                html_content = part.get_payload(decode=True)\n                html_output = self.output_dir / \"email.html\"\n                with open(html_output, \"wb\") as f:\n                    f.write(html_content)\n                logger.info(f\"Saved html to: {html_output}\")\n            if part.get_content_type() == \"text/plain\":\n                text_content = part.get_payload(decode=True)\n                text_output = self.output_dir / \"email.txt\"\n                with open(text_output, \"wb\") as f:\n                    f.write(text_content)\n                logger.info(f\"Saved text to: {text_output}\")\n\n            # save df to csv, end with .csv\n\n        # metadata to json, include subject, from, to, date\n        email_metadata = {\n            \"subject\": msg[\"subject\"],\n            \"from\": msg[\"from\"],\n            \"to\": msg[\"to\"],\n            \"date\": msg[\"date\"],\n        }\n        metadata_output = self.output_dir / \"metadata.json\"\n        with open(metadata_output, \"w\") as f:\n            json.dump(email_metadata, f)\n\n        images_df = pd.DataFrame(images)\n        images_output = self.image_output_dir / \"images.csv\"\n        images_df.to_csv(images_output, index=False)\n\n        attachments_df = pd.DataFrame(attachments)\n        attachments_output = self.attachments_output_dir / \"attachments.csv\"\n        attachments_df.to_csv(attachments_output, index=False)\n\n        return msg\n\n    @staticmethod\n    def clean_filename(filename: str, part):\n        \"\"\"\n        Clean the filename to remove special characters.\n\n        Args:\n            filename (str): Filename to clean.\n            part (email.message.Message): Email part.\n\n        Returns:\n            str: Cleaned filename.\n        \"\"\"\n        if \"?=\" in filename:\n            filename = filename.rsplit(\"?=\", 1)[0]\n        if part.get(\"Content-ID\"):\n            filename = f\"{part.get('Content-ID')}_{filename}\"\n        return filename\n</code></pre>"},{"location":"sources/parser/email/email_compose/#Docs2KG.parser.email.email_compose.EmailDecompose.clean_filename","title":"<code>clean_filename(filename, part)</code>  <code>staticmethod</code>","text":"<p>Clean the filename to remove special characters.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename to clean.</p> required <code>part</code> <code>Message</code> <p>Email part.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Cleaned filename.</p> Source code in <code>Docs2KG/parser/email/email_compose.py</code> <pre><code>@staticmethod\ndef clean_filename(filename: str, part):\n    \"\"\"\n    Clean the filename to remove special characters.\n\n    Args:\n        filename (str): Filename to clean.\n        part (email.message.Message): Email part.\n\n    Returns:\n        str: Cleaned filename.\n    \"\"\"\n    if \"?=\" in filename:\n        filename = filename.rsplit(\"?=\", 1)[0]\n    if part.get(\"Content-ID\"):\n        filename = f\"{part.get('Content-ID')}_{filename}\"\n    return filename\n</code></pre>"},{"location":"sources/parser/email/email_compose/#Docs2KG.parser.email.email_compose.EmailDecompose.decompose_email","title":"<code>decompose_email()</code>","text":"<p>Decompose the email file to images, attachments, and metadata</p> Source code in <code>Docs2KG/parser/email/email_compose.py</code> <pre><code>def decompose_email(self):\n    \"\"\"\n    Decompose the email file to images, attachments, and metadata\n    \"\"\"\n    msg = self.read_email_file()\n    self.download_email_attachment(msg)\n    return msg\n</code></pre>"},{"location":"sources/parser/email/email_compose/#Docs2KG.parser.email.email_compose.EmailDecompose.download_email_attachment","title":"<code>download_email_attachment(msg)</code>","text":"<p>Download the email attachment and save it to the output directory Args:     msg:</p> <p>Returns:</p> Source code in <code>Docs2KG/parser/email/email_compose.py</code> <pre><code>def download_email_attachment(self, msg):\n    \"\"\"\n    Download the email attachment and save it to the output directory\n    Args:\n        msg:\n\n    Returns:\n\n    \"\"\"\n    images = []\n    attachments = []\n    # extract all the attachments\n    for part in msg.walk():\n        if part.get_content_disposition() == \"attachment\":\n            filename = part.get_filename()\n            if filename:\n                filename = self.clean_filename(filename, part)\n\n                filepath = self.attachments_output_dir / filename\n                with open(filepath, \"wb\") as f:\n                    f.write(part.get_payload(decode=True))\n                attachments.append(\n                    {\n                        \"name\": filename,\n                        \"path\": filepath,\n                        \"original_filename\": part.get_filename(),\n                    }\n                )\n        # if content type is image/ , download the image\n        if part.get_content_type().startswith(\"image/\"):\n            img_data = part.get_payload(decode=True)\n            img_name = part.get_filename()\n            if img_name:\n                img_name = self.clean_filename(img_name, part)\n                img_path = self.image_output_dir / img_name\n                with open(img_path, \"wb\") as f:\n                    f.write(img_data)\n                logger.info(f\"Saved image to: {img_path}\")\n                images.append(\n                    {\n                        \"name\": img_name,\n                        \"path\": img_path,\n                        \"cid\": part.get(\"Content-ID\", \"\"),\n                    }\n                )\n        # save content to html or text, end with .html or .txt\n        if part.get_content_type() == \"text/html\":\n            html_content = part.get_payload(decode=True)\n            html_output = self.output_dir / \"email.html\"\n            with open(html_output, \"wb\") as f:\n                f.write(html_content)\n            logger.info(f\"Saved html to: {html_output}\")\n        if part.get_content_type() == \"text/plain\":\n            text_content = part.get_payload(decode=True)\n            text_output = self.output_dir / \"email.txt\"\n            with open(text_output, \"wb\") as f:\n                f.write(text_content)\n            logger.info(f\"Saved text to: {text_output}\")\n\n        # save df to csv, end with .csv\n\n    # metadata to json, include subject, from, to, date\n    email_metadata = {\n        \"subject\": msg[\"subject\"],\n        \"from\": msg[\"from\"],\n        \"to\": msg[\"to\"],\n        \"date\": msg[\"date\"],\n    }\n    metadata_output = self.output_dir / \"metadata.json\"\n    with open(metadata_output, \"w\") as f:\n        json.dump(email_metadata, f)\n\n    images_df = pd.DataFrame(images)\n    images_output = self.image_output_dir / \"images.csv\"\n    images_df.to_csv(images_output, index=False)\n\n    attachments_df = pd.DataFrame(attachments)\n    attachments_output = self.attachments_output_dir / \"attachments.csv\"\n    attachments_df.to_csv(attachments_output, index=False)\n\n    return msg\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/","title":"Email connector","text":""},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector","title":"<code>EmailConnector</code>","text":"<p>Login to the email server to download emails with keywords, or download specific number of latest emails</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>class EmailConnector:\n    \"\"\"\n    Login to the email server to download emails with keywords, or download specific number of latest emails\n    \"\"\"\n\n    def __init__(\n        self,\n        email_address,\n        password,\n        output_dir=None,\n        search_keyword: str = None,\n        num_emails: int = 50,\n        imap_server: str = \"imap.gmail.com\",\n        imap_port: int = 993,\n    ):\n        \"\"\"\n        Initialize the EmailConnector with login credentials and search parameters.\n\n        Args:\n            email_address (str): Email address to log in.\n            password (str): Password for the email address.\n            search_keyword (str, optional): Keyword to search emails. Defaults to None.\n            num_emails (int, optional): Number of latest emails to download. Defaults to 50.\n        \"\"\"\n        self.email_address = email_address\n        self.password = password\n        self.search_keyword = search_keyword\n        self.num_emails = num_emails\n        self.imap_server = imap_server\n        self.imap_port = imap_port\n        self.imap = None\n        self.login_imap()\n        self.output_dir = output_dir\n        if output_dir is None:\n            self.output_dir = DATA_INPUT_DIR / \"email\" / email_address\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    def login_imap(self):\n        \"\"\"\n        Login to the IMAP server.\n\n        \"\"\"\n        self.imap = imaplib.IMAP4_SSL(self.imap_server, self.imap_port)\n        self.imap.login(self.email_address, self.password)\n        # log whether login was successful\n        if self.imap.state == \"AUTH\":\n            logger.info(\"Login successful\")\n        else:\n            logger.error(\"Login failed\")\n            raise Exception(\"Login failed\")\n\n    def pull(self):\n        \"\"\"\n        Pull the emails from the email server.\n        \"\"\"\n        logger.info(\"Pulling emails\")\n        logger.info(self.search_keyword)\n        if self.search_keyword:\n            email_ids = self.search_emails()\n        else:\n            email_ids = self.download_latest_emails()\n\n        for email_id in email_ids:\n            logger.info(f\"Downloading email: {email_id}\")\n            self.download_email(email_id)\n\n    def search_emails(self):\n        \"\"\"\n        Search for emails based on the search keyword.\n\n        Returns:\n            list: List of email IDs that match the search criteria.\n        \"\"\"\n        self.imap.select(\"inbox\")\n        if self.search_keyword:\n            result, data = self.imap.search(None, f'(BODY \"{self.search_keyword}\")')\n            logger.info(f\"Number of emails found: {len(data[0].split())}\")\n            logger.info(f\"Email IDs: {data[0].split()}\")\n        else:\n            result, data = self.imap.search(None, \"ALL\")\n        email_ids = data[0].split()\n        return email_ids\n\n    def fetch_emails(self, email_ids):\n        \"\"\"\n        Fetch the emails based on email IDs.\n\n        Args:\n            email_ids (list): List of email IDs to fetch.\n\n        Returns:\n            list: List of email messages.\n        \"\"\"\n        emails = []\n        for email_id in email_ids:\n            result, data = self.imap.fetch(email_id, \"(RFC822)\")\n            raw_email = data[0][1]\n            msg = email.message_from_bytes(raw_email)\n            emails.append(msg)\n        return emails\n\n    def download_latest_emails(self):\n        \"\"\"\n        Download the latest emails up to the specified number.\n\n        Returns:\n            list: List of the latest email messages.\n        \"\"\"\n        email_ids = self.search_emails()\n        latest_email_ids = email_ids[-self.num_emails :]\n        latest_emails = self.fetch_emails(latest_email_ids)\n        return latest_emails\n\n    def download_email(self, email_id):\n        \"\"\"\n        Download a specific email based on the email ID.\n\n        Args:\n            email_id (str): Email ID to download.\n\n        Returns:\n            email.message.Message: Email message.\n        \"\"\"\n        # fetch the email for the content and all the attachments\n        result, data = self.imap.fetch(email_id, \"(RFC822)\")\n        raw_email = data[0][1]\n\n        # save the email to folder with email_id as filename\n        email_output_dir = self.output_dir / email_id.decode(\"utf-8\")\n        email_output_dir.mkdir(parents=True, exist_ok=True)\n        email_filepath = (\n            email_output_dir / f\"{self.email_address}.{email_id.decode('utf-8')}.eml\"\n        )\n        with open(email_filepath, \"wb\") as f:\n            f.write(raw_email)\n        logger.info(f\"Saved email to: {email_filepath}\")\n\n    def logout(self):\n        \"\"\"\n        Logout from the email servers.\n        \"\"\"\n        if self.imap:\n            self.imap.logout()\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.__init__","title":"<code>__init__(email_address, password, output_dir=None, search_keyword=None, num_emails=50, imap_server='imap.gmail.com', imap_port=993)</code>","text":"<p>Initialize the EmailConnector with login credentials and search parameters.</p> <p>Parameters:</p> Name Type Description Default <code>email_address</code> <code>str</code> <p>Email address to log in.</p> required <code>password</code> <code>str</code> <p>Password for the email address.</p> required <code>search_keyword</code> <code>str</code> <p>Keyword to search emails. Defaults to None.</p> <code>None</code> <code>num_emails</code> <code>int</code> <p>Number of latest emails to download. Defaults to 50.</p> <code>50</code> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def __init__(\n    self,\n    email_address,\n    password,\n    output_dir=None,\n    search_keyword: str = None,\n    num_emails: int = 50,\n    imap_server: str = \"imap.gmail.com\",\n    imap_port: int = 993,\n):\n    \"\"\"\n    Initialize the EmailConnector with login credentials and search parameters.\n\n    Args:\n        email_address (str): Email address to log in.\n        password (str): Password for the email address.\n        search_keyword (str, optional): Keyword to search emails. Defaults to None.\n        num_emails (int, optional): Number of latest emails to download. Defaults to 50.\n    \"\"\"\n    self.email_address = email_address\n    self.password = password\n    self.search_keyword = search_keyword\n    self.num_emails = num_emails\n    self.imap_server = imap_server\n    self.imap_port = imap_port\n    self.imap = None\n    self.login_imap()\n    self.output_dir = output_dir\n    if output_dir is None:\n        self.output_dir = DATA_INPUT_DIR / \"email\" / email_address\n    self.output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.download_email","title":"<code>download_email(email_id)</code>","text":"<p>Download a specific email based on the email ID.</p> <p>Parameters:</p> Name Type Description Default <code>email_id</code> <code>str</code> <p>Email ID to download.</p> required <p>Returns:</p> Type Description <p>email.message.Message: Email message.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def download_email(self, email_id):\n    \"\"\"\n    Download a specific email based on the email ID.\n\n    Args:\n        email_id (str): Email ID to download.\n\n    Returns:\n        email.message.Message: Email message.\n    \"\"\"\n    # fetch the email for the content and all the attachments\n    result, data = self.imap.fetch(email_id, \"(RFC822)\")\n    raw_email = data[0][1]\n\n    # save the email to folder with email_id as filename\n    email_output_dir = self.output_dir / email_id.decode(\"utf-8\")\n    email_output_dir.mkdir(parents=True, exist_ok=True)\n    email_filepath = (\n        email_output_dir / f\"{self.email_address}.{email_id.decode('utf-8')}.eml\"\n    )\n    with open(email_filepath, \"wb\") as f:\n        f.write(raw_email)\n    logger.info(f\"Saved email to: {email_filepath}\")\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.download_latest_emails","title":"<code>download_latest_emails()</code>","text":"<p>Download the latest emails up to the specified number.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of the latest email messages.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def download_latest_emails(self):\n    \"\"\"\n    Download the latest emails up to the specified number.\n\n    Returns:\n        list: List of the latest email messages.\n    \"\"\"\n    email_ids = self.search_emails()\n    latest_email_ids = email_ids[-self.num_emails :]\n    latest_emails = self.fetch_emails(latest_email_ids)\n    return latest_emails\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.fetch_emails","title":"<code>fetch_emails(email_ids)</code>","text":"<p>Fetch the emails based on email IDs.</p> <p>Parameters:</p> Name Type Description Default <code>email_ids</code> <code>list</code> <p>List of email IDs to fetch.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of email messages.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def fetch_emails(self, email_ids):\n    \"\"\"\n    Fetch the emails based on email IDs.\n\n    Args:\n        email_ids (list): List of email IDs to fetch.\n\n    Returns:\n        list: List of email messages.\n    \"\"\"\n    emails = []\n    for email_id in email_ids:\n        result, data = self.imap.fetch(email_id, \"(RFC822)\")\n        raw_email = data[0][1]\n        msg = email.message_from_bytes(raw_email)\n        emails.append(msg)\n    return emails\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.login_imap","title":"<code>login_imap()</code>","text":"<p>Login to the IMAP server.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def login_imap(self):\n    \"\"\"\n    Login to the IMAP server.\n\n    \"\"\"\n    self.imap = imaplib.IMAP4_SSL(self.imap_server, self.imap_port)\n    self.imap.login(self.email_address, self.password)\n    # log whether login was successful\n    if self.imap.state == \"AUTH\":\n        logger.info(\"Login successful\")\n    else:\n        logger.error(\"Login failed\")\n        raise Exception(\"Login failed\")\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.logout","title":"<code>logout()</code>","text":"<p>Logout from the email servers.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def logout(self):\n    \"\"\"\n    Logout from the email servers.\n    \"\"\"\n    if self.imap:\n        self.imap.logout()\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.pull","title":"<code>pull()</code>","text":"<p>Pull the emails from the email server.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def pull(self):\n    \"\"\"\n    Pull the emails from the email server.\n    \"\"\"\n    logger.info(\"Pulling emails\")\n    logger.info(self.search_keyword)\n    if self.search_keyword:\n        email_ids = self.search_emails()\n    else:\n        email_ids = self.download_latest_emails()\n\n    for email_id in email_ids:\n        logger.info(f\"Downloading email: {email_id}\")\n        self.download_email(email_id)\n</code></pre>"},{"location":"sources/parser/email/utils/email_connector/#Docs2KG.parser.email.utils.email_connector.EmailConnector.search_emails","title":"<code>search_emails()</code>","text":"<p>Search for emails based on the search keyword.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of email IDs that match the search criteria.</p> Source code in <code>Docs2KG/parser/email/utils/email_connector.py</code> <pre><code>def search_emails(self):\n    \"\"\"\n    Search for emails based on the search keyword.\n\n    Returns:\n        list: List of email IDs that match the search criteria.\n    \"\"\"\n    self.imap.select(\"inbox\")\n    if self.search_keyword:\n        result, data = self.imap.search(None, f'(BODY \"{self.search_keyword}\")')\n        logger.info(f\"Number of emails found: {len(data[0].split())}\")\n        logger.info(f\"Email IDs: {data[0].split()}\")\n    else:\n        result, data = self.imap.search(None, \"ALL\")\n    email_ids = data[0].split()\n    return email_ids\n</code></pre>"},{"location":"sources/parser/excel/base/","title":"Base","text":""},{"location":"sources/parser/excel/base/#Docs2KG.parser.excel.base.ExcelParseBase","title":"<code>ExcelParseBase</code>","text":"Source code in <code>Docs2KG/parser/excel/base.py</code> <pre><code>class ExcelParseBase:\n    def __init__(self, excel_file: Path, output_dir: Path = None):\n        \"\"\"\n        Initialize the ExcelParseBase class\n\n        Args:\n            excel_file (Path): Path to the excel file\n            output_dir (Path): Path to the output directory where the converted files will be saved\n\n        \"\"\"\n        self.excel_file = excel_file\n\n        self.output_dir = output_dir\n        if self.output_dir is None:\n            excel_output_folder = DATA_OUTPUT_DIR / self.excel_file.name\n            excel_output_folder.mkdir(parents=True, exist_ok=True)\n            self.output_dir = excel_output_folder\n\n        # export excel metadata\n        self.metadata = self.output_dir / \"metadata.json\"\n        from openpyxl import load_workbook\n\n        wb = load_workbook(self.excel_file)\n        properties = wb.properties\n        metadata_dict = {\n            \"filename\": self.excel_file.name,\n            \"title\": properties.title,\n            \"subject\": properties.subject,\n            \"creator\": properties.creator,\n            \"keywords\": properties.keywords,\n            \"description\": properties.description,\n            \"lastModifiedBy\": properties.lastModifiedBy,\n            \"revision\": properties.revision,\n            \"created\": properties.created.isoformat() if properties.created else None,\n            \"modified\": (\n                properties.modified.isoformat() if properties.modified else None\n            ),\n            \"lastPrinted\": (\n                properties.lastPrinted.isoformat() if properties.lastPrinted else None\n            ),\n            \"category\": properties.category,\n            \"contentStatus\": properties.contentStatus,\n            \"identifier\": properties.identifier,\n            \"language\": properties.language,\n            \"version\": properties.version,\n        }\n\n        metadata_json = json.dumps(metadata_dict, indent=4)\n\n        with open(self.metadata, \"w\") as f:\n            f.write(metadata_json)\n</code></pre>"},{"location":"sources/parser/excel/base/#Docs2KG.parser.excel.base.ExcelParseBase.__init__","title":"<code>__init__(excel_file, output_dir=None)</code>","text":"<p>Initialize the ExcelParseBase class</p> <p>Parameters:</p> Name Type Description Default <code>excel_file</code> <code>Path</code> <p>Path to the excel file</p> required <code>output_dir</code> <code>Path</code> <p>Path to the output directory where the converted files will be saved</p> <code>None</code> Source code in <code>Docs2KG/parser/excel/base.py</code> <pre><code>def __init__(self, excel_file: Path, output_dir: Path = None):\n    \"\"\"\n    Initialize the ExcelParseBase class\n\n    Args:\n        excel_file (Path): Path to the excel file\n        output_dir (Path): Path to the output directory where the converted files will be saved\n\n    \"\"\"\n    self.excel_file = excel_file\n\n    self.output_dir = output_dir\n    if self.output_dir is None:\n        excel_output_folder = DATA_OUTPUT_DIR / self.excel_file.name\n        excel_output_folder.mkdir(parents=True, exist_ok=True)\n        self.output_dir = excel_output_folder\n\n    # export excel metadata\n    self.metadata = self.output_dir / \"metadata.json\"\n    from openpyxl import load_workbook\n\n    wb = load_workbook(self.excel_file)\n    properties = wb.properties\n    metadata_dict = {\n        \"filename\": self.excel_file.name,\n        \"title\": properties.title,\n        \"subject\": properties.subject,\n        \"creator\": properties.creator,\n        \"keywords\": properties.keywords,\n        \"description\": properties.description,\n        \"lastModifiedBy\": properties.lastModifiedBy,\n        \"revision\": properties.revision,\n        \"created\": properties.created.isoformat() if properties.created else None,\n        \"modified\": (\n            properties.modified.isoformat() if properties.modified else None\n        ),\n        \"lastPrinted\": (\n            properties.lastPrinted.isoformat() if properties.lastPrinted else None\n        ),\n        \"category\": properties.category,\n        \"contentStatus\": properties.contentStatus,\n        \"identifier\": properties.identifier,\n        \"language\": properties.language,\n        \"version\": properties.version,\n    }\n\n    metadata_json = json.dumps(metadata_dict, indent=4)\n\n    with open(self.metadata, \"w\") as f:\n        f.write(metadata_json)\n</code></pre>"},{"location":"sources/parser/excel/excel2image/","title":"Excel2image","text":""},{"location":"sources/parser/excel/excel2image/#Docs2KG.parser.excel.excel2image.Excel2Image","title":"<code>Excel2Image</code>","text":"<p>               Bases: <code>ExcelParseBase</code></p> Source code in <code>Docs2KG/parser/excel/excel2image.py</code> <pre><code>class Excel2Image(ExcelParseBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the Excel2Image class.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.image_output_dir = self.output_dir / \"images\"\n        self.image_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def excel2image_and_pdf(self):\n        \"\"\"\n        Convert an Excel file to image and pdf files.\n        \"\"\"\n        images = []\n        xls = pd.ExcelFile(self.excel_file)\n        index = 0\n        # Loop through each sheet in the Excel file\n        for sheet_name in xls.sheet_names:\n            # Read the sheet into a DataFrame\n            df = pd.read_excel(self.excel_file, sheet_name=sheet_name)\n            df.columns = [\n                \"\" if col.startswith(\"Unnamed\") else col for col in df.columns\n            ]\n            df = df.fillna(\"\")  # Replace NaN values with an empty string\n            # Convert the DataFrame to HTML\n            html = df.to_html()\n            # Save the HTML to an image file\n            imgkit.from_string(html, f\"{self.image_output_dir}/{sheet_name}.png\")\n            logger.info(f\"Image saved to {self.image_output_dir}/{sheet_name}.png\")\n            # pdfkit.from_string(html, f\"{self.image_output_dir}/{sheet_name}.pdf\")\n            # logger.info(f\"PDF saved to {self.image_output_dir}/{sheet_name}.pdf\")\n\n            images.append(\n                {\n                    \"page_index\": index,\n                    \"filename\": f\"{sheet_name}.png\",\n                    \"file_path\": f\"{self.image_output_dir}/{sheet_name}.png\",\n                    \"sheet_name\": sheet_name,\n                }\n            )\n            index += 1\n        images_df = pd.DataFrame(images)\n        images_df.to_csv(self.image_output_dir / \"images.csv\", index=False)\n        logger.info(f\"Images metadata saved to {self.image_output_dir}/images.csv\")\n</code></pre>"},{"location":"sources/parser/excel/excel2image/#Docs2KG.parser.excel.excel2image.Excel2Image.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the Excel2Image class.</p> Source code in <code>Docs2KG/parser/excel/excel2image.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the Excel2Image class.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.image_output_dir = self.output_dir / \"images\"\n    self.image_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/excel/excel2image/#Docs2KG.parser.excel.excel2image.Excel2Image.excel2image_and_pdf","title":"<code>excel2image_and_pdf()</code>","text":"<p>Convert an Excel file to image and pdf files.</p> Source code in <code>Docs2KG/parser/excel/excel2image.py</code> <pre><code>def excel2image_and_pdf(self):\n    \"\"\"\n    Convert an Excel file to image and pdf files.\n    \"\"\"\n    images = []\n    xls = pd.ExcelFile(self.excel_file)\n    index = 0\n    # Loop through each sheet in the Excel file\n    for sheet_name in xls.sheet_names:\n        # Read the sheet into a DataFrame\n        df = pd.read_excel(self.excel_file, sheet_name=sheet_name)\n        df.columns = [\n            \"\" if col.startswith(\"Unnamed\") else col for col in df.columns\n        ]\n        df = df.fillna(\"\")  # Replace NaN values with an empty string\n        # Convert the DataFrame to HTML\n        html = df.to_html()\n        # Save the HTML to an image file\n        imgkit.from_string(html, f\"{self.image_output_dir}/{sheet_name}.png\")\n        logger.info(f\"Image saved to {self.image_output_dir}/{sheet_name}.png\")\n        # pdfkit.from_string(html, f\"{self.image_output_dir}/{sheet_name}.pdf\")\n        # logger.info(f\"PDF saved to {self.image_output_dir}/{sheet_name}.pdf\")\n\n        images.append(\n            {\n                \"page_index\": index,\n                \"filename\": f\"{sheet_name}.png\",\n                \"file_path\": f\"{self.image_output_dir}/{sheet_name}.png\",\n                \"sheet_name\": sheet_name,\n            }\n        )\n        index += 1\n    images_df = pd.DataFrame(images)\n    images_df.to_csv(self.image_output_dir / \"images.csv\", index=False)\n    logger.info(f\"Images metadata saved to {self.image_output_dir}/images.csv\")\n</code></pre>"},{"location":"sources/parser/excel/excel2markdown/","title":"Excel2markdown","text":""},{"location":"sources/parser/excel/excel2markdown/#Docs2KG.parser.excel.excel2markdown.Excel2Markdown","title":"<code>Excel2Markdown</code>","text":"<p>               Bases: <code>ExcelParseBase</code></p> Source code in <code>Docs2KG/parser/excel/excel2markdown.py</code> <pre><code>class Excel2Markdown(ExcelParseBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the Excel2Table class.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.text_output = self.output_dir / \"texts\"\n        self.text_output.mkdir(parents=True, exist_ok=True)\n        self.md_csv = self.text_output / \"md.csv\"\n\n    def extract2markdown(self):\n        # A simple example to extract tables from Excel\n        md_csv = []\n        df = pd.read_excel(self.excel_file, sheet_name=None)\n        index = 0\n        for sheet_name, sheet_data in df.items():\n            # to markdown\n            md_csv.append(\n                {\n                    \"sheet_name\": sheet_name,\n                    \"text\": sheet_data.to_markdown(),\n                    \"page_number\": index,\n                }\n            )\n            index += 1\n        md_df = pd.DataFrame(md_csv)\n        md_df.to_csv(self.md_csv, index=False)\n</code></pre>"},{"location":"sources/parser/excel/excel2markdown/#Docs2KG.parser.excel.excel2markdown.Excel2Markdown.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the Excel2Table class.</p> Source code in <code>Docs2KG/parser/excel/excel2markdown.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the Excel2Table class.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.text_output = self.output_dir / \"texts\"\n    self.text_output.mkdir(parents=True, exist_ok=True)\n    self.md_csv = self.text_output / \"md.csv\"\n</code></pre>"},{"location":"sources/parser/excel/excel2table/","title":"Excel2table","text":""},{"location":"sources/parser/excel/excel2table/#Docs2KG.parser.excel.excel2table.Excel2Table","title":"<code>Excel2Table</code>","text":"<p>               Bases: <code>ExcelParseBase</code></p> Source code in <code>Docs2KG/parser/excel/excel2table.py</code> <pre><code>class Excel2Table(ExcelParseBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the Excel2Table class.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.table_output_dir = self.output_dir / \"tables\"\n        self.table_output_dir.mkdir(parents=True, exist_ok=True)\n\n    @staticmethod\n    def find_table_start(df, min_non_nan=3):\n        \"\"\"\n        Identify the most likely starting row of a table in a DataFrame.\n\n        Parameters:\n        df (pd.DataFrame): The DataFrame to inspect.\n        min_non_nan (int): Minimum number of non-NaN values in a row to consider it as the start.\n\n        Returns:\n        int: Index of the row most likely to be the start of the table.\n        \"\"\"\n        for i, row in df.iterrows():\n            non_nan_count = row.count() - row.isna().sum()\n            if non_nan_count &gt;= min_non_nan:\n                # Check if subsequent rows have similar non-NaN counts\n                if (\n                    df.iloc[i + 1 : i + 5]\n                    .apply(lambda x: x.count() - x.isna().sum(), axis=1)\n                    .mean()\n                    &gt;= min_non_nan\n                ):\n                    return i\n        return 0\n\n    def extract_tables_from_excel(self):\n        # A simple example to extract tables from Excel\n        tables = []\n\n        df = pd.read_excel(self.excel_file, sheet_name=None)\n        index = 0\n        for sheet_name, sheet_data in df.items():\n            start_row = self.find_table_start(sheet_data)\n            sheet_data = sheet_data.iloc[start_row:].reset_index(drop=True)\n            sheet_data.to_csv(self.table_output_dir / f\"{sheet_name}.csv\", index=False)\n            tables.append(\n                {\n                    \"page_index\": index,\n                    \"table_index\": 1,\n                    \"filename\": f\"{sheet_name}.csv\",\n                    \"file_path\": f\"{self.table_output_dir}/{sheet_name}.csv\",\n                    \"sheet_name\": sheet_name,\n                }\n            )\n            index += 1\n        logger.info(f\"Tables extracted from {self.excel_file}\")\n        table_df = pd.DataFrame(tables)\n        table_df.to_csv(self.table_output_dir / \"tables.csv\", index=False)\n        return self.table_output_dir\n</code></pre>"},{"location":"sources/parser/excel/excel2table/#Docs2KG.parser.excel.excel2table.Excel2Table.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the Excel2Table class.</p> Source code in <code>Docs2KG/parser/excel/excel2table.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the Excel2Table class.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.table_output_dir = self.output_dir / \"tables\"\n    self.table_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/excel/excel2table/#Docs2KG.parser.excel.excel2table.Excel2Table.find_table_start","title":"<code>find_table_start(df, min_non_nan=3)</code>  <code>staticmethod</code>","text":"<p>Identify the most likely starting row of a table in a DataFrame.</p> <p>Parameters: df (pd.DataFrame): The DataFrame to inspect. min_non_nan (int): Minimum number of non-NaN values in a row to consider it as the start.</p> <p>Returns: int: Index of the row most likely to be the start of the table.</p> Source code in <code>Docs2KG/parser/excel/excel2table.py</code> <pre><code>@staticmethod\ndef find_table_start(df, min_non_nan=3):\n    \"\"\"\n    Identify the most likely starting row of a table in a DataFrame.\n\n    Parameters:\n    df (pd.DataFrame): The DataFrame to inspect.\n    min_non_nan (int): Minimum number of non-NaN values in a row to consider it as the start.\n\n    Returns:\n    int: Index of the row most likely to be the start of the table.\n    \"\"\"\n    for i, row in df.iterrows():\n        non_nan_count = row.count() - row.isna().sum()\n        if non_nan_count &gt;= min_non_nan:\n            # Check if subsequent rows have similar non-NaN counts\n            if (\n                df.iloc[i + 1 : i + 5]\n                .apply(lambda x: x.count() - x.isna().sum(), axis=1)\n                .mean()\n                &gt;= min_non_nan\n            ):\n                return i\n    return 0\n</code></pre>"},{"location":"sources/parser/pdf/base/","title":"Base","text":""},{"location":"sources/parser/pdf/base/#Docs2KG.parser.pdf.base.PDFParserBase","title":"<code>PDFParserBase</code>","text":"Source code in <code>Docs2KG/parser/pdf/base.py</code> <pre><code>class PDFParserBase:\n    def __init__(self, pdf_file: Path, output_dir: Path = None) -&gt; None:\n        \"\"\"\n        Initialize the class with the pdf file\n\n        Args:\n            pdf_file (Path): The path to the pdf file\n            output_dir (Path): The path to the output directory, default is None, will be default to DATA_OUTPUT_DIR\n\n        \"\"\"\n        self.pdf_file = pdf_file\n        self.output_dir = output_dir\n        if self.output_dir is None:\n            # get it to be that the input file =&gt; input folder\n            pdf_file_folder = DATA_OUTPUT_DIR / pdf_file.name\n            pdf_file_folder.mkdir(parents=True, exist_ok=True)\n            self.output_dir = pdf_file_folder\n\n        # copy original pdf to the output folder\n        pdf_file_output = self.output_dir / pdf_file.name\n        if not pdf_file_output.exists():\n            shutil.copy(pdf_file, pdf_file_output)\n        self.metadata_json = self.output_dir / \"metadata.json\"\n        if self.metadata_json.exists():\n            self.metadata = json.load(open(self.metadata_json))\n        else:\n            metadata = get_meda_for_file(pdf_file)\n            logger.info(metadata)\n            self.metadata = metadata\n            with open(self.metadata_json, \"w\") as f:\n                f.write(json.dumps(metadata))\n</code></pre>"},{"location":"sources/parser/pdf/base/#Docs2KG.parser.pdf.base.PDFParserBase.__init__","title":"<code>__init__(pdf_file, output_dir=None)</code>","text":"<p>Initialize the class with the pdf file</p> <p>Parameters:</p> Name Type Description Default <code>pdf_file</code> <code>Path</code> <p>The path to the pdf file</p> required <code>output_dir</code> <code>Path</code> <p>The path to the output directory, default is None, will be default to DATA_OUTPUT_DIR</p> <code>None</code> Source code in <code>Docs2KG/parser/pdf/base.py</code> <pre><code>def __init__(self, pdf_file: Path, output_dir: Path = None) -&gt; None:\n    \"\"\"\n    Initialize the class with the pdf file\n\n    Args:\n        pdf_file (Path): The path to the pdf file\n        output_dir (Path): The path to the output directory, default is None, will be default to DATA_OUTPUT_DIR\n\n    \"\"\"\n    self.pdf_file = pdf_file\n    self.output_dir = output_dir\n    if self.output_dir is None:\n        # get it to be that the input file =&gt; input folder\n        pdf_file_folder = DATA_OUTPUT_DIR / pdf_file.name\n        pdf_file_folder.mkdir(parents=True, exist_ok=True)\n        self.output_dir = pdf_file_folder\n\n    # copy original pdf to the output folder\n    pdf_file_output = self.output_dir / pdf_file.name\n    if not pdf_file_output.exists():\n        shutil.copy(pdf_file, pdf_file_output)\n    self.metadata_json = self.output_dir / \"metadata.json\"\n    if self.metadata_json.exists():\n        self.metadata = json.load(open(self.metadata_json))\n    else:\n        metadata = get_meda_for_file(pdf_file)\n        logger.info(metadata)\n        self.metadata = metadata\n        with open(self.metadata_json, \"w\") as f:\n            f.write(json.dumps(metadata))\n</code></pre>"},{"location":"sources/parser/pdf/constants/","title":"Constants","text":"<p>This file contains the constants used in the pdf parser.</p> <p>Examples:</p> <pre><code>PDF_TYPE_SCANNED = \"scanned\"\nPDF_TYPE_EXPORTED = \"exported\"\nPDF_METADATA_SCAN_INDICATOR = \"scan\"\n</code></pre> <p>The indicators are used to determine the type of the pdf file and the metadata of the pdf file.</p> <p>This can be extended or further improved later</p>"},{"location":"sources/parser/pdf/pdf2blocks/","title":"Pdf2blocks","text":""},{"location":"sources/parser/pdf/pdf2blocks/#Docs2KG.parser.pdf.pdf2blocks.PDF2Blocks","title":"<code>PDF2Blocks</code>","text":"<p>               Bases: <code>PDFParserBase</code></p> Source code in <code>Docs2KG/parser/pdf/pdf2blocks.py</code> <pre><code>class PDF2Blocks(PDFParserBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the PDF2Images class\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.images_output_dir = self.output_dir / \"images\"\n        self.images_output_dir.mkdir(parents=True, exist_ok=True)\n        self.text_output_dir = self.output_dir / \"texts\"\n        self.text_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract_df(self, output_csv: bool = False) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"\n        It will extract figures and text from the pdf file and return a pandas dataframe\n\n        Have tried to extract the page to a xhtml, however, it does not contain the hierarchy of the text information.\n\n        Args:\n            output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n        Returns:\n            Dict[str, pd.DataFrame]: The dictionary containing the text and image information\n\n        \"\"\"\n\n        doc = fitz.open(self.pdf_file)\n        logger.info(f\"Extracting data from {self.pdf_file}\")\n\n        images = []\n        texts = []\n        for page in doc:\n            page_dict = page.get_text(\"dict\")\n            blocks = page_dict[\"blocks\"]\n            for block in blocks:\n                if block[\"type\"] == 0:\n                    for line in block[\"lines\"]:\n                        for span in line[\"spans\"]:\n                            span[\"page_number\"] = page.number\n                            span[\"block_number\"] = block[\"number\"]\n                            texts.append(span)\n                elif block[\"type\"] == 1:\n                    # remove \"image\" key from the block\n                    image_bytes = block.pop(\"image\", None)\n                    block[\"page_number\"] = page.number\n                    block[\"block_number\"] = block[\"number\"]\n                    with open(\n                        self.images_output_dir\n                        / f\"page_{page.number}_block_{block['block_number']}.{block['ext']}\",\n                        \"wb\",\n                    ) as f:\n                        f.write(image_bytes)\n                    block[\"image_path\"] = (\n                        self.images_output_dir\n                        / f\"page_{page.number}_block_{block['block_number']}.{block['ext']}\"\n                    )\n                    images.append(block)\n\n        texts_df = pd.DataFrame(texts)\n        images_df = pd.DataFrame(images)\n        if output_csv:\n            texts_df.to_csv(self.text_output_dir / \"blocks_texts.csv\", index=False)\n            images_df.to_csv(self.images_output_dir / \"blocks_images.csv\", index=False)\n        return {\n            \"texts\": texts_df,\n            \"images\": images_df,\n            \"file_name\": self.pdf_file.name,\n            \"file_path\": self.pdf_file,\n            \"pages_no\": len(doc),\n            \"file_type\": \"exported_pdf\",\n            \"file_size\": self.pdf_file.stat().st_size,\n        }\n</code></pre>"},{"location":"sources/parser/pdf/pdf2blocks/#Docs2KG.parser.pdf.pdf2blocks.PDF2Blocks.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the PDF2Images class</p> Source code in <code>Docs2KG/parser/pdf/pdf2blocks.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the PDF2Images class\n\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.images_output_dir = self.output_dir / \"images\"\n    self.images_output_dir.mkdir(parents=True, exist_ok=True)\n    self.text_output_dir = self.output_dir / \"texts\"\n    self.text_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/pdf/pdf2blocks/#Docs2KG.parser.pdf.pdf2blocks.PDF2Blocks.extract_df","title":"<code>extract_df(output_csv=False)</code>","text":"<p>It will extract figures and text from the pdf file and return a pandas dataframe</p> <p>Have tried to extract the page to a xhtml, however, it does not contain the hierarchy of the text information.</p> <p>Parameters:</p> Name Type Description Default <code>output_csv</code> <code>bool</code> <p>Whether to output the extracted data to a csv file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dict[str, pd.DataFrame]: The dictionary containing the text and image information</p> Source code in <code>Docs2KG/parser/pdf/pdf2blocks.py</code> <pre><code>def extract_df(self, output_csv: bool = False) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    It will extract figures and text from the pdf file and return a pandas dataframe\n\n    Have tried to extract the page to a xhtml, however, it does not contain the hierarchy of the text information.\n\n    Args:\n        output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n    Returns:\n        Dict[str, pd.DataFrame]: The dictionary containing the text and image information\n\n    \"\"\"\n\n    doc = fitz.open(self.pdf_file)\n    logger.info(f\"Extracting data from {self.pdf_file}\")\n\n    images = []\n    texts = []\n    for page in doc:\n        page_dict = page.get_text(\"dict\")\n        blocks = page_dict[\"blocks\"]\n        for block in blocks:\n            if block[\"type\"] == 0:\n                for line in block[\"lines\"]:\n                    for span in line[\"spans\"]:\n                        span[\"page_number\"] = page.number\n                        span[\"block_number\"] = block[\"number\"]\n                        texts.append(span)\n            elif block[\"type\"] == 1:\n                # remove \"image\" key from the block\n                image_bytes = block.pop(\"image\", None)\n                block[\"page_number\"] = page.number\n                block[\"block_number\"] = block[\"number\"]\n                with open(\n                    self.images_output_dir\n                    / f\"page_{page.number}_block_{block['block_number']}.{block['ext']}\",\n                    \"wb\",\n                ) as f:\n                    f.write(image_bytes)\n                block[\"image_path\"] = (\n                    self.images_output_dir\n                    / f\"page_{page.number}_block_{block['block_number']}.{block['ext']}\"\n                )\n                images.append(block)\n\n    texts_df = pd.DataFrame(texts)\n    images_df = pd.DataFrame(images)\n    if output_csv:\n        texts_df.to_csv(self.text_output_dir / \"blocks_texts.csv\", index=False)\n        images_df.to_csv(self.images_output_dir / \"blocks_images.csv\", index=False)\n    return {\n        \"texts\": texts_df,\n        \"images\": images_df,\n        \"file_name\": self.pdf_file.name,\n        \"file_path\": self.pdf_file,\n        \"pages_no\": len(doc),\n        \"file_type\": \"exported_pdf\",\n        \"file_size\": self.pdf_file.stat().st_size,\n    }\n</code></pre>"},{"location":"sources/parser/pdf/pdf2image/","title":"Pdf2image","text":""},{"location":"sources/parser/pdf/pdf2image/#Docs2KG.parser.pdf.pdf2image.PDF2Image","title":"<code>PDF2Image</code>","text":"<p>               Bases: <code>PDFParserBase</code></p> Source code in <code>Docs2KG/parser/pdf/pdf2image.py</code> <pre><code>class PDF2Image(PDFParserBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the PDF2Images class\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.images_output_dir = self.output_dir / \"images\"\n        self.images_output_dir.mkdir(parents=True, exist_ok=True)\n        self.text_output_dir = self.output_dir / \"texts\"\n        self.text_output_dir.mkdir(parents=True, exist_ok=True)\n        # clean the image folder\n        for file in self.images_output_dir.glob(\"*\"):\n            # delete the file\n            file.unlink()\n\n    def extract_page_2_image_df(self):\n        \"\"\"\n        Extract the page to image across the whole pdf, each page will be one image\n\n        Returns:\n\n        \"\"\"\n        doc = fitz.open(self.pdf_file)\n        logger.info(f\"Extracting data from {self.pdf_file}\")\n        df = pd.DataFrame()\n        image_data = []\n\n        for page in doc:\n            pix = page.get_pixmap()\n            image_bytes = pix.tobytes(\"png\")\n            image_path = (\n                self.images_output_dir / f\"page_{page.number + 1}.png\"\n            )  # Page numbers are 0-indexed in PyMuPDF\n            with open(image_path, \"wb\") as f:\n                f.write(image_bytes)\n            image_data.append(\n                {\"page_number\": page.number, \"image_path\": str(image_path)}\n            )\n\n        df = pd.DataFrame(image_data)\n        df.to_csv(self.images_output_dir / \"page_images.csv\", index=False)\n        return df\n</code></pre>"},{"location":"sources/parser/pdf/pdf2image/#Docs2KG.parser.pdf.pdf2image.PDF2Image.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the PDF2Images class</p> Source code in <code>Docs2KG/parser/pdf/pdf2image.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the PDF2Images class\n\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.images_output_dir = self.output_dir / \"images\"\n    self.images_output_dir.mkdir(parents=True, exist_ok=True)\n    self.text_output_dir = self.output_dir / \"texts\"\n    self.text_output_dir.mkdir(parents=True, exist_ok=True)\n    # clean the image folder\n    for file in self.images_output_dir.glob(\"*\"):\n        # delete the file\n        file.unlink()\n</code></pre>"},{"location":"sources/parser/pdf/pdf2image/#Docs2KG.parser.pdf.pdf2image.PDF2Image.extract_page_2_image_df","title":"<code>extract_page_2_image_df()</code>","text":"<p>Extract the page to image across the whole pdf, each page will be one image</p> <p>Returns:</p> Source code in <code>Docs2KG/parser/pdf/pdf2image.py</code> <pre><code>def extract_page_2_image_df(self):\n    \"\"\"\n    Extract the page to image across the whole pdf, each page will be one image\n\n    Returns:\n\n    \"\"\"\n    doc = fitz.open(self.pdf_file)\n    logger.info(f\"Extracting data from {self.pdf_file}\")\n    df = pd.DataFrame()\n    image_data = []\n\n    for page in doc:\n        pix = page.get_pixmap()\n        image_bytes = pix.tobytes(\"png\")\n        image_path = (\n            self.images_output_dir / f\"page_{page.number + 1}.png\"\n        )  # Page numbers are 0-indexed in PyMuPDF\n        with open(image_path, \"wb\") as f:\n            f.write(image_bytes)\n        image_data.append(\n            {\"page_number\": page.number, \"image_path\": str(image_path)}\n        )\n\n    df = pd.DataFrame(image_data)\n    df.to_csv(self.images_output_dir / \"page_images.csv\", index=False)\n    return df\n</code></pre>"},{"location":"sources/parser/pdf/pdf2metadata/","title":"Pdf2metadata","text":""},{"location":"sources/parser/pdf/pdf2metadata/#Docs2KG.parser.pdf.pdf2metadata.get_meda_for_file","title":"<code>get_meda_for_file(pdf_file)</code>","text":"<p>Get metadata for a single pdf file</p> <p>Parameters:</p> Name Type Description Default <code>pdf_file</code> <code>Path</code> <p>Path to the pdf file</p> required <p>Returns:</p> Name Type Description <code>metadata</code> <code>dict</code> <p>Metadata for the pdf file</p> Source code in <code>Docs2KG/parser/pdf/pdf2metadata.py</code> <pre><code>def get_meda_for_file(pdf_file: Path) -&gt; dict:\n    \"\"\"\n    Get metadata for a single pdf file\n\n    Args:\n        pdf_file (Path): Path to the pdf file\n\n    Returns:\n        metadata (dict): Metadata for the pdf file\n    \"\"\"\n    doc = fitz.open(pdf_file)\n    metadata = doc.metadata\n    texts = []\n    for page in doc:\n        texts.append(page.get_text())\n    metadata[\"text_token\"] = count_tokens(\" \".join(texts))\n    # estimate the price\n    metadata[\"estimated_price_gpt35\"] = estimate_price(metadata[\"text_token\"])\n    metadata[\"estimated_price_gpt4o\"] = estimate_price(\n        metadata[\"text_token\"], model_name=\"gpt-4o\"\n    )\n    metadata[\"estimated_price_4_turbo\"] = estimate_price(\n        metadata[\"text_token\"], model_name=\"gpt-4-turbo\"\n    )\n    metadata[\"file_path\"] = pdf_file.as_posix()\n    metadata[\"scanned_or_exported\"] = get_scanned_or_exported(pdf_file)\n    # to dict\n    metadata = dict(metadata)\n    return metadata\n</code></pre>"},{"location":"sources/parser/pdf/pdf2metadata/#Docs2KG.parser.pdf.pdf2metadata.get_metadata_for_files","title":"<code>get_metadata_for_files(pdf_files, log_summary=False)</code>","text":"<p>Get metadata for a list of pdf files</p> <p>We will return the metadata as a DataFrame, and for further processing</p> <p>If it is set as log_summary -&gt; True, we will log the summary of the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_files</code> <code>list[Path]</code> <p>list of pdf files</p> required <code>log_summary</code> <code>bool</code> <p>If True, log the summary of the metadata</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing metadata for the pdf files</p> Source code in <code>Docs2KG/parser/pdf/pdf2metadata.py</code> <pre><code>def get_metadata_for_files(\n    pdf_files: list[Path], log_summary: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Get metadata for a list of pdf files\n\n    We will return the metadata as a DataFrame, and for further processing\n\n    If it is set as **log_summary** -&gt; True, we will log the summary of the metadata.\n\n    Args:\n        pdf_files (list[Path]): list of pdf files\n        log_summary (bool): If True, log the summary of the metadata\n\n    Returns:\n        pd.DataFrame: DataFrame containing metadata for the pdf files\n    \"\"\"\n\n    all_metadata = []\n    for pdf_file in pdf_files:\n        metadata = get_meda_for_file(pdf_file)\n        all_metadata.append(metadata)\n    metadata_df = pd.DataFrame(all_metadata)\n    # add a line called \"file_path\" to the metadata\n    metadata_df[\"file_path\"] = pdf_files\n    metadata_df[\"scanned_or_exported\"] = metadata_df[\"file_path\"].apply(\n        get_scanned_or_exported\n    )\n\n    if log_summary:\n        # log all the columns we have\n        logger.info(f\"All columns within Metadata:\\n {metadata_df.columns.tolist()}\")\n        # log format column into  value|count format\n        logger.info(\n            f\"Format Column:\\n {metadata_df['format'].value_counts().to_markdown()}\"\n        )\n        # log creator column into  value|count format\n        logger.info(\n            f\"Creator Column:\\n {metadata_df['creator'].value_counts().to_markdown()}\"\n        )\n        # log producer column into  value|count format\n        logger.info(\n            f\"Producer Column:\\n {metadata_df['producer'].value_counts().to_markdown()}\"\n        )\n        # log the encrypted column into  value|count format\n        logger.info(\n            f\"Encrypted Column:\\n {metadata_df['encryption'].value_counts().to_markdown()}\"\n        )\n        # log the scanned_or_exported column into  value|count format\n        logger.info(\n            f\"Scanned or Exported Column:\\n {metadata_df['scanned_or_exported'].value_counts().to_markdown()}\"\n        )\n        # estimate token in total\n        logger.info(f\"Total Token Count: {metadata_df['text_token'].sum()}\")\n        # estimate the price in total\n        logger.info(f\"Estimated Price 3.5: {metadata_df['estimated_price_3.5'].sum()}\")\n        logger.info(f\"Estimated Price 4o: {metadata_df['estimated_price_4o'].sum()}\")\n        logger.info(\n            f\"Estimated Price 4 Turbo: {metadata_df['estimated_price_4_turbo'].sum()}\"\n        )\n\n    return metadata_df\n</code></pre>"},{"location":"sources/parser/pdf/pdf2metadata/#Docs2KG.parser.pdf.pdf2metadata.get_scanned_or_exported","title":"<code>get_scanned_or_exported(pdf_path)</code>","text":"<p>Check if the PDF is scanned or exported If with_metadata is True, return the metadata also</p> <p>Current, we will use the keyword \"scan\" in the metadata to determine if the pdf is scanned or not.</p> <p>This can be extended based on the use case scenario</p> Key Value producer producer (producing software) format format: \u2018PDF-1.4\u2019, \u2018EPUB\u2019, etc. encryption encryption method used if any author author modDate date of last modification keywords keywords title title creationDate date of creation creator creating application subject subject text_token number of tokens in the text Args: pdf_path (Path): Path to the pdf file <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The type of the pdf file</p> Source code in <code>Docs2KG/parser/pdf/pdf2metadata.py</code> <pre><code>def get_scanned_or_exported(pdf_path: Path) -&gt; str:\n    \"\"\"\n    Check if the PDF is scanned or exported\n    If with_metadata is True, return the metadata also\n\n    Current, we will use the keyword \"scan\" in the metadata to determine if the pdf is scanned or not.\n\n    This can be extended based on the use case scenario\n\n    | Key           | Value                                   |\n    |---------------|-----------------------------------------|\n    | producer      | producer (producing software)           |\n    | format        | format: \u2018PDF-1.4\u2019, \u2018EPUB\u2019, etc.         |\n    | encryption    | encryption method used if any           |\n    | author        | author                                  |\n    | modDate       | date of last modification               |\n    | keywords      | keywords                                |\n    | title         | title                                   |\n    | creationDate  | date of creation                        |\n    | creator       | creating application                    |\n    | subject       | subject                                 |\n    | text_token    | number of tokens in the text            |\n    Args:\n        pdf_path (Path): Path to the pdf file\n\n    Returns:\n        str: The type of the pdf file\n\n    \"\"\"\n    doc = fitz.open(pdf_path)\n    metadata = doc.metadata\n    logger.debug(f\"Metadata: {metadata}\")\n    if PDF_METADATA_SCAN_INDICATOR in str(metadata).lower():\n        return PDF_TYPE_SCANNED\n    return PDF_TYPE_EXPORTED\n</code></pre>"},{"location":"sources/parser/pdf/pdf2tables/","title":"Pdf2tables","text":""},{"location":"sources/parser/pdf/pdf2tables/#Docs2KG.parser.pdf.pdf2tables.PDF2Tables","title":"<code>PDF2Tables</code>","text":"<p>               Bases: <code>PDFParserBase</code></p> Source code in <code>Docs2KG/parser/pdf/pdf2tables.py</code> <pre><code>class PDF2Tables(PDFParserBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the class with the pdf file\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.table_output_dir = self.output_dir / \"tables\"\n        self.table_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract2tables(self, output_csv: bool = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Extract Tables from the pdf file\n\n        Args:\n            output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n        Returns:\n            pd.DataFrame: The dataframe containing the table information\n        \"\"\"\n        doc = fitz.open(self.pdf_file)  # open a document\n        tables_list = []\n\n        for page_index in range(len(doc)):  # iterate over pdf pages\n            page = doc[page_index]  # get the page\n            tabs = page.find_tables()\n            if tabs.tables:\n                logger.debug(f\"Found {len(tabs.tables)} tables on page {page_index}\")\n                for table_index, tab in enumerate(tabs.tables, start=1):\n                    # save to csv\n\n                    filename = \"page_%s-table_%s.csv\" % (page_index, table_index)\n                    # save it to bounding box cropped image\n                    df = tab.to_pandas()\n                    df.to_csv(self.table_output_dir / filename)\n                    logger.debug(tab.bbox)\n                    tables_list.append(\n                        {\n                            \"page_index\": page_index,\n                            \"table_index\": table_index,\n                            \"bbox\": tab.bbox,\n                            \"filename\": filename,\n                            \"file_path\": self.table_output_dir / filename,\n                        }\n                    )\n        df = pd.DataFrame(tables_list)\n        if output_csv:\n            df.to_csv(self.table_output_dir / \"tables.csv\", index=False)\n        return df\n</code></pre>"},{"location":"sources/parser/pdf/pdf2tables/#Docs2KG.parser.pdf.pdf2tables.PDF2Tables.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the class with the pdf file</p> Source code in <code>Docs2KG/parser/pdf/pdf2tables.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the class with the pdf file\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.table_output_dir = self.output_dir / \"tables\"\n    self.table_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/pdf/pdf2tables/#Docs2KG.parser.pdf.pdf2tables.PDF2Tables.extract2tables","title":"<code>extract2tables(output_csv=False)</code>","text":"<p>Extract Tables from the pdf file</p> <p>Parameters:</p> Name Type Description Default <code>output_csv</code> <code>bool</code> <p>Whether to output the extracted data to a csv file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataframe containing the table information</p> Source code in <code>Docs2KG/parser/pdf/pdf2tables.py</code> <pre><code>def extract2tables(self, output_csv: bool = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Extract Tables from the pdf file\n\n    Args:\n        output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n    Returns:\n        pd.DataFrame: The dataframe containing the table information\n    \"\"\"\n    doc = fitz.open(self.pdf_file)  # open a document\n    tables_list = []\n\n    for page_index in range(len(doc)):  # iterate over pdf pages\n        page = doc[page_index]  # get the page\n        tabs = page.find_tables()\n        if tabs.tables:\n            logger.debug(f\"Found {len(tabs.tables)} tables on page {page_index}\")\n            for table_index, tab in enumerate(tabs.tables, start=1):\n                # save to csv\n\n                filename = \"page_%s-table_%s.csv\" % (page_index, table_index)\n                # save it to bounding box cropped image\n                df = tab.to_pandas()\n                df.to_csv(self.table_output_dir / filename)\n                logger.debug(tab.bbox)\n                tables_list.append(\n                    {\n                        \"page_index\": page_index,\n                        \"table_index\": table_index,\n                        \"bbox\": tab.bbox,\n                        \"filename\": filename,\n                        \"file_path\": self.table_output_dir / filename,\n                    }\n                )\n    df = pd.DataFrame(tables_list)\n    if output_csv:\n        df.to_csv(self.table_output_dir / \"tables.csv\", index=False)\n    return df\n</code></pre>"},{"location":"sources/parser/pdf/pdf2text/","title":"Pdf2text","text":""},{"location":"sources/parser/pdf/pdf2text/#Docs2KG.parser.pdf.pdf2text.PDF2Text","title":"<code>PDF2Text</code>","text":"<p>               Bases: <code>PDFParserBase</code></p> Source code in <code>Docs2KG/parser/pdf/pdf2text.py</code> <pre><code>class PDF2Text(PDFParserBase):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the PDF2Text class\n\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        self.text_output_dir = self.output_dir / \"texts\"\n        self.text_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract2text(self, output_csv: bool = False) -&gt; dict:\n        \"\"\"\n        Extract text from the pdf file\n\n        Args\n        output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n        Returns:\n            text (str): The extracted text\n            output_file (Path): The path to the output file\n            df (pd.Dataframe): The dataframe containing the text information\n        \"\"\"\n        doc = fitz.open(self.pdf_file)\n        text = \"\"\n        texts = []\n        for page in doc:\n            text += page.get_text()\n            texts.append({\"page_number\": page.number, \"text\": page.get_text()})\n\n        df = pd.DataFrame(texts)\n        if output_csv:\n            df.to_csv(self.text_output_dir / \"text.csv\", index=False)\n            return {\n                \"text\": text,\n                \"output_file\": self.text_output_dir / \"text.csv\",\n                \"df\": df,\n            }\n        return {\"text\": text, \"output_file\": None, \"df\": df}\n\n    def extract2markdown(self, output_csv: bool = False) -&gt; dict:\n        \"\"\"\n        Convert the extracted text to markdown\n\n        Args:\n            output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n        Returns:\n            md (str): The Markdown text,\n            output_file (Path): Where the Markdown text save to\n            df (pd.Dataframe): Each page for the Markdown text\n        \"\"\"\n        doc = fitz.open(self.pdf_file)\n        md_text = pymupdf4llm.to_markdown(doc)\n        logger.debug(f\"Markdown text: {md_text}\")\n\n        # split the Markdown text into pages\n        markdown_texts = []\n        for page in doc:\n            page_text = pymupdf4llm.to_markdown(doc=doc, pages=[page.number])\n            logger.debug(f\"Page {page.number} Markdown text: {page_text}\")\n            markdown_texts.append({\"page_number\": page.number, \"text\": page_text})\n        df = pd.DataFrame(markdown_texts)\n\n        if output_csv:\n            df.to_csv(self.text_output_dir / \"md.csv\", index=False)\n            return {\n                \"md\": md_text,\n                \"output_file\": self.text_output_dir / \"md.csv\",\n                \"df\": df,\n            }\n\n        return {\"md\": md_text, \"df\": df, \"output_file\": None}\n</code></pre>"},{"location":"sources/parser/pdf/pdf2text/#Docs2KG.parser.pdf.pdf2text.PDF2Text.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the PDF2Text class</p> Source code in <code>Docs2KG/parser/pdf/pdf2text.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"\n    Initialize the PDF2Text class\n\n    \"\"\"\n    super().__init__(*args, **kwargs)\n\n    self.text_output_dir = self.output_dir / \"texts\"\n    self.text_output_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"sources/parser/pdf/pdf2text/#Docs2KG.parser.pdf.pdf2text.PDF2Text.extract2markdown","title":"<code>extract2markdown(output_csv=False)</code>","text":"<p>Convert the extracted text to markdown</p> <p>Parameters:</p> Name Type Description Default <code>output_csv</code> <code>bool</code> <p>Whether to output the extracted data to a csv file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>md</code> <code>str</code> <p>The Markdown text,</p> <code>output_file</code> <code>Path</code> <p>Where the Markdown text save to</p> <code>df</code> <code>Dataframe</code> <p>Each page for the Markdown text</p> Source code in <code>Docs2KG/parser/pdf/pdf2text.py</code> <pre><code>def extract2markdown(self, output_csv: bool = False) -&gt; dict:\n    \"\"\"\n    Convert the extracted text to markdown\n\n    Args:\n        output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n    Returns:\n        md (str): The Markdown text,\n        output_file (Path): Where the Markdown text save to\n        df (pd.Dataframe): Each page for the Markdown text\n    \"\"\"\n    doc = fitz.open(self.pdf_file)\n    md_text = pymupdf4llm.to_markdown(doc)\n    logger.debug(f\"Markdown text: {md_text}\")\n\n    # split the Markdown text into pages\n    markdown_texts = []\n    for page in doc:\n        page_text = pymupdf4llm.to_markdown(doc=doc, pages=[page.number])\n        logger.debug(f\"Page {page.number} Markdown text: {page_text}\")\n        markdown_texts.append({\"page_number\": page.number, \"text\": page_text})\n    df = pd.DataFrame(markdown_texts)\n\n    if output_csv:\n        df.to_csv(self.text_output_dir / \"md.csv\", index=False)\n        return {\n            \"md\": md_text,\n            \"output_file\": self.text_output_dir / \"md.csv\",\n            \"df\": df,\n        }\n\n    return {\"md\": md_text, \"df\": df, \"output_file\": None}\n</code></pre>"},{"location":"sources/parser/pdf/pdf2text/#Docs2KG.parser.pdf.pdf2text.PDF2Text.extract2text","title":"<code>extract2text(output_csv=False)</code>","text":"<p>Extract text from the pdf file</p> <p>Args output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.</p> <p>Returns:</p> Name Type Description <code>text</code> <code>str</code> <p>The extracted text</p> <code>output_file</code> <code>Path</code> <p>The path to the output file</p> <code>df</code> <code>Dataframe</code> <p>The dataframe containing the text information</p> Source code in <code>Docs2KG/parser/pdf/pdf2text.py</code> <pre><code>def extract2text(self, output_csv: bool = False) -&gt; dict:\n    \"\"\"\n    Extract text from the pdf file\n\n    Args\n    output_csv (bool, optional): Whether to output the extracted data to a csv file. Defaults to False.\n\n    Returns:\n        text (str): The extracted text\n        output_file (Path): The path to the output file\n        df (pd.Dataframe): The dataframe containing the text information\n    \"\"\"\n    doc = fitz.open(self.pdf_file)\n    text = \"\"\n    texts = []\n    for page in doc:\n        text += page.get_text()\n        texts.append({\"page_number\": page.number, \"text\": page.get_text()})\n\n    df = pd.DataFrame(texts)\n    if output_csv:\n        df.to_csv(self.text_output_dir / \"text.csv\", index=False)\n        return {\n            \"text\": text,\n            \"output_file\": self.text_output_dir / \"text.csv\",\n            \"df\": df,\n        }\n    return {\"text\": text, \"output_file\": None, \"df\": df}\n</code></pre>"},{"location":"sources/parser/web/base/","title":"Base","text":""},{"location":"sources/parser/web/base/#Docs2KG.parser.web.base.WebParserBase","title":"<code>WebParserBase</code>","text":"Source code in <code>Docs2KG/parser/web/base.py</code> <pre><code>class WebParserBase:\n    def __init__(\n        self, url: str, output_dir: Path = None, input_dir: Path = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize the WebParserBase class\n\n        Args:\n            url (str): URL to download the HTML files\n            output_dir (Path): Path to the output directory where the converted files will be saved\n            input_dir (Path): Path to the input directory where the html files will be downloaded\n        \"\"\"\n        self.url = url\n        self.output_dir = output_dir\n        self.input_dir = input_dir\n        self.quoted_url = quote(url, \"\")\n        if self.output_dir is None:\n            self.output_dir = DATA_OUTPUT_DIR / self.quoted_url\n            self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.download_html_file()\n\n    def download_html_file(self):\n        \"\"\"\n        Download the html file from the url and save it to the input directory\n\n        \"\"\"\n        response = requests.get(self.url)\n        if response.status_code == 200:\n            with open(f\"{DATA_INPUT_DIR}/index.html\", \"wb\") as f:\n                f.write(response.content)\n            logger.info(f\"Downloaded the HTML file from {self.url}\")\n        else:\n            logger.error(f\"Failed to download the HTML file from {self.url}\")\n</code></pre>"},{"location":"sources/parser/web/base/#Docs2KG.parser.web.base.WebParserBase.__init__","title":"<code>__init__(url, output_dir=None, input_dir=None)</code>","text":"<p>Initialize the WebParserBase class</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to download the HTML files</p> required <code>output_dir</code> <code>Path</code> <p>Path to the output directory where the converted files will be saved</p> <code>None</code> <code>input_dir</code> <code>Path</code> <p>Path to the input directory where the html files will be downloaded</p> <code>None</code> Source code in <code>Docs2KG/parser/web/base.py</code> <pre><code>def __init__(\n    self, url: str, output_dir: Path = None, input_dir: Path = None\n) -&gt; None:\n    \"\"\"\n    Initialize the WebParserBase class\n\n    Args:\n        url (str): URL to download the HTML files\n        output_dir (Path): Path to the output directory where the converted files will be saved\n        input_dir (Path): Path to the input directory where the html files will be downloaded\n    \"\"\"\n    self.url = url\n    self.output_dir = output_dir\n    self.input_dir = input_dir\n    self.quoted_url = quote(url, \"\")\n    if self.output_dir is None:\n        self.output_dir = DATA_OUTPUT_DIR / self.quoted_url\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    self.download_html_file()\n</code></pre>"},{"location":"sources/parser/web/base/#Docs2KG.parser.web.base.WebParserBase.download_html_file","title":"<code>download_html_file()</code>","text":"<p>Download the html file from the url and save it to the input directory</p> Source code in <code>Docs2KG/parser/web/base.py</code> <pre><code>def download_html_file(self):\n    \"\"\"\n    Download the html file from the url and save it to the input directory\n\n    \"\"\"\n    response = requests.get(self.url)\n    if response.status_code == 200:\n        with open(f\"{DATA_INPUT_DIR}/index.html\", \"wb\") as f:\n            f.write(response.content)\n        logger.info(f\"Downloaded the HTML file from {self.url}\")\n    else:\n        logger.error(f\"Failed to download the HTML file from {self.url}\")\n</code></pre>"},{"location":"sources/parser/web/web2images/","title":"Web2images","text":""},{"location":"sources/parser/web/web2images/#Docs2KG.parser.web.web2images.Web2Images","title":"<code>Web2Images</code>","text":"<p>               Bases: <code>WebParserBase</code></p> Source code in <code>Docs2KG/parser/web/web2images.py</code> <pre><code>class Web2Images(WebParserBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.image_output_dir = self.output_dir / \"images\"\n        self.image_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract2images(self):\n        \"\"\"\n        Extract the HTML file to images and save it to the output directory\n\n        \"\"\"\n        url = unquote(self.url)\n        with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n            html_content = f.read()\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        for imgtag in soup.find_all(\"img\"):\n            img_url = imgtag.get(\"src\")\n            if not img_url.startswith(\"http\"):\n                img_url = urljoin(url, img_url)\n            img_data = requests.get(img_url).content\n            img_name = quote(imgtag[\"src\"], \"\")\n\n            with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n                f.write(img_data)\n            logger.info(f\"Extracted the HTML file from {url} to images\")\n</code></pre>"},{"location":"sources/parser/web/web2images/#Docs2KG.parser.web.web2images.Web2Images.extract2images","title":"<code>extract2images()</code>","text":"<p>Extract the HTML file to images and save it to the output directory</p> Source code in <code>Docs2KG/parser/web/web2images.py</code> <pre><code>def extract2images(self):\n    \"\"\"\n    Extract the HTML file to images and save it to the output directory\n\n    \"\"\"\n    url = unquote(self.url)\n    with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n        html_content = f.read()\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    for imgtag in soup.find_all(\"img\"):\n        img_url = imgtag.get(\"src\")\n        if not img_url.startswith(\"http\"):\n            img_url = urljoin(url, img_url)\n        img_data = requests.get(img_url).content\n        img_name = quote(imgtag[\"src\"], \"\")\n\n        with open(f\"{self.output_dir}/images/{img_name}\", \"wb\") as f:\n            f.write(img_data)\n        logger.info(f\"Extracted the HTML file from {url} to images\")\n</code></pre>"},{"location":"sources/parser/web/web2markdown/","title":"Web2markdown","text":""},{"location":"sources/parser/web/web2markdown/#Docs2KG.parser.web.web2markdown.Web2Markdown","title":"<code>Web2Markdown</code>","text":"<p>               Bases: <code>WebParserBase</code></p> Source code in <code>Docs2KG/parser/web/web2markdown.py</code> <pre><code>class Web2Markdown(WebParserBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.markdown_output_dir = self.output_dir / \"markdowns\"\n        self.markdown_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def convert2markdown(self):\n        \"\"\"\n        Convert the HTML file to markdown and save it to the output directory\n\n        \"\"\"\n        with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n            html_content = f.read()\n        markdown_file = md(html_content)\n        with open(f\"{self.markdown_output_dir}/content.md\", \"w\") as f:\n            f.write(str(markdown_file))\n</code></pre>"},{"location":"sources/parser/web/web2markdown/#Docs2KG.parser.web.web2markdown.Web2Markdown.convert2markdown","title":"<code>convert2markdown()</code>","text":"<p>Convert the HTML file to markdown and save it to the output directory</p> Source code in <code>Docs2KG/parser/web/web2markdown.py</code> <pre><code>def convert2markdown(self):\n    \"\"\"\n    Convert the HTML file to markdown and save it to the output directory\n\n    \"\"\"\n    with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n        html_content = f.read()\n    markdown_file = md(html_content)\n    with open(f\"{self.markdown_output_dir}/content.md\", \"w\") as f:\n        f.write(str(markdown_file))\n</code></pre>"},{"location":"sources/parser/web/web2tables/","title":"Web2tables","text":""},{"location":"sources/parser/web/web2tables/#Docs2KG.parser.web.web2tables.Web2Tables","title":"<code>Web2Tables</code>","text":"<p>               Bases: <code>WebParserBase</code></p> Source code in <code>Docs2KG/parser/web/web2tables.py</code> <pre><code>class Web2Tables(WebParserBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.table_output_dir = self.output_dir / \"tables\"\n        self.table_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract2tables(self):\n        \"\"\"\n        Extract the HTML file to tables and save it to the output directory\n\n        \"\"\"\n\n        with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n            html_content = f.read()\n\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        for i, table in enumerate(soup.find_all(\"table\")):\n            rows = []\n            for row in table.find_all(\"tr\"):\n                cells = [\n                    cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n                ]\n                rows.append(cells)\n            df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n            csv_filename = f\"{self.output_dir}/tables/{i}.csv\"\n            df.to_csv(csv_filename, index=False)\n            logger.info(f\"Extracted the HTML file from {self.url} to tables\")\n</code></pre>"},{"location":"sources/parser/web/web2tables/#Docs2KG.parser.web.web2tables.Web2Tables.extract2tables","title":"<code>extract2tables()</code>","text":"<p>Extract the HTML file to tables and save it to the output directory</p> Source code in <code>Docs2KG/parser/web/web2tables.py</code> <pre><code>def extract2tables(self):\n    \"\"\"\n    Extract the HTML file to tables and save it to the output directory\n\n    \"\"\"\n\n    with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n        html_content = f.read()\n\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    for i, table in enumerate(soup.find_all(\"table\")):\n        rows = []\n        for row in table.find_all(\"tr\"):\n            cells = [\n                cell.get_text(strip=True) for cell in row.find_all([\"th\", \"td\"])\n            ]\n            rows.append(cells)\n        df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming first row is header\n        csv_filename = f\"{self.output_dir}/tables/{i}.csv\"\n        df.to_csv(csv_filename, index=False)\n        logger.info(f\"Extracted the HTML file from {self.url} to tables\")\n</code></pre>"},{"location":"sources/parser/web/web2urls/","title":"Web2urls","text":""},{"location":"sources/parser/web/web2urls/#Docs2KG.parser.web.web2urls.Web2URLs","title":"<code>Web2URLs</code>","text":"<p>               Bases: <code>WebParserBase</code></p> Source code in <code>Docs2KG/parser/web/web2urls.py</code> <pre><code>class Web2URLs(WebParserBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.table_output_dir = self.output_dir / \"urls\"\n        self.table_output_dir.mkdir(parents=True, exist_ok=True)\n\n    def extract2tables(self):\n        \"\"\"\n        Extract the HTML file to tables and save it to the output directory\n\n        \"\"\"\n\n        with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n            html_content = f.read()\n\n        soup = BeautifulSoup(html_content, \"html.parser\")\n        # find all urls and save them to a csv file\n        urls = []\n        for a in soup.find_all(\"a\"):\n            urls.append(a.get(\"href\"))\n        df = pd.DataFrame(urls, columns=[\"URL\"])\n        csv_filename = f\"{self.output_dir}/urls/urls.csv\"\n        df.to_csv(csv_filename, index=False)\n</code></pre>"},{"location":"sources/parser/web/web2urls/#Docs2KG.parser.web.web2urls.Web2URLs.extract2tables","title":"<code>extract2tables()</code>","text":"<p>Extract the HTML file to tables and save it to the output directory</p> Source code in <code>Docs2KG/parser/web/web2urls.py</code> <pre><code>def extract2tables(self):\n    \"\"\"\n    Extract the HTML file to tables and save it to the output directory\n\n    \"\"\"\n\n    with open(f\"{self.output_dir}/index.html\", \"r\") as f:\n        html_content = f.read()\n\n    soup = BeautifulSoup(html_content, \"html.parser\")\n    # find all urls and save them to a csv file\n    urls = []\n    for a in soup.find_all(\"a\"):\n        urls.append(a.get(\"href\"))\n    df = pd.DataFrame(urls, columns=[\"URL\"])\n    csv_filename = f\"{self.output_dir}/urls/urls.csv\"\n    df.to_csv(csv_filename, index=False)\n</code></pre>"},{"location":"sources/rag/neo4j_rag/","title":"Neo4j rag","text":""},{"location":"sources/rag/neo4j_rag/#Docs2KG.rag.neo4j_rag.Neo4jRAG","title":"<code>Neo4jRAG</code>","text":"Source code in <code>Docs2KG/rag/neo4j_rag.py</code> <pre><code>class Neo4jRAG:\n    def __init__(self, uri: str, user: str, password: str):\n        \"\"\"\n\n        Args:\n            uri (str): uri of the graph database\n            user (str): username of the graph database\n            password (str): password of the graph database\n        \"\"\"\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def retrieval(self, query: str, top_k: int = 10) -&gt; dict:\n        \"\"\"\n        Retrieve the best matching node from the graph based on the query\n\n        We choose to use our way to do the similarity calculation, which will be slow\n\n        However, it is just a demonstration about how should you do it.\n        Args:\n            query (str): query string\n            top_k (int): number of results to return\n\n        Returns:\n\n        \"\"\"\n        query_embedding = get_openai_embedding(query)\n        with self.driver.session() as session:\n            result = session.run(\n                \"\"\"\n                MATCH (n)\n                RETURN n\n                \"\"\"\n            )\n            nodes = []\n\n            for record in result:\n                node = record[\"n\"]\n                logger.debug(node)\n                node_properties = dict(node.items())\n\n                logger.debug(node_properties)\n                nodes.append(node_properties)\n\n        df = pd.DataFrame(nodes)\n\n        # calculate the similarity\n        # using tqdm to show the progress bar\n        tqdm.pandas()\n        logger.info(\"Calculating the Content similarity\")\n        df[\"content_similarity\"] = df[\"content_embedding\"].progress_apply(\n            lambda x: self.cosine_similarity(x, query_embedding)\n        )\n        logger.info(\"Calculating the meta similarity\")\n        df[\"meta_similarity\"] = df[\"meta_embedding\"].progress_apply(\n            lambda x: self.cosine_similarity(x, query_embedding)\n        )\n\n        # get top k content similarity\n        top_k_content = df.sort_values(\"content_similarity\", ascending=False).head(\n            top_k\n        )\n        # log the content value\n        top_k_meta = df.sort_values(\"meta_similarity\", ascending=False).head(top_k)\n        return {\n            \"top_k_content\": top_k_content[\"uuid\"].tolist(),\n            \"top_k_meta\": top_k_meta[\"uuid\"].tolist(),\n        }\n\n    @staticmethod\n    def cosine_similarity(embedding1, embedding2):\n        \"\"\"\n        Calculate the cosine similarity between two embeddings\n\n        Args:\n            embedding1 (list): embedding 1\n            embedding2 (list): embedding 2\n\n        Returns:\n            float: similarity score\n        \"\"\"\n        vec1 = np.array(embedding1)\n        vec2 = np.array(embedding2)\n        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n\n    def retrieval_strategy_hops_away(self, uuids: List[str], hops: int = 1):\n        \"\"\"\n        Match all nodes and relationships that are `hops` away from the given uuids\n\n        the node will have a property `uuid`\n\n        Args:\n            uuids (List[str]): list of uuids\n            hops (int): number of hops away\n\n        Returns:\n\n        \"\"\"\n        \"\"\"\n          Retrieves nodes connected within a given number of hops from nodes with specified UUIDs.\n\n          Args:\n              uuids (list): List of UUIDs to query from.\n              hops (int): Number of hops to consider in the relationship.\n\n          Returns:\n              list: A list of dictionaries where each dictionary contains properties of a node.\n          \"\"\"\n\n        nodes = []\n\n        query = f\"\"\"\n            MATCH (startNode)-[*1..{hops}]-&gt;(endNode)\n            WHERE startNode.uuid IN {uuids}\n            RETURN endNode\n            \"\"\"\n        logger.info(query)\n        try:\n            with self.driver.session() as session:\n                result = session.run(query)\n                for record in result:\n                    logger.debug(record)\n                    node = record[\"endNode\"]\n                    node_properties = dict(node.items())\n                    logger.info(node_properties.keys())\n\n                    nodes.append(\n                        {\n                            \"uuid\": node_properties[\"uuid\"],\n                            \"content\": node_properties.get(\"content\", \"\"),\n                        }\n                    )\n        except Exception as e:\n            logger.error(\"Failed to fetch connected nodes: %s\", e)\n            raise\n        logger.info(nodes)\n        return nodes\n</code></pre>"},{"location":"sources/rag/neo4j_rag/#Docs2KG.rag.neo4j_rag.Neo4jRAG.__init__","title":"<code>__init__(uri, user, password)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>uri of the graph database</p> required <code>user</code> <code>str</code> <p>username of the graph database</p> required <code>password</code> <code>str</code> <p>password of the graph database</p> required Source code in <code>Docs2KG/rag/neo4j_rag.py</code> <pre><code>def __init__(self, uri: str, user: str, password: str):\n    \"\"\"\n\n    Args:\n        uri (str): uri of the graph database\n        user (str): username of the graph database\n        password (str): password of the graph database\n    \"\"\"\n    self.driver = GraphDatabase.driver(uri, auth=(user, password))\n</code></pre>"},{"location":"sources/rag/neo4j_rag/#Docs2KG.rag.neo4j_rag.Neo4jRAG.cosine_similarity","title":"<code>cosine_similarity(embedding1, embedding2)</code>  <code>staticmethod</code>","text":"<p>Calculate the cosine similarity between two embeddings</p> <p>Parameters:</p> Name Type Description Default <code>embedding1</code> <code>list</code> <p>embedding 1</p> required <code>embedding2</code> <code>list</code> <p>embedding 2</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>similarity score</p> Source code in <code>Docs2KG/rag/neo4j_rag.py</code> <pre><code>@staticmethod\ndef cosine_similarity(embedding1, embedding2):\n    \"\"\"\n    Calculate the cosine similarity between two embeddings\n\n    Args:\n        embedding1 (list): embedding 1\n        embedding2 (list): embedding 2\n\n    Returns:\n        float: similarity score\n    \"\"\"\n    vec1 = np.array(embedding1)\n    vec2 = np.array(embedding2)\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n</code></pre>"},{"location":"sources/rag/neo4j_rag/#Docs2KG.rag.neo4j_rag.Neo4jRAG.retrieval","title":"<code>retrieval(query, top_k=10)</code>","text":"<p>Retrieve the best matching node from the graph based on the query</p> <p>We choose to use our way to do the similarity calculation, which will be slow</p> <p>However, it is just a demonstration about how should you do it. Args:     query (str): query string     top_k (int): number of results to return</p> <p>Returns:</p> Source code in <code>Docs2KG/rag/neo4j_rag.py</code> <pre><code>def retrieval(self, query: str, top_k: int = 10) -&gt; dict:\n    \"\"\"\n    Retrieve the best matching node from the graph based on the query\n\n    We choose to use our way to do the similarity calculation, which will be slow\n\n    However, it is just a demonstration about how should you do it.\n    Args:\n        query (str): query string\n        top_k (int): number of results to return\n\n    Returns:\n\n    \"\"\"\n    query_embedding = get_openai_embedding(query)\n    with self.driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (n)\n            RETURN n\n            \"\"\"\n        )\n        nodes = []\n\n        for record in result:\n            node = record[\"n\"]\n            logger.debug(node)\n            node_properties = dict(node.items())\n\n            logger.debug(node_properties)\n            nodes.append(node_properties)\n\n    df = pd.DataFrame(nodes)\n\n    # calculate the similarity\n    # using tqdm to show the progress bar\n    tqdm.pandas()\n    logger.info(\"Calculating the Content similarity\")\n    df[\"content_similarity\"] = df[\"content_embedding\"].progress_apply(\n        lambda x: self.cosine_similarity(x, query_embedding)\n    )\n    logger.info(\"Calculating the meta similarity\")\n    df[\"meta_similarity\"] = df[\"meta_embedding\"].progress_apply(\n        lambda x: self.cosine_similarity(x, query_embedding)\n    )\n\n    # get top k content similarity\n    top_k_content = df.sort_values(\"content_similarity\", ascending=False).head(\n        top_k\n    )\n    # log the content value\n    top_k_meta = df.sort_values(\"meta_similarity\", ascending=False).head(top_k)\n    return {\n        \"top_k_content\": top_k_content[\"uuid\"].tolist(),\n        \"top_k_meta\": top_k_meta[\"uuid\"].tolist(),\n    }\n</code></pre>"},{"location":"sources/rag/neo4j_rag/#Docs2KG.rag.neo4j_rag.Neo4jRAG.retrieval_strategy_hops_away","title":"<code>retrieval_strategy_hops_away(uuids, hops=1)</code>","text":"<p>Match all nodes and relationships that are <code>hops</code> away from the given uuids</p> <p>the node will have a property <code>uuid</code></p> <p>Parameters:</p> Name Type Description Default <code>uuids</code> <code>List[str]</code> <p>list of uuids</p> required <code>hops</code> <code>int</code> <p>number of hops away</p> <code>1</code> <p>Returns:</p> Source code in <code>Docs2KG/rag/neo4j_rag.py</code> <pre><code>def retrieval_strategy_hops_away(self, uuids: List[str], hops: int = 1):\n    \"\"\"\n    Match all nodes and relationships that are `hops` away from the given uuids\n\n    the node will have a property `uuid`\n\n    Args:\n        uuids (List[str]): list of uuids\n        hops (int): number of hops away\n\n    Returns:\n\n    \"\"\"\n    \"\"\"\n      Retrieves nodes connected within a given number of hops from nodes with specified UUIDs.\n\n      Args:\n          uuids (list): List of UUIDs to query from.\n          hops (int): Number of hops to consider in the relationship.\n\n      Returns:\n          list: A list of dictionaries where each dictionary contains properties of a node.\n      \"\"\"\n\n    nodes = []\n\n    query = f\"\"\"\n        MATCH (startNode)-[*1..{hops}]-&gt;(endNode)\n        WHERE startNode.uuid IN {uuids}\n        RETURN endNode\n        \"\"\"\n    logger.info(query)\n    try:\n        with self.driver.session() as session:\n            result = session.run(query)\n            for record in result:\n                logger.debug(record)\n                node = record[\"endNode\"]\n                node_properties = dict(node.items())\n                logger.info(node_properties.keys())\n\n                nodes.append(\n                    {\n                        \"uuid\": node_properties[\"uuid\"],\n                        \"content\": node_properties.get(\"content\", \"\"),\n                    }\n                )\n    except Exception as e:\n        logger.error(\"Failed to fetch connected nodes: %s\", e)\n        raise\n    logger.info(nodes)\n    return nodes\n</code></pre>"},{"location":"sources/rag/neo4j_vector/","title":"Neo4j vector","text":""},{"location":"sources/rag/neo4j_vector/#Docs2KG.rag.neo4j_vector.Neo4jVector","title":"<code>Neo4jVector</code>","text":"Source code in <code>Docs2KG/rag/neo4j_vector.py</code> <pre><code>class Neo4jVector:\n    def __init__(self, uri, user, password):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def close(self):\n        self.driver.close()\n\n    def add_embedding(self):\n        \"\"\"\n        Loop the whole graph\n        For all nodes without embedding field, get the nodes out, and grab the embedding.\n\n        Returns:\n\n        \"\"\"\n        with self.driver.session() as session:\n            result = session.run(\n                \"\"\"\n                MATCH (n)\n                WHERE n.content_embedding IS NULL\n                RETURN n\n                \"\"\"\n            )\n            records = list(result)\n            total_records = len(records)\n            with tqdm(total=total_records, desc=\"Adding embedding\") as pbar:\n                for record in records:\n                    node = record[\"n\"]\n                    logger.debug(node)\n                    node_properties = dict(node.items())\n                    logger.debug(node_properties)\n                    content_embedding = get_openai_embedding(\n                        node_properties.get(\"content\", \"\")\n                    )\n                    meta_embedding = get_openai_embedding(json.dumps(node_properties))\n                    logger.debug(content_embedding)\n                    logger.debug(meta_embedding)\n                    session.run(\n                        \"\"\"\n                        MATCH (n)\n                        WHERE id(n) = $id\n                        SET n.content_embedding = $content_embedding\n                        SET n.meta_embedding = $meta_embedding\n                        \"\"\",\n                        id=node.id,\n                        content_embedding=content_embedding,\n                        meta_embedding=meta_embedding,\n                    )\n                    pbar.update(1)\n</code></pre>"},{"location":"sources/rag/neo4j_vector/#Docs2KG.rag.neo4j_vector.Neo4jVector.add_embedding","title":"<code>add_embedding()</code>","text":"<p>Loop the whole graph For all nodes without embedding field, get the nodes out, and grab the embedding.</p> <p>Returns:</p> Source code in <code>Docs2KG/rag/neo4j_vector.py</code> <pre><code>def add_embedding(self):\n    \"\"\"\n    Loop the whole graph\n    For all nodes without embedding field, get the nodes out, and grab the embedding.\n\n    Returns:\n\n    \"\"\"\n    with self.driver.session() as session:\n        result = session.run(\n            \"\"\"\n            MATCH (n)\n            WHERE n.content_embedding IS NULL\n            RETURN n\n            \"\"\"\n        )\n        records = list(result)\n        total_records = len(records)\n        with tqdm(total=total_records, desc=\"Adding embedding\") as pbar:\n            for record in records:\n                node = record[\"n\"]\n                logger.debug(node)\n                node_properties = dict(node.items())\n                logger.debug(node_properties)\n                content_embedding = get_openai_embedding(\n                    node_properties.get(\"content\", \"\")\n                )\n                meta_embedding = get_openai_embedding(json.dumps(node_properties))\n                logger.debug(content_embedding)\n                logger.debug(meta_embedding)\n                session.run(\n                    \"\"\"\n                    MATCH (n)\n                    WHERE id(n) = $id\n                    SET n.content_embedding = $content_embedding\n                    SET n.meta_embedding = $meta_embedding\n                    \"\"\",\n                    id=node.id,\n                    content_embedding=content_embedding,\n                    meta_embedding=meta_embedding,\n                )\n                pbar.update(1)\n</code></pre>"},{"location":"sources/utils/constants/","title":"Constants","text":""},{"location":"sources/utils/constants/#Docs2KG.utils.constants.DATA_INPUT_DIR","title":"<code>DATA_INPUT_DIR = DATA_DIR / 'input'</code>  <code>module-attribute</code>","text":"<p>This is Output directory for the data</p> <p>Example</p> <pre><code>from Docs2KG.utils.constants import DATA_OUTPUT_DIR\n</code></pre>"},{"location":"sources/utils/empty_check/","title":"Empty check","text":""},{"location":"sources/utils/get_logger/","title":"Get logger","text":""},{"location":"sources/utils/get_logger/#Docs2KG.utils.get_logger.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Get a logger object with the given name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>logging.Logger: Logger object</p> Source code in <code>Docs2KG/utils/get_logger.py</code> <pre><code>def get_logger(name) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger object with the given name\n\n    Args:\n        name (str): Name of the logger\n\n    Returns:\n        logging.Logger: Logger object\n\n    \"\"\"\n    # Create a logger\n    the_logger = logging.getLogger(name)\n    the_logger.setLevel(logging.INFO)\n\n    # Create console handler and set level to debug\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.DEBUG)\n\n    # Create formatter, start with file name and line of code (line number) that issued the log statement\n    formatter = logging.Formatter(\n        \"%(asctime)s|%(filename)s|Line: %(lineno)d -- %(name)s - %(levelname)s - %(message)s\"\n    )\n\n    # Add formatter to console handler\n    console_handler.setFormatter(formatter)\n\n    # Add console handler to the_logger\n    the_logger.addHandler(console_handler)\n    return the_logger\n</code></pre>"},{"location":"sources/utils/rect/","title":"Rect","text":""},{"location":"sources/utils/rect/#Docs2KG.utils.rect.BlockFinder","title":"<code>BlockFinder</code>","text":"Source code in <code>Docs2KG/utils/rect.py</code> <pre><code>class BlockFinder:\n    @classmethod\n    def find_closest_blocks(\n        cls,\n        block_rect: str,\n        text_blocks: List[str],\n    ) -&gt; dict:\n        \"\"\"\n        Find the closest text block to the image block\n\n        TODO:\n        - Based on how large it close to the margin, we can decide whether it is a important image or not\n        - Based on the size of the image, we can determine whether we need the left, right information\n\n        Args:\n            block_rect (str): The bounding box of the image block as a string\n            text_blocks (List[str]): The bounding boxes of the text blocks as strings\n\n        Returns:\n            dict: The index of the closest text block to the image block in each direction\n        \"\"\"\n        left_index = None\n        right_index = None\n        above_index = None\n        below_index = None\n        distances = []\n        for i, text_block in enumerate(text_blocks):\n            distance = cls.bbox_distance(block_rect, text_block)\n            distance[\"index\"] = i\n            distance[\"bbox\"] = text_block\n            distances.append(distance)\n\n        df = pd.DataFrame(distances)\n\n        # for all the text blocks on top, get the closest one\n        # get above one and also the abs(min_vertical_distance) is &gt; 1\n        above_blocks = df[df[\"position_vertical\"] == \"above\"].copy(deep=True)\n        # get is all value to be abs\n        above_blocks[\"min_vertical_distance\"] = above_blocks[\n            \"min_vertical_distance\"\n        ].apply(abs)\n        above_blocks = above_blocks[abs(above_blocks[\"min_vertical_distance\"]) &gt; 1]\n\n        if not above_blocks.empty:\n            # if it is above it, then the closest one is the one with the highest vertical distance as the value is &lt;0\n            closest_above_block = above_blocks[\n                above_blocks[\"min_vertical_distance\"]\n                == above_blocks[\"min_vertical_distance\"].min()\n            ]\n            above_index = closest_above_block[\"index\"].values[0]\n\n        # for all the text blocks on bottom, get the closest one\n        below_blocks = df[df[\"position_vertical\"] == \"below\"].copy(deep=True)\n        # get is all value to be abs\n        below_blocks[\"min_vertical_distance\"] = below_blocks[\n            \"min_vertical_distance\"\n        ].apply(abs)\n        below_blocks = below_blocks[abs(below_blocks[\"min_vertical_distance\"]) &gt; 1]\n        if not below_blocks.empty:\n            # if it is below it, then the closest one is the one with the lowest vertical distance as the value is &gt;0\n            closest_below_block = below_blocks[\n                below_blocks[\"min_vertical_distance\"]\n                == below_blocks[\"min_vertical_distance\"].min()\n            ]\n            below_index = closest_below_block[\"index\"].values[0]\n\n        # for all the text blocks on left, get the closest one\n        left_blocks = df[df[\"position_horizontal\"] == \"left\"].copy(deep=True)\n        # get is all value to be abs\n        left_blocks[\"min_horizontal_distance\"] = left_blocks[\n            \"min_horizontal_distance\"\n        ].apply(abs)\n        left_blocks = left_blocks[abs(left_blocks[\"min_horizontal_distance\"]) &gt; 1]\n        if not left_blocks.empty:\n            closest_left_block = left_blocks[\n                left_blocks[\"min_horizontal_distance\"]\n                == left_blocks[\"min_horizontal_distance\"].min()\n            ]\n            left_index = closest_left_block[\"index\"].values[0]\n\n        # for all the text blocks on right, get the closest one\n        right_blocks = df[df[\"position_horizontal\"] == \"right\"].copy(deep=True)\n        # get is all value to be abs\n        right_blocks[\"min_horizontal_distance\"] = right_blocks[\n            \"min_horizontal_distance\"\n        ].apply(abs)\n        right_blocks = right_blocks[abs(right_blocks[\"min_horizontal_distance\"]) &gt; 1]\n        if not right_blocks.empty:\n            closest_right_block = right_blocks[\n                right_blocks[\"min_horizontal_distance\"]\n                == right_blocks[\"min_horizontal_distance\"].min()\n            ]\n            right_index = closest_right_block[\"index\"].values[0]\n\n        return {\n            \"left\": left_index,\n            \"right\": right_index,\n            \"above\": above_index,\n            \"below\": below_index,\n        }\n\n    @classmethod\n    def parse_bbox(cls, s):\n        \"\"\"Parse a bounding box string into a tuple of floats.\"\"\"\n        return tuple(map(float, ast.literal_eval(s)))\n\n    @classmethod\n    def bbox_distance(cls, bbox_a, bbox_b) -&gt; dict:\n        \"\"\"\n        Calculate the minimum distance between two bounding boxes.\n        The (0,0) point is the top-left corner of the image.\n\n        So within the bbox\n\n        x0, y0 = top-left corner\n        x1, y1 = bottom-right corner\n        x0 &lt;= x1\n        y0 &lt;= y1\n\n        Args:\n            bbox_a:\n            bbox_b:\n\n        Returns:\n\n        \"\"\"\n        bbox_a = cls.parse_bbox(bbox_a)\n        bbox_b = cls.parse_bbox(bbox_b)\n\n        x0_a, y0_a, x1_a, y1_a = bbox_a\n        x0_b, y0_b, x1_b, y1_b = bbox_b\n\n        # if both x0_a and x1_a are less than x0_b or x1_b\n        # this means that bbox_a is to the left of bbox_b\n        # if both y0_a and y1_a are less than y0_b or y1_b\n        # this means that bbox_a is above bbox_b\n        horizontal_distance = [x0_a - x0_b, x1_a - x0_b, x0_a - x1_b, x1_a - x1_b]\n        vertical_distance = [y0_a - y1_b, y1_a - y0_b, y0_a - y0_b, y1_a - y1_b]\n        abs_horizontal_distance = [abs(item) for item in horizontal_distance]\n        abs_vertical_distance = [abs(item) for item in vertical_distance]\n\n        # get the mini distance and then find their real value, keep the sign\n        min_horizontal_distance = min(abs_horizontal_distance)\n        min_vertical_distance = min(abs_vertical_distance)\n\n        # get the index\n        min_horizontal_distance_index = abs_horizontal_distance.index(\n            min_horizontal_distance\n        )\n        min_vertical_distance_index = abs_vertical_distance.index(min_vertical_distance)\n\n        # get the real value\n        min_horizontal_distance_value = horizontal_distance[\n            min_horizontal_distance_index\n        ]\n        min_vertical_distance_value = vertical_distance[min_vertical_distance_index]\n\n        # whether it is left or right, above or below will be control by the\n        # angle of the center of the two bounding boxes\n        center_a = ((x0_a + x1_a) / 2, (y0_a + y1_a) / 2)\n        center_b = ((x0_b + x1_b) / 2, (y0_b + y1_b) / 2)\n\n        # use bbox_a as the reference point\n        center_x_diff = center_a[0] - center_b[0]\n        center_y_diff = center_a[1] - center_b[1]\n\n        # if center_x_diff &gt; 0, then mean bbox_a is to the right of bbox_b, so bbox_b is on the left\n        position_horizontal = \"left\" if center_x_diff &gt; 0 else \"right\"\n        # if center_y_diff &gt; 0, then mean bbox_a is below bbox_b, so bbox_b is above\n        position_vertical = \"above\" if center_y_diff &gt; 0 else \"below\"\n        return {\n            \"min_horizontal_distance\": min_horizontal_distance_value,\n            \"min_vertical_distance\": min_vertical_distance_value,\n            \"min_distance\": min(min_horizontal_distance, min_vertical_distance),\n            \"position_horizontal\": position_horizontal,\n            \"position_vertical\": position_vertical,\n        }\n</code></pre>"},{"location":"sources/utils/rect/#Docs2KG.utils.rect.BlockFinder.bbox_distance","title":"<code>bbox_distance(bbox_a, bbox_b)</code>  <code>classmethod</code>","text":"<p>Calculate the minimum distance between two bounding boxes. The (0,0) point is the top-left corner of the image.</p> <p>So within the bbox</p> <p>x0, y0 = top-left corner x1, y1 = bottom-right corner x0 &lt;= x1 y0 &lt;= y1</p> <p>Parameters:</p> Name Type Description Default <code>bbox_a</code> required <code>bbox_b</code> required <p>Returns:</p> Source code in <code>Docs2KG/utils/rect.py</code> <pre><code>@classmethod\ndef bbox_distance(cls, bbox_a, bbox_b) -&gt; dict:\n    \"\"\"\n    Calculate the minimum distance between two bounding boxes.\n    The (0,0) point is the top-left corner of the image.\n\n    So within the bbox\n\n    x0, y0 = top-left corner\n    x1, y1 = bottom-right corner\n    x0 &lt;= x1\n    y0 &lt;= y1\n\n    Args:\n        bbox_a:\n        bbox_b:\n\n    Returns:\n\n    \"\"\"\n    bbox_a = cls.parse_bbox(bbox_a)\n    bbox_b = cls.parse_bbox(bbox_b)\n\n    x0_a, y0_a, x1_a, y1_a = bbox_a\n    x0_b, y0_b, x1_b, y1_b = bbox_b\n\n    # if both x0_a and x1_a are less than x0_b or x1_b\n    # this means that bbox_a is to the left of bbox_b\n    # if both y0_a and y1_a are less than y0_b or y1_b\n    # this means that bbox_a is above bbox_b\n    horizontal_distance = [x0_a - x0_b, x1_a - x0_b, x0_a - x1_b, x1_a - x1_b]\n    vertical_distance = [y0_a - y1_b, y1_a - y0_b, y0_a - y0_b, y1_a - y1_b]\n    abs_horizontal_distance = [abs(item) for item in horizontal_distance]\n    abs_vertical_distance = [abs(item) for item in vertical_distance]\n\n    # get the mini distance and then find their real value, keep the sign\n    min_horizontal_distance = min(abs_horizontal_distance)\n    min_vertical_distance = min(abs_vertical_distance)\n\n    # get the index\n    min_horizontal_distance_index = abs_horizontal_distance.index(\n        min_horizontal_distance\n    )\n    min_vertical_distance_index = abs_vertical_distance.index(min_vertical_distance)\n\n    # get the real value\n    min_horizontal_distance_value = horizontal_distance[\n        min_horizontal_distance_index\n    ]\n    min_vertical_distance_value = vertical_distance[min_vertical_distance_index]\n\n    # whether it is left or right, above or below will be control by the\n    # angle of the center of the two bounding boxes\n    center_a = ((x0_a + x1_a) / 2, (y0_a + y1_a) / 2)\n    center_b = ((x0_b + x1_b) / 2, (y0_b + y1_b) / 2)\n\n    # use bbox_a as the reference point\n    center_x_diff = center_a[0] - center_b[0]\n    center_y_diff = center_a[1] - center_b[1]\n\n    # if center_x_diff &gt; 0, then mean bbox_a is to the right of bbox_b, so bbox_b is on the left\n    position_horizontal = \"left\" if center_x_diff &gt; 0 else \"right\"\n    # if center_y_diff &gt; 0, then mean bbox_a is below bbox_b, so bbox_b is above\n    position_vertical = \"above\" if center_y_diff &gt; 0 else \"below\"\n    return {\n        \"min_horizontal_distance\": min_horizontal_distance_value,\n        \"min_vertical_distance\": min_vertical_distance_value,\n        \"min_distance\": min(min_horizontal_distance, min_vertical_distance),\n        \"position_horizontal\": position_horizontal,\n        \"position_vertical\": position_vertical,\n    }\n</code></pre>"},{"location":"sources/utils/rect/#Docs2KG.utils.rect.BlockFinder.find_closest_blocks","title":"<code>find_closest_blocks(block_rect, text_blocks)</code>  <code>classmethod</code>","text":"<p>Find the closest text block to the image block</p> <p>TODO: - Based on how large it close to the margin, we can decide whether it is a important image or not - Based on the size of the image, we can determine whether we need the left, right information</p> <p>Parameters:</p> Name Type Description Default <code>block_rect</code> <code>str</code> <p>The bounding box of the image block as a string</p> required <code>text_blocks</code> <code>List[str]</code> <p>The bounding boxes of the text blocks as strings</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The index of the closest text block to the image block in each direction</p> Source code in <code>Docs2KG/utils/rect.py</code> <pre><code>@classmethod\ndef find_closest_blocks(\n    cls,\n    block_rect: str,\n    text_blocks: List[str],\n) -&gt; dict:\n    \"\"\"\n    Find the closest text block to the image block\n\n    TODO:\n    - Based on how large it close to the margin, we can decide whether it is a important image or not\n    - Based on the size of the image, we can determine whether we need the left, right information\n\n    Args:\n        block_rect (str): The bounding box of the image block as a string\n        text_blocks (List[str]): The bounding boxes of the text blocks as strings\n\n    Returns:\n        dict: The index of the closest text block to the image block in each direction\n    \"\"\"\n    left_index = None\n    right_index = None\n    above_index = None\n    below_index = None\n    distances = []\n    for i, text_block in enumerate(text_blocks):\n        distance = cls.bbox_distance(block_rect, text_block)\n        distance[\"index\"] = i\n        distance[\"bbox\"] = text_block\n        distances.append(distance)\n\n    df = pd.DataFrame(distances)\n\n    # for all the text blocks on top, get the closest one\n    # get above one and also the abs(min_vertical_distance) is &gt; 1\n    above_blocks = df[df[\"position_vertical\"] == \"above\"].copy(deep=True)\n    # get is all value to be abs\n    above_blocks[\"min_vertical_distance\"] = above_blocks[\n        \"min_vertical_distance\"\n    ].apply(abs)\n    above_blocks = above_blocks[abs(above_blocks[\"min_vertical_distance\"]) &gt; 1]\n\n    if not above_blocks.empty:\n        # if it is above it, then the closest one is the one with the highest vertical distance as the value is &lt;0\n        closest_above_block = above_blocks[\n            above_blocks[\"min_vertical_distance\"]\n            == above_blocks[\"min_vertical_distance\"].min()\n        ]\n        above_index = closest_above_block[\"index\"].values[0]\n\n    # for all the text blocks on bottom, get the closest one\n    below_blocks = df[df[\"position_vertical\"] == \"below\"].copy(deep=True)\n    # get is all value to be abs\n    below_blocks[\"min_vertical_distance\"] = below_blocks[\n        \"min_vertical_distance\"\n    ].apply(abs)\n    below_blocks = below_blocks[abs(below_blocks[\"min_vertical_distance\"]) &gt; 1]\n    if not below_blocks.empty:\n        # if it is below it, then the closest one is the one with the lowest vertical distance as the value is &gt;0\n        closest_below_block = below_blocks[\n            below_blocks[\"min_vertical_distance\"]\n            == below_blocks[\"min_vertical_distance\"].min()\n        ]\n        below_index = closest_below_block[\"index\"].values[0]\n\n    # for all the text blocks on left, get the closest one\n    left_blocks = df[df[\"position_horizontal\"] == \"left\"].copy(deep=True)\n    # get is all value to be abs\n    left_blocks[\"min_horizontal_distance\"] = left_blocks[\n        \"min_horizontal_distance\"\n    ].apply(abs)\n    left_blocks = left_blocks[abs(left_blocks[\"min_horizontal_distance\"]) &gt; 1]\n    if not left_blocks.empty:\n        closest_left_block = left_blocks[\n            left_blocks[\"min_horizontal_distance\"]\n            == left_blocks[\"min_horizontal_distance\"].min()\n        ]\n        left_index = closest_left_block[\"index\"].values[0]\n\n    # for all the text blocks on right, get the closest one\n    right_blocks = df[df[\"position_horizontal\"] == \"right\"].copy(deep=True)\n    # get is all value to be abs\n    right_blocks[\"min_horizontal_distance\"] = right_blocks[\n        \"min_horizontal_distance\"\n    ].apply(abs)\n    right_blocks = right_blocks[abs(right_blocks[\"min_horizontal_distance\"]) &gt; 1]\n    if not right_blocks.empty:\n        closest_right_block = right_blocks[\n            right_blocks[\"min_horizontal_distance\"]\n            == right_blocks[\"min_horizontal_distance\"].min()\n        ]\n        right_index = closest_right_block[\"index\"].values[0]\n\n    return {\n        \"left\": left_index,\n        \"right\": right_index,\n        \"above\": above_index,\n        \"below\": below_index,\n    }\n</code></pre>"},{"location":"sources/utils/rect/#Docs2KG.utils.rect.BlockFinder.parse_bbox","title":"<code>parse_bbox(s)</code>  <code>classmethod</code>","text":"<p>Parse a bounding box string into a tuple of floats.</p> Source code in <code>Docs2KG/utils/rect.py</code> <pre><code>@classmethod\ndef parse_bbox(cls, s):\n    \"\"\"Parse a bounding box string into a tuple of floats.\"\"\"\n    return tuple(map(float, ast.literal_eval(s)))\n</code></pre>"},{"location":"sources/utils/timer/","title":"Timer","text":""},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, the_logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Args:\n            the_logger (Logger): logger object\n            message (str): message to be logged\n\n\n        \"\"\"\n        self.message = message\n        self.logger = the_logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enters to start to write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enters to start to write this</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enters to start to write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"sources/utils/timer/#Docs2KG.utils.timer.timer.__init__","title":"<code>__init__(the_logger, message)</code>","text":"<p>init the timer</p> <p>Parameters:</p> Name Type Description Default <code>the_logger</code> <code>Logger</code> <p>logger object</p> required <code>message</code> <code>str</code> <p>message to be logged</p> required Source code in <code>Docs2KG/utils/timer.py</code> <pre><code>def __init__(self, the_logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Args:\n        the_logger (Logger): logger object\n        message (str): message to be logged\n\n\n    \"\"\"\n    self.message = message\n    self.logger = the_logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"sources/utils/llm/count_tokens/","title":"Count tokens","text":""},{"location":"sources/utils/llm/count_tokens/#Docs2KG.utils.llm.count_tokens.count_tokens","title":"<code>count_tokens(text, model_name='cl100k_base')</code>","text":"<p>Count the number of tokens in the text</p> <p>References: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</p> Encoding name OpenAI models cl100k_base gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large p50k_base Codex models, text-davinci-002, text-davinci-003 r50k_base (or gpt2) GPT-3 models like davinci <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to count the tokens</p> required <code>model_name</code> <code>str</code> <p>The model name to use for tokenization. Default is \"cl100k_base\"</p> <code>'cl100k_base'</code> <p>Returns:</p> Name Type Description <code>total_token</code> <code>int</code> <p>The number of tokens in the text</p> Source code in <code>Docs2KG/utils/llm/count_tokens.py</code> <pre><code>def count_tokens(text, model_name=\"cl100k_base\") -&gt; int:\n    \"\"\"\n    Count the number of tokens in the text\n\n    References: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n\n    | Encoding name | OpenAI models                                                                                 |\n    |---------------|-----------------------------------------------------------------------------------------------|\n    | cl100k_base   | gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large  |\n    | p50k_base     | Codex models, text-davinci-002, text-davinci-003                                              |\n    | r50k_base (or gpt2) | GPT-3 models like davinci                                                               |\n\n    Args:\n        text (str): The text to count the tokens\n        model_name (str): The model name to use for tokenization. Default is \"cl100k_base\"\n\n\n    Returns:\n        total_token (int): The number of tokens in the text\n\n    \"\"\"\n    enc = tiktoken.get_encoding(model_name)\n    tokens = enc.encode(text)\n    return len(tokens)\n</code></pre>"},{"location":"sources/utils/llm/estimate_price/","title":"Estimate price","text":""},{"location":"sources/utils/llm/estimate_price/#Docs2KG.utils.llm.estimate_price.estimate_price","title":"<code>estimate_price(token_count, model_name='gpt-3.5-turbo')</code>","text":"<p>Estimate the price for the token count, for different models.</p> <p>Current, we will use the gpt to extract layout and content structure from the document.</p> <p>So roughly input and output tokens are the same So we will have (input price + output price) * token_count * call_it_twice</p> <p>Parameters:</p> Name Type Description Default <code>token_count</code> <code>int</code> <p>Number of tokens</p> required <code>model_name</code> <code>str</code> <p>Model name to estimate the price Choices: \"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-4-turbo\"</p> <code>'gpt-3.5-turbo'</code> <p>Returns:</p> Name Type Description <code>price_in_usd</code> <code>float</code> <p>Price in USD</p> <ul> <li>Model: gpt-3.5-turbo =&gt; Price US$0.50 / 1M tokens (Input), US1.50 / 1M tokens (Output)</li> </ul> Source code in <code>Docs2KG/utils/llm/estimate_price.py</code> <pre><code>def estimate_price(token_count: int, model_name: str = \"gpt-3.5-turbo\") -&gt; float:\n    \"\"\"\n    Estimate the price for the token count, for different models.\n\n    Current, we will use the gpt to extract layout and content structure from the document.\n\n    So roughly input and output tokens are the same\n    So we will have (input price + output price) * token_count * call_it_twice\n\n    Args:\n        token_count (int): Number of tokens\n        model_name (str): Model name to estimate the price\n            Choices: \"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-4-turbo\"\n\n    Returns:\n        price_in_usd (float): Price in USD\n\n\n    - Model: gpt-3.5-turbo =&gt; Price US$0.50 / 1M tokens (Input), US1.50 / 1M tokens (Output)\n\n    \"\"\"\n    if model_name == \"gpt-3.5-turbo\":\n        return 2 * token_count * 2 / 1000000\n    if model_name == \"gpt-4o\":\n        return 2 * token_count * 20 / 1000000\n    if model_name == \"gpt-4-turbo\":\n        return 2 * token_count * 40 / 1000000\n    raise ValueError(f\"Model {model_name} is not supported\")\n</code></pre>"},{"location":"sources/utils/llm/track_usage/","title":"Track usage","text":""},{"location":"sources/utils/llm/track_usage/#Docs2KG.utils.llm.track_usage.track_usage","title":"<code>track_usage(response)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatCompletion</code> <p>The response from the OpenAI API</p> required <p>OpenAI Model Price</p> Model Name Input Cost (per 1M tokens) Output Cost (per 1M tokens) gpt-3.5-turbo $0.5 $1.5 gpt-4o $5 $15 gpt-4-turbo $10 $30 gpt-4 $30 $60 <p>Returns:</p> Name Type Description <code>total_cost</code> <code>float</code> <p>The total cost of the response</p> Source code in <code>Docs2KG/utils/llm/track_usage.py</code> <pre><code>def track_usage(response: ChatCompletion) -&gt; float:\n    \"\"\"\n    Args:\n        response: The response from the OpenAI API\n\n    OpenAI Model Price\n\n    | Model Name    | Input Cost (per 1M tokens) | Output Cost (per 1M tokens) |\n    |---------------|----------------------------|-----------------------------|\n    | gpt-3.5-turbo | $0.5                       | $1.5                        |\n    | gpt-4o        | $5                         | $15                         |\n    | gpt-4-turbo   | $10                        | $30                         |\n    | gpt-4         | $30                        | $60                         |\n\n    Returns:\n        total_cost (float): The total cost of the response\n    \"\"\"\n    llm_model = response.model\n    prompt_tokens = response.usage.prompt_tokens\n    completion_tokens = response.usage.completion_tokens\n    input_cost = (\n        OPENAI_MODEL_PRICE[llm_model][\"input_cost\"]\n        * (prompt_tokens + completion_tokens)\n        / 1e6\n    )\n    output_cost = OPENAI_MODEL_PRICE[llm_model][\"output_cost\"] * completion_tokens / 1e6\n    logger.debug(f\"Input Cost: ${input_cost}\")\n    logger.debug(f\"Output Cost: ${output_cost}\")\n    total_cost = input_cost + output_cost\n    logger.debug(f\"Total Cost: ${total_cost}\")\n    return total_cost\n</code></pre>"}]}